<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LXiHa`Notes</title>
  
  <subtitle>The House Belong to Love and Freedom.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liujunjie11.github.io/"/>
  <updated>2018-04-04T02:29:05.477Z</updated>
  <id>https://liujunjie11.github.io/</id>
  
  <author>
    <name>刘俊</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python3爬取豆瓣Top250电影信息</title>
    <link href="https://liujunjie11.github.io/2018/04/03/python3%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3Top250%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"/>
    <id>https://liujunjie11.github.io/2018/04/03/python3爬取豆瓣Top250电影信息/</id>
    <published>2018-04-03T14:09:36.000Z</published>
    <updated>2018-04-04T02:29:05.477Z</updated>
    
    <content type="html"><![CDATA[<p>以下是通过正则表达式爬取的<em>猫眼电影</em>以及<em>豆瓣电影</em>的相关的电影信息的代码过程。</p><hr><h2 id="爬取猫眼电影"><a href="#爬取猫眼电影" class="headerlink" title="爬取猫眼电影"></a>爬取猫眼电影</h2><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>打开<em>Chrome</em>的调试工具，可发现相关的信息都在，并且以翻页的形式来得到更多的电影信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-04%20%E4%B8%8A%E5%8D%8810.17.59.png" alt=""></p><blockquote><p>我们可通过其爬取需要的信息，利用正则表达式来进行文本的挖掘。</p></blockquote><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre><code>&apos;&apos;&apos;    函数目标：    爬取猫眼电影的top100信息    编写时间：    2018-03-28&apos;&apos;&apos;&apos;&apos;&apos;import requestsimport reimport jsonif __name__ == &apos;__main__&apos;:print(&apos;猫眼电影Top100信息如下：&apos;)#循环10次，得出页面上的Top100的电影信息for i in range(0, 10):    url = &quot;http://maoyan.com/board/4?&quot;    header = {            &apos;Host&apos;:&apos;maoyan.com&apos;,            &apos;Referer&apos;:&apos;http://maoyan.com/board/4?offset=20&apos;,            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&apos;}    paramters = {            &apos;offset&apos;: i * 10}    request_tasget = requests.get(url=url, headers=header, params=paramters)    request_tasget.encoding = &apos;utf-8&apos;    # 实际上应该考虑考虑网的问题...连不上就一般没有数据返回了，找一个字符作为接口之后利用万能表达式即可    infos_list = re.findall(    r&apos;&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?alt.*?src=&quot;(.*?)&quot;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?&quot;releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?&quot;integer&quot;&gt;(.*?)&lt;/i&gt;.*?&quot;fraction&quot;&gt;(.*?)&lt;/i&gt;&apos;,    request_tasget.text, re.S)  # @UndefinedVariable    #建立存储信息的字典    for each in infos_list:        yields = {                &apos;index&apos;:each[0],                &apos;image_info&apos;:each[1],                &apos;name&apos;:each[2],                &apos;actor&apos;:each[3].strip(),                &apos;time&apos;:each[4],                &apos;score&apos;:each[5] + each[6]                }        print(yields)</code></pre><h2 id="爬取豆瓣电影"><a href="#爬取豆瓣电影" class="headerlink" title="爬取豆瓣电影"></a>爬取豆瓣电影</h2><h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><p>与爬取猫眼电影信息一样，都是一样的，具体的差距还是在豆瓣上爬取信息需要<em>cookie</em>，具体可参考下面的代码。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-04%20%E4%B8%8A%E5%8D%8810.18.27.png" alt=""></p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><pre><code>&apos;&apos;&apos;    函数目标：    爬取豆瓣Top250的电影信息    编写时间：    2018-04-01&apos;&apos;&apos;import requestsimport re if __name__ == &apos;__main__&apos;:print(&apos;以下为豆瓣Top250的电影信息：&apos;)# 从页面可看到一共有十页，每页上有25个电影的信息for i in range(0, 10):    url = &quot;https://movie.douban.com/top250?&quot;    header = {            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Cookie&apos;: &apos;bid=x9ipkoKiQgw; _pk_ses.100001.4cf6=*; __utma=30149280.2001666009.1522587991.1522587991.1522587991.1; __utmb=30149280.0.10.1522587991; __utmc=30149280; __utmz=30149280.1522587991.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utma=223695111.1530785565.1522587991.1522587991.1522587991.1; __utmb=223695111.0.10.1522587991; __utmc=223695111; __utmz=223695111.1522587991.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); ll=&quot;118146&quot;; _vwo_uuid_v2=D575771A914BC38B3B7D081C0A0296FDC|939d8f7fdff1f383a97d572dab39fa1a; _pk_id.100001.4cf6=1de732c7ac4dc22b.1522587990.1.1522588516.1522587990.; ct=y&apos;,            &apos;Host&apos;: &apos;movie.douban.com&apos;,            &apos;Referer&apos;: &apos;https://movie.douban.com/top250?start=25&amp;filter=&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36}&apos;             }    paramters = {             &apos;start&apos;: i * 25,             &apos;filter&apos;:&apos;&apos;              }    tasget = requests.get(url=url, headers=header, params=paramters)    tasget.encoding = &apos;utf-8&apos;    # 匹配正则表达式    infos = re.findall(r&apos;&lt;li&gt;.*?&lt;em class=&quot;&quot;&gt;(.*?)&lt;/em&gt;.*?alt=.*?src=&quot;(.*?)&quot;.*?&quot;title&quot;&gt;(.*?)&lt;/span&gt;.*?class=&quot;title&quot;&gt;&amp;nbsp;/&amp;nbsp;(.*?)&lt;/span&gt;.*?&quot;other&quot;&gt;&amp;nbsp;/&amp;nbsp;(.*?)&lt;/span&gt;.*?&lt;p class=&quot;&quot;&gt;(.*?)&amp;nbsp;&amp;nbsp;&amp;nbsp;(.*?)&lt;br&gt;(.*?)&amp;nbsp;/&amp;nbsp(.*?)&amp;nbsp;/&amp;nbsp(.*?)&lt;/p&gt;.*?average&quot;&gt;(.*?)&lt;/span&gt;&apos;, tasget.text, re.S)  # @UndefinedVariable    # 建立一个空字典用于存储相关的信息    for each in infos:        yields = {            &apos;index&apos;:each[0],  # 排名            &apos;img_info&apos;:each[1],  # 照片地址            &apos;name&apos;:each[2].strip() + &quot;/&quot; + each[3].strip() + &quot;/&quot; + each[4].strip(),  # 影片名称            &apos;director&apos;:each[5].strip(),  # 导演            &apos;actor&apos;:each[6],  # 演员            &apos;time_cy&apos;:each[7].strip() + each[8],  # 上演时间及地区            &apos;type&apos;:each[9].strip(),  # 影片类型            &apos;score&apos;:each[10]  # 评分            }        print(yields)</code></pre><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>简单说说其中的含义，表达式<code>.*?</code>是一个万能的匹配式，<code>(.*?)</code>是匹配想要爬取的内容，并且每一次伴随着一个索引号，每一个索引号对应着的信息不同。利用正则表达式可方便的进行文本与代码的分开挖掘，一般在写正则表达式时用<em>html</em>代码中的一些词来进行过渡就可以方便的写出来了。</p><blockquote><p>多说无益，还是需要自己来进行代码的测试了解的。</p></blockquote><ul><li>参考的博客：<a href="https://cuiqingcai.com" target="_blank" rel="external">https://cuiqingcai.com</a></li></ul><blockquote><p>可根据此博客来学习更多的爬虫知识。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以下是通过正则表达式爬取的&lt;em&gt;猫眼电影&lt;/em&gt;以及&lt;em&gt;豆瓣电影&lt;/em&gt;的相关的电影信息的代码过程。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;爬取猫眼电影&quot;&gt;&lt;a href=&quot;#爬取猫眼电影&quot; class=&quot;headerlink&quot; title=&quot;爬取猫眼电影&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="python爬虫" scheme="https://liujunjie11.github.io/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python3爬虫" scheme="https://liujunjie11.github.io/tags/python3%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Mac下的mysql安装以及相关的问题解决</title>
    <link href="https://liujunjie11.github.io/2018/04/03/Mac%E4%B8%8B%E7%9A%84mysql%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    <id>https://liujunjie11.github.io/2018/04/03/Mac下的mysql安装以及相关的问题解决/</id>
    <published>2018-04-03T11:48:48.000Z</published>
    <updated>2018-04-03T12:58:55.064Z</updated>
    
    <content type="html"><![CDATA[<p>最近因为学习到了爬虫的原因，存储数据需要<em>mysql</em>了，因为以前学习过其命令行，所以在之前安装过了，太久没用了，怎么开都不懂了，搞了几十分钟不想搞了，重新安装。下面记录下遇到的一些问题。</p><hr><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><h3 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h3><p>进入官网下载对应<em>Mac</em>的<em>mysql</em>。</p><blockquote><p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="external">官网地址</a></p></blockquote><h3 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h3><p>直接用命令行<code>brew Install mysql</code>。</p><blockquote><p>前提是必须安装了<em>homebrew</em>。</p></blockquote><h2 id="配置以及开启过程"><a href="#配置以及开启过程" class="headerlink" title="配置以及开启过程"></a>配置以及开启过程</h2><h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><p>在手动安装正后一步记下默认的密码（如下图）。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.34.15.png" alt=""></p><blockquote><p>即<strong>QeV.a&gt;zGa1m3</strong>为默认密码。</p></blockquote><p>在<em>偏好系统</em>中手动打开<em>mysql</em>应用。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.34.36.png" alt=""></p><p>之后打开终端，为其配置。（命令行如下依次）</p><p><code>vi ~/bash_profile</code></p><p>进入按字母<em>i</em>进入编辑模式，输入：</p><p><code>export PATH=&quot;$PATH:/usr/local/mysql/bin&quot;</code></p><blockquote><p>在此可能有疑惑，为什么可以直接<code>mysql/bin</code>？因为在我们手动下载<br><em>mysql</em>之后系统已经自动的复制了一遍，并且将其名为<em>mysql</em>，如图在<em>/usr/local/</em>可找到。</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.18.02.png" alt=""></p><hr><p>接着输入相关的命令…这时已经配置好了，按下<em>esc</em>健进入命令行模式，输入<code>：wq</code>,推出。之后为了快速见效输入<code>source ~/bash_profile</code></p><h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><p>输入命令行<code>mysql -uroot -p</code>,提示输入刚刚的默认密码，进入到了<em>mysql</em>的编译界面之后，我们修改默认密码，输入代码<code>set PASSWORD =PASSWORD(&#39;123456&#39;);</code></p><blockquote><p><strong>其中的<em>123456</em>为新的密码。</strong></p></blockquote><p>之后为测试是否已经修改成功，输入<code>exit();</code>推出界面，输入命令行<code>mysql -uroot -p</code>,提示输入刚刚的新密码，成功进入编译界面。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.32.00.png" alt=""></p><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><p>可参考：<a href="https://www.jianshu.com/p/b02be6026a2a" target="_blank" rel="external">https://www.jianshu.com/p/b02be6026a2a</a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p><strong>解决问题：ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: YES)</strong></p><p>可参考：<a href="https://www.digitalocean.com/community/questions/setup-mysql-on-ubuntu-droplet-getting-error-error-1045-28000-access-denied-for-user-root-localhost-using-password-yes" target="_blank" rel="external">https://www.digitalocean.com/community/questions/setup-mysql-on-ubuntu-droplet-getting-error-error-1045-28000-access-denied-for-user-root-localhost-using-password-yes</a></p><p><strong>解决进程问题</strong></p><p>可参考：<a href="https://blog.csdn.net/liumaolincycle/article/details/51896592" target="_blank" rel="external">https://blog.csdn.net/liumaolincycle/article/details/51896592</a></p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>有时下载完之后也会出现上面的那个问题，我是通过重新启动电脑之后获得了解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近因为学习到了爬虫的原因，存储数据需要&lt;em&gt;mysql&lt;/em&gt;了，因为以前学习过其命令行，所以在之前安装过了，太久没用了，怎么开都不懂了，搞了几十分钟不想搞了，重新安装。下面记录下遇到的一些问题。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;安装过程&quot;&gt;&lt;a href=&quot;#安装
      
    
    </summary>
    
      <category term="mysql" scheme="https://liujunjie11.github.io/categories/mysql/"/>
    
    
      <category term="mysql" scheme="https://liujunjie11.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>关于解决python中UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe8 in position 67986: ordinal not in range(128)的问题</title>
    <link href="https://liujunjie11.github.io/2018/03/28/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3python%E4%B8%ADUnicodeDecodeError-ascii-codec-can-t-decode-byte-0xe8-in-position-67986-ordinal-not-in-range-128-%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/28/关于解决python中UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xe8-in-position-67986-ordinal-not-in-range-128-的问题/</id>
    <published>2018-03-28T10:18:54.000Z</published>
    <updated>2018-03-28T10:26:59.245Z</updated>
    
    <content type="html"><![CDATA[<p>在学习爬虫的过程中，在运行编者的代码时出现了<em>UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe8 in position 67986: ordinal not in range(128)</em>的编码问题。</p><p>具体原因是因为程序默认的解析编码格式发生了冲突造成的，简单来说就是需要解析的内容与<em>API</em>程序默认的解析编码格式不同，所以才会出现如上的错误。</p><p>因为是打算用<em>python</em>解析<em>JS</em>文件，所以我就贴上我的解决方案了：</p><pre><code>ctx = node.compile(open(file, encoding=&apos;utf-8&apos;).read())</code></pre><blockquote><p>加上自行规定的解码格式即可。</p></blockquote><p><strong>解决来源于GitHub论坛…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习爬虫的过程中，在运行编者的代码时出现了&lt;em&gt;UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe8 in position 67986: ordinal not in range(128)&lt;/em&gt;的编码问题
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>关于homebrew下载出现&quot;Error/go/version missing for  gotools resource!&quot;错误的解决方案</title>
    <link href="https://liujunjie11.github.io/2018/03/28/%E5%85%B3%E4%BA%8Ehomebrew%E4%B8%8B%E8%BD%BD%E5%87%BA%E7%8E%B0Error:go:version%20missing%20for%20%20gotools%20resource!%E9%94%99%E8%AF%AF%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://liujunjie11.github.io/2018/03/28/关于homebrew下载出现Error:go:version missing for  gotools resource!错误的解决方案/</id>
    <published>2018-03-28T02:47:34.000Z</published>
    <updated>2018-03-28T02:48:35.030Z</updated>
    
    <content type="html"><![CDATA[<p>最近用<em>homebrew</em>下载<em>mongodb</em>，发现出现了<em>Error: go: version missing for “gotools” resource!</em>的错误，试了几次都不行，最后在谷歌用英文搜索关键词在<em>GitHub</em>的一个论坛用相关的解决方案。</p><p><strong>使用命令行</strong></p><blockquote><p><strong>git -C “$(brew –repo)” fetch –tags</strong></p></blockquote><p>之后再次输入：</p><blockquote><p><strong>brew update –force</strong></p></blockquote><p>最后再次用命令行<em>brew install mongodb</em>，发现已经能够正确成功下载了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近用&lt;em&gt;homebrew&lt;/em&gt;下载&lt;em&gt;mongodb&lt;/em&gt;，发现出现了&lt;em&gt;Error: go: version missing for “gotools” resource!&lt;/em&gt;的错误，试了几次都不行，最后在谷歌用英文搜索关键词在&lt;em&gt;GitH
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：线性表</title>
    <link href="https://liujunjie11.github.io/2018/03/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8/"/>
    <id>https://liujunjie11.github.io/2018/03/26/数据结构与算法：线性表/</id>
    <published>2018-03-26T10:56:19.000Z</published>
    <updated>2018-04-03T12:29:47.274Z</updated>
    
    <content type="html"><![CDATA[<p>打算重新好好系统的学习数据结构了。这是开始。</p><p>因为概念已经是烂大街了，在此只记录下我用<em>java</em>或者是<em>python</em>实现相关功能的代码。</p><hr><h2 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h2><h3 id="java实现"><a href="#java实现" class="headerlink" title="java实现"></a>java实现</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;打算重新好好系统的学习数据结构了。这是开始。&lt;/p&gt;
&lt;p&gt;因为概念已经是烂大街了，在此只记录下我用&lt;em&gt;java&lt;/em&gt;或者是&lt;em&gt;python&lt;/em&gt;实现相关功能的代码。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;查找&quot;&gt;&lt;a href=&quot;#查找&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://liujunjie11.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构与算法" scheme="https://liujunjie11.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识：认识计算机</title>
    <link href="https://liujunjie11.github.io/2018/03/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%9A%E8%AE%A4%E8%AF%86%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    <id>https://liujunjie11.github.io/2018/03/26/计算机基础知识：认识计算机/</id>
    <published>2018-03-26T03:44:12.000Z</published>
    <updated>2018-03-26T05:12:33.821Z</updated>
    
    <content type="html"><![CDATA[<p>计算机发展到了如今这样的一个地步实在让人感兴趣…</p><p>于是看了一些书和文章，想了解了解一个计算机是如何跑起来的。</p><hr><h2 id="机器本质"><a href="#机器本质" class="headerlink" title="机器本质"></a>机器本质</h2><blockquote><p><strong>计算机的本质</strong></p></blockquote><ul><li>计算机的本质<strong>：（电脑）计算机 = 计算机器</strong></li></ul><p>没错，如今<strong>我们使用的计算机就是和我们小学时按的那个只会加减乘除计算器的本质毫无区别</strong>。使用计算机时，我们会先通过键盘或者是语音输入想要了解的信息，之后计算机会通过内部一系列的运算之后，输出相关的界面信息到我们的屏幕上。那我们可理解计算机就是一通过我们输入，然后自己默默运算完输出结果的机器。电脑电脑，正所谓脑子就是人类的<strong>计算机器</strong>，电脑不过是插上电源才能运行的计算机器。</p><p><img src="" alt=""></p><p>什么图画啊，文档内容等等均是先转化为相关的数字信息（如我们学过的二进制，十六进制等）后才在显示器上呈现出来的，所以在此可理解为什么还会有那么多各式各样的字符编码了吧。如中文编码常见的<em>GB2312</em>等。</p><blockquote><p><strong>计算机上的1与0</strong></p></blockquote><ul><li><strong>计算机是电子产品，其构造是由一些硬件组成。</strong></li></ul><p>再继续说说烂大街的绝大部分地球人都知道的事实：<strong>计算机只认识1和0</strong>。因为计算机是集成电路（IC）组成,运行起来接上电源之后需要传输数据了（即属于电子数字电路），而在数字电路中，二进制（binary）数是指用二进制记数系统，即以2为基数的记数系统表示的数字。这一系统中，通常用两个不同的符号0（代表零）和1（代表一）来表示。以2为基数代表系统是二进位制的。数字电子电路中，逻辑门的实现直接应用了二进制，因此现代的计算机和依赖计算机的设备里都用到二进制。每个数字称为一个位元（二进制位）或比特（Bit，Binary digit的缩写）。补充：<strong>比特是信息的最小单位， 字节是信息的基本单位。</strong></p><p>结合上面的说明，可知道数据都是在计算机中以1和0组成的，一是因为计算机本身的结构所致，二是因为这样可方便的转化为其他的数字类型（包括十进制之类的）。<strong>对于计算机来说，什么都是数字。只是数字的组成不同而已。</strong></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><p>维基百科：<a href="https://zh.wikipedia.org/wiki/二进制" target="_blank" rel="external">https://zh.wikipedia.org/wiki/二进制</a></p></li><li><p>书籍：《计算机是怎么跑起来的》 矢泽久雄 著</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算机发展到了如今这样的一个地步实在让人感兴趣…&lt;/p&gt;
&lt;p&gt;于是看了一些书和文章，想了解了解一个计算机是如何跑起来的。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;机器本质&quot;&gt;&lt;a href=&quot;#机器本质&quot; class=&quot;headerlink&quot; title=&quot;机器本质&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://liujunjie11.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="计算机基础知识" scheme="https://liujunjie11.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Mac下安装Redis以及其可视化客户端</title>
    <link href="https://liujunjie11.github.io/2018/03/22/Mac%E4%B8%8B%E5%AE%89%E8%A3%85Redis%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
    <id>https://liujunjie11.github.io/2018/03/22/Mac下安装Redis以及其可视化客户端/</id>
    <published>2018-03-22T13:17:30.000Z</published>
    <updated>2018-03-22T13:40:22.174Z</updated>
    
    <content type="html"><![CDATA[<p>最近想要用<em>Nosql</em>结合做一些小项目，用的是<em>Mac</em>，看到网上的教程有点乱七八糟的了，就打算记录下来了。</p><p>关于这个数据库就不在此介绍了。</p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><blockquote><p>来到官网页面下载：<a href="https://redis.io/download" target="_blank" rel="external">https://redis.io/download</a></p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%889.35.10.png" alt=""></p><hr><p>下载好之后解压，并且要知道目录文件在哪。现在打开终端，用命令 <code>cd 文件目录地址</code>，如我的是 <code>cd /Users/junjieliu/Documents/编程文件/redis-4.0.8</code>，之后使用命令 <code>sudo make</code> 成功之后出现如下图1所示，之后再使用命令 <code>make test</code>,成功之后出现如图2所示。</p><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.53.59.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.57.13.png" alt=""></p><p>之后，编译安装：在终端中输入命令：<code>sudo make install</code></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.57.56.png" alt=""></p><p>启动Redis,输入命令<code>redis-server</code></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.58.33.png" alt=""></p><blockquote><p>至此就安装成功了，以上命令有时可能会有出现错误的结果，多试试。</p></blockquote><h2 id="安装Redis-Desktop-Manager"><a href="#安装Redis-Desktop-Manager" class="headerlink" title="安装Redis Desktop Manager"></a>安装Redis Desktop Manager</h2><p>到此网站下载即可：<a href="https://sourceforge.net/projects/redis-desktop-manager.mirror/" target="_blank" rel="external">https://sourceforge.net/projects/redis-desktop-manager.mirror/</a></p><blockquote><p>当然可见我的云盘分享：<a href="https://pan.baidu.com/s/1Bvc7_tZ5yUnnwfnH2bDocg" target="_blank" rel="external">https://pan.baidu.com/s/1Bvc7_tZ5yUnnwfnH2bDocg</a></p></blockquote><p>下载完之后将软件移植系统<em>应用程序</em>一栏，打开：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%889.10.11.png" alt=""></p><hr><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>介绍好评高的一本电子书《Redis实战》：</p><p><a href="http://www.java1234.com/a/javabook/database/2017/0625/8356.html" target="_blank" rel="external">http://www.java1234.com/a/javabook/database/2017/0625/8356.html</a></p><blockquote><p>进入网站即有相关的资源。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想要用&lt;em&gt;Nosql&lt;/em&gt;结合做一些小项目，用的是&lt;em&gt;Mac&lt;/em&gt;，看到网上的教程有点乱七八糟的了，就打算记录下来了。&lt;/p&gt;
&lt;p&gt;关于这个数据库就不在此介绍了。&lt;/p&gt;
&lt;h2 id=&quot;安装过程&quot;&gt;&lt;a href=&quot;#安装过程&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="教程" scheme="https://liujunjie11.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="教程" scheme="https://liujunjie11.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>用python/R数据分析可视化汽车使用燃料情况</title>
    <link href="https://liujunjie11.github.io/2018/03/21/%E7%94%A8python:R%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96%E6%B1%BD%E8%BD%A6%E4%BD%BF%E7%94%A8%E7%87%83%E6%96%99%E6%83%85%E5%86%B5/"/>
    <id>https://liujunjie11.github.io/2018/03/21/用python:R数据分析可视化汽车使用燃料情况/</id>
    <published>2018-03-21T12:40:06.000Z</published>
    <updated>2018-03-22T07:26:01.239Z</updated>
    
    <content type="html"><![CDATA[<p>分析的是美国一个网站统计的多年汽车使用燃料情况的数据，需要从网上下载，在此利用好<em>python爬虫</em>大有裨益，可作为实战运行分析。</p><blockquote><p>此篇文章案例来源于：<img src="" alt=""></p><p>本书文章中用<em>R语言</em>实现的数据可视化，是从网上直接下的文件然后分析，我打算在此用<em>python</em>实现爬取相关文件，并且运用<em>python</em>进行可视化分析。</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>进入主页，得到了下载文件的主链接：</p><p><img src="" alt=""></p><p>在此可见资源的下载地址：</p><p><img src="" alt=""></p><hr><blockquote><p>接下来就是运用爬虫知识实现此文件的下载了。</p></blockquote><h2 id="爬虫代码"><a href="#爬虫代码" class="headerlink" title="爬虫代码"></a>爬虫代码</h2><p>我们可有这样的思路：<strong>从主页出发 –&gt; 爬取到下载文件页面的链接 –&gt; 再从此爬取到下载地址链接 –&gt; 之后运用相关的函数下载至本目录即可。</strong>以下是实现过程。</p><pre><code>&apos;&apos;&apos;    函数目标：    用python爬取相关的下载文件    编写时间：    2018-03-21&apos;&apos;&apos;import requestsimport refrom bs4 import BeautifulSoupfrom urllib.request import urlretrieve&apos;&apos;&apos;    first_url方法获取进入下载页面的地址    运用了正则表达式的匹配方法                                        &apos;&apos;&apos;def first_url():    url = &apos;https://www.fueleconomy.gov/&apos;    # 添加代理    header = {&apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,            &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;,            &apos;Accept-Language&apos;: &apos;zh-CN,zh&apos;,            &apos;Cache-Control&apos;: &apos;max-age=0&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Host&apos;: &apos;www.fueleconomy.gov&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36}&apos;            }    request = requests.get(url=url, headers=header)    # 指定编码格式    request.encoding = &apos;utf-8&apos;    &apos;&apos;&apos;        发现不用指定re.S亦可以完成匹配，用了反而会报错...        注意每次匹配到的数据之后还有索引要记得标明    &apos;&apos;&apos;    link = re.findall(r&apos;&lt;a href=&quot;(.*?)&quot;&gt;Developer Tools&lt;/a&gt;&apos;, request.text)[0]    return link&apos;&apos;&apos;    second_url方法是用来获取下载文件的地址    方法也是运用了正则表达式                                            &apos;&apos;&apos;def second_url():    # 进入下载资源的页面    se_url = &apos;https://www.fueleconomy.gov&apos; + first_url()    # 添加代理,基本上的代理信息没什么变化    header_2 = {            &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,            &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;,            &apos;Accept-Language&apos;: &apos;zh-CN,zh&apos;,            &apos;Cache-Control&apos;: &apos;max-age=0&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Host&apos;: &apos;www.fueleconomy.gov&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36}&apos;            }    request_2 = requests.get(url=se_url, headers=header_2)    request_2.encoding = &apos;utf-8&apos;    # 匹配正则表达式    link_2 = re.findall(r&apos;&lt;a href=&quot;(.*?)&quot;&gt;CSV&lt;/a&gt;&apos;, request_2.text)[0]    return link_2&apos;&apos;&apos;    已经得到了资源下载地址，在主函数中进行下载并且进行解压                                                    &apos;&apos;&apos;if __name__ == &apos;__main__&apos;:    # 获取下载文件的资源地址    down_url = &apos;https://www.fueleconomy.gov&apos; + second_url()    print(&apos;数据采集完成...&apos;)    print(&apos;开始下载文件...&apos;)    # 文件名称是：vehicles.csv.zip，格式也是非常的重要！    urlretrieve(url=down_url, filename=&apos;vehicles.csv.zip&apos;)    print(&apos;下载完成！可在本工程目录查收！&apos;)</code></pre><p>至此可在本目录下查看到下载好的压缩文件：</p><p><img src="" alt=""></p><blockquote><p>当然也可加上运用<em>python</em>解压这一部分的模块，有兴趣的朋友可自行学习运用。</p></blockquote><h2 id="代码可视化分析阶段"><a href="#代码可视化分析阶段" class="headerlink" title="代码可视化分析阶段"></a>代码可视化分析阶段</h2><p>在解压之后可先用<em>excel</em>打开来看看（会发现有<strong>39000+行</strong>数据），在此只需要知道其中的参数<code>year</code>与<code>com08</code>，前者为年份，后者为燃料的使用情况相关的数值。</p><p>可视化实现代码在下：</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这是一个平时做的小项目，适合新手入门。</p><blockquote><p>另外，以上代码中不懂的模块知识，我建议你利用好搜索引擎，查看相关的文档或者是找本书看看。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分析的是美国一个网站统计的多年汽车使用燃料情况的数据，需要从网上下载，在此利用好&lt;em&gt;python爬虫&lt;/em&gt;大有裨益，可作为实战运行分析。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;此篇文章案例来源于：&lt;img src=&quot;&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本书文章中用&lt;
      
    
    </summary>
    
      <category term="可视化 爬虫" scheme="https://liujunjie11.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96-%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="可视化 爬虫" scheme="https://liujunjie11.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96-%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>关于anaconda2与anaconda3两个版本的共存问题</title>
    <link href="https://liujunjie11.github.io/2018/03/21/%E5%85%B3%E4%BA%8Eanaconda2%E4%B8%8Eanaconda3%E4%B8%A4%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E5%85%B1%E5%AD%98%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/21/关于anaconda2与anaconda3两个版本的共存问题/</id>
    <published>2018-03-21T03:03:01.000Z</published>
    <updated>2018-03-22T08:11:50.294Z</updated>
    
    <content type="html"><![CDATA[<p>最近因为用<em>anaconda</em>的关系，下载的包导致了冲突，编译器总是识别不了，就把以前<em>Mac</em>上的<em>anaconda</em>都卸载了。打算重新来过一遍解决两者（即<em>anaconda2</em>与<em>anaconda3</em>）的共存问题。</p><blockquote><p>简单说明一下<em>anaconda3</em>对应<em>python3</em>，<em>anaconda2</em>对应<em>python2</em></p></blockquote><hr><p>马上开始吧。</p><p>先是在<a href="https://www.anaconda.com/download/#macos" target="_blank" rel="external">官网上</a>下载了两个版本，如下：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.25.39.png" alt=""></p><blockquote><p>一路确定，直到安装完成为止。<strong>不过要说明一下，安装应该分好顺序，最后安装好的即为系统默认的了（即当我们在终端输入命令：<code>python</code> 时会出现最后安装好的那个目录中的<em>python</em>版本，我是最后安装的anaconda3）。</strong>如下图所示：</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.29.20.png" alt=""></p><blockquote><p>Python 3.6.4 |Anaconda, Inc.| <strong>即为anaconda3为系统默认的了。</strong></p></blockquote><p>当我们再输入命令：<code>python2</code> 时与输入命令： <code>python3</code> 时，如下图所示均成为了系统默认的两个版本了。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.33.55.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.34.06.png" alt=""></p><blockquote><p>这样在下载包时就能方便多了，<strong>可以直接在终端输入命令：<code>pip install ..</code> 就可以达到anaconda3对应的python3版本的包下载问题了（经过测试，已通过！）。</strong>可能有朋友疑惑，<em>python3</em>不是对应着命令<em>pip3</em>吗，实际上经过我的发现，<em>anaconda</em>的<em>python2</em>与<em>python3</em>对应的都是命令<em>pip</em>。</p></blockquote><p>虽然解决了<em>anaconda3</em>下的<em>python3</em>的问题，<strong>那么我们必须记得我们的目标是：anaconda3与anaconda2的切换使用问题。不过遗憾的是没有找到解决方案。不过倒是有一个能在同一个版本内（即anaconda3/anaconda2）同时安装两个python版本（2/3）的方案。</strong></p><p>在<a href="https://conda.io/docs/user-guide/getting-started.html#managing-envs" target="_blank" rel="external">此官网教程中</a>说明了命令。</p><ul><li>详细的命令过程可参考此篇文章：<a href="https://foofish.net/compatible-py2-and-py3.html" target="_blank" rel="external">https://foofish.net/compatible-py2-and-py3.html</a></li></ul><blockquote><p>当然参考官网亦可。</p></blockquote><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>虽然在同一版本中解决了两个<em>python</em>版本的虚拟环境的问题（可在对应的目录中找到，并且此虚拟环境均可正常使用），但是我们的问题依旧没有得到解决（即anaconda3与anaconda2如何切换使用问题），我试想用指定目录的方法运行命令，但是一无所获。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8B%E5%8D%8812.43.20.png" alt=""></p><blockquote><p>Anaconda2 includes Python 2.7 and Anaconda3 includes Python 3.6. However, it does not matter which one you download, because you can create new environments that include any version of Python packaged with conda.</p><p>官网的解释已经说明了，建立虚拟环境只是解决在同一个<em>anaconda</em>版本下使用不同版本的<em>python</em>而已。所以我们想的关于anaconda2与anaconda3两个版本的共存切换问题目前或许没有办法实现。</p></blockquote><hr><p><strong>已经知道了解决方案：那就是打开各个的客户端进行下载即可！！！太简单了！！</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近因为用&lt;em&gt;anaconda&lt;/em&gt;的关系，下载的包导致了冲突，编译器总是识别不了，就把以前&lt;em&gt;Mac&lt;/em&gt;上的&lt;em&gt;anaconda&lt;/em&gt;都卸载了。打算重新来过一遍解决两者（即&lt;em&gt;anaconda2&lt;/em&gt;与&lt;em&gt;anaconda3&lt;/em&gt;
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>解决python文件读取时的UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xd4 in position 904: ordinal not in range(128)问题</title>
    <link href="https://liujunjie11.github.io/2018/03/20/%E8%A7%A3%E5%86%B3python%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%97%B6%E7%9A%84UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xd4-in-position-904-ordinal-not-in-range-128-%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/20/解决python文件读取时的UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xd4-in-position-904-ordinal-not-in-range-128-问题/</id>
    <published>2018-03-20T13:54:55.000Z</published>
    <updated>2018-03-20T14:01:51.630Z</updated>
    
    <content type="html"><![CDATA[<p>最近在处理一个<em>csv格式</em>的数据时，出现了<em>UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xd4 in<br>  position 904: ordinal not in range(128)</em>的错误告知。</p><p>查了一下，发现只要指定<strong><em>encoding=’iso-8859-1’</em></strong>即可解决此问题了。</p><p>代码演示：</p><pre><code>.....        with open(filename, encoding=&apos;iso-8859-1&apos;) as f:        ......</code></pre><hr><p>简单说明一下为何如此指定编码格式：</p><p><strong>ISO 8859-1，正式编号为ISO/IEC 8859-1:1998，又称Latin-1或“西欧语言”，是国际标准化组织内ISO/IEC 8859的第一个8位字符集。它以ASCII为基础，在空置的0xA0-0xFF的范围内，即解决0xA0-0xFF的范围内的编码错误问题，如上我们的0xd4在此范围之内。</strong></p><blockquote><p>详细可见维基百科地址：<a href="https://zh.wikipedia.org/wiki/ISO/IEC_8859-1" target="_blank" rel="external">https://zh.wikipedia.org/wiki/ISO/IEC_8859-1</a></p></blockquote><ul><li>参考：<a href="http://blog.csdn.net/programmer_wei/article/details/50994318" target="_blank" rel="external">http://blog.csdn.net/programmer_wei/article/details/50994318</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在处理一个&lt;em&gt;csv格式&lt;/em&gt;的数据时，出现了&lt;em&gt;UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xd4 in&lt;br&gt;  position 904: ordinal not in range(128)
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>理解多线程编程</title>
    <link href="https://liujunjie11.github.io/2018/03/20/%E7%90%86%E8%A7%A3%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"/>
    <id>https://liujunjie11.github.io/2018/03/20/理解多线程编程/</id>
    <published>2018-03-20T08:37:17.000Z</published>
    <updated>2018-03-20T08:58:49.769Z</updated>
    
    <content type="html"><![CDATA[<p>最近想研究研究什么是<em>多线程编程</em>的问题。简单了解了一下之后发现原来我对其的理解都错了…把<em>面对对象编程</em>活生生的说成了<em>多线程</em>…唉，从前文中的”口误“就…</p><h2 id="多线程与多进程"><a href="#多线程与多进程" class="headerlink" title="多线程与多进程"></a>多线程与多进程</h2><p>简单理解这两个一下：</p><ul><li><p><strong>多进程：可先理解为一个应用程序，如我们的上网用的浏览器。</strong></p></li><li><p><strong>多线程：多线程就是我们浏览器中的各种小工具，如刷新功能与新开一个标签页的功能就是两个在其中的线程。</strong></p></li></ul><blockquote><p><strong>多个进程是分开的两个应用程序，就像QQ和微信两者就是毫不相干的两个应用程序。多线程就是两个应用程序之中的多个可同时运用的小工具。</strong></p></blockquote><h2 id="多线程编程"><a href="#多线程编程" class="headerlink" title="多线程编程"></a>多线程编程</h2><p>再来看看<em>多线程编程</em>。</p><p>平时我们写程序，会将整个代码构建出各种方法函数，各有各的实现意图，以便在主类当中直接调用，最终实现目标整体意愿。</p><blockquote><p><strong>而多线程编程就是可同时在主类中调用两个或者是多个实现意图不同的方法函数，并且一同被编译器/解释器运行出结果。就成了一个多线程的问题…多线程多线程，就是能将两个或者是多个不同的方法同时运行出结果。</strong>至于是编译器还是解释器得分是什么语言了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想研究研究什么是&lt;em&gt;多线程编程&lt;/em&gt;的问题。简单了解了一下之后发现原来我对其的理解都错了…把&lt;em&gt;面对对象编程&lt;/em&gt;活生生的说成了&lt;em&gt;多线程&lt;/em&gt;…唉，从前文中的”口误“就…&lt;/p&gt;
&lt;h2 id=&quot;多线程与多进程&quot;&gt;&lt;a href=&quot;#多线程与多
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用python/R可视化GitHub上的java热门开源项目</title>
    <link href="https://liujunjie11.github.io/2018/03/19/%E7%94%A8python%E5%8F%AF%E8%A7%86%E5%8C%96GitHub%E4%B8%8A%E7%9A%84java%E7%83%AD%E9%97%A8%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/03/19/用python可视化GitHub上的java热门开源项目/</id>
    <published>2018-03-19T08:36:14.000Z</published>
    <updated>2018-04-03T12:41:40.170Z</updated>
    
    <content type="html"><![CDATA[<p>直接开始这个小项目吧。</p><ul><li>网页地址：<a href="https://api.github.com/search/repositories?q=language:java&amp;sort=stars" target="_blank" rel="external">https://api.github.com/search/repositories?q=language:java&amp;sort=stars</a></li></ul><blockquote><p>在这个网页中有相关的目前比较热门的开源项目（以<em>star</em>的数目来衡量），打开发现这是典型的<em>json</em>格式啊。</p></blockquote><h2 id="简单分析"><a href="#简单分析" class="headerlink" title="简单分析"></a>简单分析</h2><p>经过抓包可发现：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.31.32.png" alt=""></p><p>即：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.29.37.png" alt=""></p><blockquote><p>换一换就可知道所有编程语言目前比较热门的开源项目了。</p></blockquote><h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><pre><code>&apos;&apos;&apos;    函数目标：    将GitHub上的java热门的开源项目可视化    编写时间：    2018-3-19&apos;&apos;&apos;import requestsfrom matplotlib import pyplot as pltimport pygalfrom pygal.style import LightColorizedStyle as lcs, LightenStyle as lsif __name__ == &apos;__main__&apos;:    #添加代理配置    url = &apos;https://api.github.com/search/repositories&apos;    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    paramter = {&apos;q&apos;: &apos;language:java&apos;,              &apos;sort&apos;: &apos;stars&apos;}    re = requests.get(url=url, params=paramter, headers=header)    # 将网页转化为python字典即用json()函数方法才可显示与网页内容一致！    re.encoding = &apos;utf-8&apos;    js_cont = re.json()    item = js_cont[&apos;items&apos;]    star_count = []    names = []    full_names = []    for each in item:            star_count.append(each[&apos;stargazers_count&apos;])            names.append(each[&apos;name&apos;])            full_names.append(each[&apos;full_name&apos;])    # 添加高亮颜色    my_style = ls(&apos;#333366&apos;, base_style=lcs)    # 添加相关的设置    my_config = pygal.Config()    my_config.label_font_size = 28    bar_chart = pygal.Bar(config=my_config, style=my_style, x_label_rotation=60, show_legend=False)    bar_chart.add(&apos;&apos;, star_count)    bar_chart.title = &apos;Java  projects stars on Github&apos;    bar_chart.x_labels = names    # 保存至目录下的文件中    bar_chart.render_to_file(&apos;Java stars in Github.svg&apos;)</code></pre><blockquote><p>之中不懂的可利用好搜索引擎。有一些爬虫的知识。</p></blockquote><p>运行得到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.36.33.png" alt=""></p><blockquote><p>简单说说用<em>python</em>可视化的感受，比较喜欢用<em>python</em>，用的比较多，意味发现<em>pygal</em>这个库做的图很漂亮。</p></blockquote><h2 id="R代码实现"><a href="#R代码实现" class="headerlink" title="R代码实现"></a>R代码实现</h2><ul><li>说明：因为<em>R</em>的爬虫没怎么看，先用可视化…日后有时间爬虫写上…</li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>参考：《python入门与实践》【美】Eric Matthes著</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;直接开始这个小项目吧。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网页地址：&lt;a href=&quot;https://api.github.com/search/repositories?q=language:java&amp;amp;sort=stars&quot; target=&quot;_blank&quot; rel=&quot;ex
      
    
    </summary>
    
      <category term="可视化" scheme="https://liujunjie11.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="python/R可视化" scheme="https://liujunjie11.github.io/tags/python-R%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（三）：带宽单位换算与存储单位换算</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%B8%A6%E5%AE%BD%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97%E4%B8%8E%E5%AD%98%E5%82%A8%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（三）：带宽单位换算与存储单位换算/</id>
    <published>2018-03-15T14:06:15.000Z</published>
    <updated>2018-03-15T14:25:50.120Z</updated>
    
    <content type="html"><![CDATA[<p>位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.</p><blockquote><p>1比特 = 一个二进制位，只有0和1两种状态<br>  1字节 = 8 比特</p></blockquote><p>1 Byte(B)＝8bit（位）<br>1KB＝1024Byte（字节）</p><ul><li>再来看看平时常见的下载参数：</li></ul><p><strong>Mbps：</strong>带宽单位，在 Mbps 单位中的“b”是指“Bit（位）</p><p><strong>MB/s：</strong>速度单位，其中的 B 是指“Byte（字节）</p><blockquote><p>其中1MB/s=8Mbps，下载工具一般以Bps计算，所以它们之间有8bit=1Byte的换算关系，一个字节，是由八位二进制位组成的，所以可解释一个200M的网，换算为字节，实际上仅仅极限速度能达到200/8=25M的速度。</p></blockquote><h2 id="存储单位的换算"><a href="#存储单位的换算" class="headerlink" title="存储单位的换算"></a>存储单位的换算</h2><p>计算机存储单位一般用bit、B、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB……来表示，它们之间的关系是：<br>位 bit (比特)(Binary Digits)：存放一位二进制数，即 0 或 1，最小的存储单位。</p><p><strong>换算：</strong></p><p>字节byte：8个二进制位为一个字节(B)，最常用的单位。</p><p>1 Byte（B） = 8 bit</p><p>1 Kilo Byte（KB） = 1024B</p><p>1 Mega Byte（MB） = 1024 KB</p><p>1 Giga Byte （GB）= 1024 MB</p><p>1 Tera Byte（TB）= 1024 GB</p><p>1 Peta Byte（PB） = 1024 TB</p><p>1 Exa Byte（EB） = 1024 PB</p><p>1 Zetta Byte（ZB） = 1024 EB</p><p>1Yotta Byte（YB）= 1024 ZB</p><p>1 Bronto Byte（BB） = 1024 YB</p><p>1Nona Byte（NB）=1024 BB</p><p>1 Dogga Byte（DB）=1024 NB</p><p>1 Corydon Byte（CB）=1024DB</p><p>1 Xero Byte （XB）=1024CB</p><blockquote><p>进制单位全称及译音：</p><p>yotta，[尧]它， Y. 10^24，</p><p>zetta，[泽]它， Z. 10^21，</p><p>exa，[艾]可萨， E. 10^18，</p><p>peta，[拍]它， P. 10^15，</p><p>tera，[太]拉， T. 10^12，</p><p>giga，[吉]咖， G. 10^9，</p><p>mega，[兆]，M. 10^6</p></blockquote><ul><li><strong>b(bit)与B的认识</strong></li></ul><p>字节(B)是电脑中表示信息含义的最小单位，通常情况下一个ACSII码就是一个字节的空间来存放。而事实上电脑中还有比字节更小的单位，因为一个字节是由八个二进制位组成的，换一句话说，每个二进制位所占的空间才是电脑中最小的单位，我们把它称为位，也称比特（bit）。一个字节等于八位。人们之所以把字节称为电脑中表示信息含义的最小单位，是因为一位并不能表示我们现实生活中的一个相对完整的信息。</p><ul><li><strong>计算机储存单位的进率是1024而不是1000？</strong></li></ul><p>目前计算机都是二进制的，让它们计算单位，只有2的整数幂时才能非常方便计算机计算，因为电脑内部的电路工作有高电平和低电平两种状态.所以就用二进制来表示信号，(控制信号和数据)，以便计算机识别。而人习惯于使用10进制，所以存储器厂商们才用1000作进率。这样导致的后果就是实际容量要比标称容量少，不过这是合法的。1024是2的10次方，因为如果取大了，不接近10的整数次方，不方便人们计算；取小了，进率太低，单位要更多才能满足需求，所以取2的10次方正好。<br>计算实例：标称100GB的硬盘，其实际容量为100×1000×1000×1000字节/1024×1024×1024≈93.1GB<br>可见产品容量缩水只要满足计算的实际容量结果（上下误差应该在10%内）。</p><ul><li>参考：</li></ul><p><a href="https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305" target="_blank" rel="external">https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305</a></p><p><a href="https://www.jianshu.com/p/2b57116c27de" target="_blank" rel="external">https://www.jianshu.com/p/2b57116c27de</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1比特 = 一个二进制位，只有0和1两种状态&lt;br&gt;  1字节 = 8 比特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1 Byte(B)＝8bi
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%95%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程/</id>
    <published>2018-03-15T13:58:48.000Z</published>
    <updated>2018-03-15T14:03:52.921Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>转载自：<a href="http://blog.csdn.net/zolalad/article/details/28393209" target="_blank" rel="external">http://blog.csdn.net/zolalad/article/details/28393209</a></p></blockquote><p><strong>一．进程、线程、单核处理器</strong></p><p>进程和线程都是操作系统的概念。进程是应用程序的执行实例，每个进程是由私有的虚拟地址空间、代码、数据和其它各种系统资源组成，即进程是操作系统进行资源分配的最小单元。进程在运行过程中创建的资源随着进程的终止而被销毁，所使用的系统资源在进程终止时被释放或关闭。</p><p>线程是进程内部的一个执行单元。系统创建好进程后，实际上就启动执行了该进程的主执行线程，主执行线程以函数地址形式，比如说main或WinMain函数，将程序的启动点提供给Windows系统。主执行线程终止了，进程也就随之终止。</p><p>每一个进程至少有一个主执行线程，它无需由用户去主动创建，是由系统自动创建的。用户根据需要在应用程序中创建其它线程，多个线程并发地运行于同一个进程中。一个进程中的所有线程都在该进程的虚拟地址空间中，共同使用这些虚拟地址空间、全局变量和系统资源，所以线程间的通讯非常方便，多线程技术的应用也较为广泛。</p><p>多线程可以实现并行处理，避免了某项任务长时间占用CPU时间。要说明的一点是，目前大多数的操作系统教材中的单处理器都是指的单核处理器。对于单核单处理器（CPU）的，为了运行所有这些线程，操作系统为每个独立线程安排一些CPU时间，操作系统以轮换方式向线程提供时间片，这就给人一种假象，好象这些线程都在同时运行。由此可见，如果两个非常活跃的线程为了抢夺对CPU的控制权，在线程切换时会消耗很多的CPU资源，反而会降低系统的性能。</p><p>最开始，线程只是用于分配单个处理器的处理时间的一种工具。但假如操作系统本身支持多个处理器，那么每个线程都可分配给一个不同的处理器，真正进入“并行运算”状态。从程序设计语言的角度看，多线程操作最有价值的特性之一就是程序员不必关心到底使用了多少个处理器，程序员只需将程序编写成多线程模式即可。程序在逻辑意义上被分割为数个线程；假如机器本身安装了多个处理器，那么程序会运行得更快，毋需作出任何特殊的调校。根据前面的论述，大家可能感觉线程处理非常简单。但必须注意一个问题：共享资源！如果有多个线程同时运行，而且它们试图访问相同的资源，就会遇到一个问题。举个例子来说，两个线程不能将信息同时发送给一台打印机。为解决这个问题，对那些可共享的资源来说（比如打印机），它们在使用期间必须进入锁定状态。所以一个线程可将资源锁定，在完成了它的任务后，再解开（释放）这个锁，使其他线程可以接着使用同样的资源。</p><p><strong>多线程是为了同步完成多项任务，不是为了提高运行效率，而是为了提高资源使用效率来提高系统的效率。线程是在同一时间需要完成多项任务的时候实现的。</strong></p><p>最简单的比喻多线程就像火车的每一节车厢，而进程则是火车。车厢离开火车是无法跑动的，同理火车也不可能只有一节车厢。多线程的出现就是为了提高效率。同时它的出现也带来了一些问题。</p><p><strong>注</strong>：单核处理器并不是一个长久以来存在的概念，在近年来多核心处理器逐步普及之后，单核心的处理器为了与双核和四核对应而提出。顾名思义处理器只有一个逻辑核心。</p><p><strong>二、多核处理器和多处理器的区别</strong></p><p>多核是指一个CPU有多个核心处理器，处理器之间通过CPU内部总线进行通讯。而多CPU是指简单的多个CPU工作在同一个系统上，多个CPU之间的通讯是通过主板上的总线进行的。从以上原理可知，N个核的CPU，要比N个CPU在一起的工作效率要高（单核性能一致的情况下）。</p><p><strong>三、 处理器结构对并发程序的影响</strong></p><p>对称多处理器是最主要的多核处理器架构。在这种架构中所有的CPU共享一条系统总线（BUS）来连接主存。而每一个核又有自己的一级缓存，相对于BUS对称分布[2]，如下图：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/20140509162415500.jpeg" alt=""></p><p>这种架构在并发程序设计中，大致会引来两个问题，一个是内存可见性，一个是Cache一致性流量。内存可见性属于并发安全的问题，Cache一致性流量引起的是性能上的问题。</p><p><strong>内存可见性</strong>：内存可见性在单处理器或单线程情况下是不会发生的。在一个单线程环境中，一个变量选写入值，然后在没有干涉的情况下读取这个变量，得到的值应该是修改过的值。但是在读和写不在同一个线程中的时候，情况却是不可以预料的。Core1和Core2可能会同时把主存中某个位置的值Load到自己的一级缓存中，而Core1修改了自己一级缓存中的值后，却不更新主存中的值，这样对于Core2来讲，永远看不到Core1对值的修改。在Java程序设计中，用锁，关键字volatile，CAS原子操作可以保证内存可见。</p><p><strong>Cache一致性问题</strong>：指的是在SMP结构中，Core1和Core2同时下载了主存中的值到自己的一级缓存中，Core1修改了值后，会通过总线让Core2中的值失效，Core2发现自己存的值失效后，会再通过总线从主存中得到新值。总线的通信能力是固定的，通过总线使各CPU的一级缓存值数据同步的流量过大，那么总线就会成瓶颈。这种影响属于性能上的影响，减小同步竞争就能减少一致性流量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;转载自：&lt;a href=&quot;http://blog.csdn.net/zolalad/article/details/28393209&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/zola
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取QQ音乐列表音乐</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96QQ%E9%9F%B3%E4%B9%90%E5%88%97%E8%A1%A8%E9%9F%B3%E4%B9%90/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取QQ音乐列表音乐/</id>
    <published>2018-03-14T12:57:31.000Z</published>
    <updated>2018-03-15T08:48:37.038Z</updated>
    
    <content type="html"><![CDATA[<p>最近想爬取一些音乐来实战一下，选择了<em>qq音乐</em>。</p><p><em>qq音乐</em>明显的就是一个动态网页，所以需要抓包了。</p><blockquote><p>不懂的关键词可利用好搜索引擎。</p></blockquote><h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>在此就说说分析的大致过程吧。</p><p>先看看主页：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.08.29.png" alt=""></p><p>我们随便点开一个主题列表：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.09.44.png" alt=""></p><p>因为是动态网页，所以就在这里抓包吧，因为<em>qq音乐</em>是动态网页，需要相关的参数信息才能得到想要的音乐地址，随便以播放一首歌曲为例，如下图1中的歌曲<em>ID</em>，点进去这个看看，即点击播放按钮，发现来到了播放页面，打开我用的<em>Chrome</em>中的开发者工具，里面有我们想要的音乐地址（如下图2所示），图3展示得到列表歌曲的所有信息，需要编程清洗之后才能得到我们想要，在此会在下面的代码中标明。</p><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.37.52.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.41.18.png" alt=""></p><ul><li>图3:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.51.03.png" alt=""></p><p>其中的<em>URL</em>地址，代码中会用到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.48.57.png" alt=""></p><blockquote><p>现在我们可能就会有思路出现了：抓包爬取列表的所有歌曲的<em>ID号</em>以及歌曲信息 <strong>–&gt;</strong> 整合到以<em>ID</em>为基的<em>html</em>地址 <strong>–&gt;</strong> 到播放页面利用<em>beautiful模块</em>爬取相关的音乐地址即可！so easy~</p></blockquote><p><strong>但经过我的测试说明，爬取播放页面的<em>html</em>信息是没有相关的音乐地址的，所以在得到歌曲<em>ID</em>信息，整合到以<em>ID</em>为基的<em>html</em>地址之后，我们还需要对播放页面进行抓包。下面说说如何在播放页面抓包。</strong></p><p>先播放一首歌曲，再进行抓包，发现了这些信息（如下图1），再结合上面的音乐地址分析一下，发现了<em>vkey</em>信息的存在，即每一首歌的<em>vkey</em>信息是不同的，并且经过测试即便是同一个<em>ID</em>，<em>vkey</em>也是一直不断自动变换着的，不过在测试之后可得出结论：<strong>只要得到<em>vkey</em>信息，再整合上面的音乐地址就能抓到音乐信息，并且经过代码编译之后下载下来。</strong></p><blockquote><p>特别说明一下，<strong>下载歌曲的地址以及我们抓包时的<em>URL</em>之中，仅仅有如<em>vkey</em>的不同，或者是一些<em>ID</em>的不同，其他的参数是相同的！</strong>所以我们才能仅仅抓到<em>vkey</em>信息就能方便的下载歌曲，就是这么个意思。不理解的朋友可细心观察一下。</p></blockquote><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.55.11.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.58.13.png" alt=""></p><p>其中的<em>URL</em>地址：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.49.29.png" alt=""></p><hr><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><pre><code>&apos;&apos;&apos;函数目标：爬取QQ音乐列表音乐编写时间：2018-3-15&apos;&apos;&apos;import requestsimport os import timeimport json from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    # 建立目录用于装爬取的音乐    if &apos;QQ音乐列表音乐&apos; not in os.listdir():        os.makedirs(&apos;QQ音乐列表音乐&apos;)    &apos;&apos;&apos;        从URL中添加代理记忆必要的相关的参数以获取歌曲的ID以及歌曲名                                                            &apos;&apos;&apos;    playlist_url = &apos;https://c.y.qq.com/qzone/fcg-bin/fcg_ucc_getcdinfo_byids_cp.fcg&apos;    # 添加页面中的代理信息    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yplayer_open=1; yq_index=0; yq_playschange=0; player_exist=1&apos;,            &apos;referer&apos;:&apos;https://y.qq.com/n/yqq/playlist/3766176211.html&apos;}    # 添加参数信息,有些是非必须的，待研究，有兴趣的可以自己测试    paramter = {                &apos;type&apos;:&apos;1&apos;,                &apos;json&apos;:&apos;1&apos;,                &apos;utf8&apos;:&apos;1&apos;,                &apos;onlysong&apos;:&apos;0&apos;,                &apos;disstid&apos;:&apos;3766176211&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                &apos;jsonpCallback&apos;:&apos;playlistinfoCallback&apos;,  # 值可更改                &apos;loginUin&apos;:&apos;0&apos;,                &apos;hostUin&apos;:&apos;0&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &quot;inCharset&quot;:&apos;utf8&apos;,                &apos;outCharset&apos;:&apos;utf-8&apos;,                &apos;notice&apos;:&apos;0&apos;,                &apos;platform&apos;:&apos;yqq&apos;,                &apos;needNewCode&apos;:&apos;0&apos;,                }    playlist_re = requests.get(url=playlist_url, params=paramter, headers=header)    # 指定编码格式    playlist_re.encoding = &apos;utf-8&apos;    # 改变为python可识别的json格式,进行必要的数据清洗,去掉前面的&apos;jsonpCallback&apos;部分    playlist_info = json.loads(playlist_re.text.lstrip(&apos;playlistinfoCallback(&apos;).rstrip(&apos;)&apos;))    # 指定整体索引    playlist_info1 = playlist_info[&apos;cdlist&apos;][0]    # 先存储歌手的姓名,观察可知，一共有19个索引,因为歌曲本身仅仅有20首，取前20个歌手名    singer_name = []    for num in range(0, 17):        singer_eainfo = playlist_info1[&apos;songlist&apos;][num]        for each_info in singer_eainfo[&apos;singer&apos;]:            singer_name.append(each_info[&apos;name&apos;])    num = 0    # 在循环体系中进行下一步的编写    for each in playlist_info1[&apos;songlist&apos;]:        &apos;&apos;&apos;            在获取歌曲vkey的主URL传入相关的参数得到相关的数据之后进行挖掘，得到vkey信息            其中的参数有些是不必要的，可自由修改，有些是必要的                                                               &apos;&apos;&apos;        key_url = &apos;https://c.y.qq.com/base/fcgi-bin/fcg_music_express_mobile3.fcg&apos;        # 传入相关的代理以及参数        header_1 = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                  &apos;referer&apos;:&apos;https://y.qq.com/portal/player.html&apos;,                  &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yq_index=0; yq_playschange=0; player_exist=1; ts_last=y.qq.com/n/yqq/playlist/3766176211.html; yplayer_open=1&apos;}            paramter_1 = {                    &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                    &apos;jsonpCallback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &quot;loginUin&quot;:&apos;0&apos;,                    &apos;hostUin&apos;:&apos;0&apos;,                    &apos;format&apos;:&apos;json&apos;,                    &apos;inCharset&apos;:&apos;utf8&apos;,                    &apos;outCharset&apos;:&apos;utf-8&apos;,                    &apos;notice&apos;:&apos;0&apos;,                    &apos;platform&apos;:&quot;yqq&quot;,                    &apos;needNewCode&apos;:&apos;0&apos;,                    &apos;cid&apos;:&apos;205361747&apos;,  # 一致必须                    &apos;callback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &apos;uin&apos;:&apos;0&apos;,                    # 传入获取的信息                    &apos;songmid&apos;:each[&apos;songmid&apos;],                    &apos;filename&apos;:&apos;C400&apos; + each[&apos;songmid&apos;] + &apos;.m4a&apos;,                    &apos;guid&apos;:&apos;7344469728&apos;                       }        # 解析得到含有vkey的数据信息，然后进行清洗得到想要的信息        key_re = requests.get(url=key_url, params=paramter_1, headers=header_1)        # 指定编码格式        key_re.encoding = &apos;utf-8&apos;        # 转换为python的json格式，进行简单的清洗        key_info = json.loads(key_re.text.lstrip(&apos;MusicJsonCallback(&apos;).rstrip(&apos;)&apos;))        # 进一步的清洗        data_info = key_info[&apos;data&apos;]        items_info = data_info[&apos;items&apos;][0]        print(&apos;数据采集完成，开始下载任务...&apos;)        # 接下来就是可以下载了        urlretrieve(url=&apos;http://dl.stream.qqmusic.qq.com/C4000041FwTv0Ai3Ku.m4a?vkey=&apos; + items_info[&apos;vkey&apos;] + &apos;&amp;guid=7344469728&amp;uin=0&amp;fromtag=66.mp3&apos;, filename=&apos;QQ音乐列表音乐/&apos; + singer_name[num] + &apos;-&apos; + each[&apos;songname&apos;] + &apos;.mp3&apos;)        print(&apos;正在下载:&apos; + singer_name[num] + &apos;的&apos; + each[&apos;songname&apos;] + &apos;!&apos;)        print(&apos;下载中....&apos;)        print(&apos;下载此歌曲完成！&apos;)        # 跳传到下一个歌手名        num = num + 1        time.sleep(1)    print(&apos;全部下载完成，请在本过程目录下查收！&apos;)</code></pre><blockquote><p><strong>在使用urlretrieve函数时，其中的url参数输入时应当加上格式，如下载视频时加上.mp4,下载音乐时加上.mp3,否则会易出现HTTP 403 错误 – 禁止访问 (Forbidden)</strong></p></blockquote><ul><li>链接：<a href="http://www.checkupdown.com/status/E403_zh.html" target="_blank" rel="external">认识HTTP 403 错误 – 禁止访问 (Forbidden)</a></li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>运行：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.08.01.png" alt=""></p><p>到目录查看：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.09.17.png" alt=""></p><blockquote><p>一切还算是顺利。前段时间想爬取腾讯视频，研究了挺久，没有成功，还需要学习，腾讯的资源都在腾讯云上，我想方式都差不多。</p></blockquote><ul><li>参考：<a href="http://blog.csdn.net/lht_okk/article/details/77206510" target="_blank" rel="external">http://blog.csdn.net/lht_okk/article/details/77206510</a></li></ul><h2 id="后续说明"><a href="#后续说明" class="headerlink" title="后续说明"></a>后续说明</h2><p><strong>经过后来的测试，本代码爬取的思路还是正确的，但是爬取的信息流只能是同一个了…即便是不同的ID…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想爬取一些音乐来实战一下，选择了&lt;em&gt;qq音乐&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;qq音乐&lt;/em&gt;明显的就是一个动态网页，所以需要抓包了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不懂的关键词可利用好搜索引擎。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;分
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取新加坡联合早报新闻小视频</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96%E6%96%B0%E5%8A%A0%E5%9D%A1%E8%81%94%E5%90%88%E6%97%A9%E6%8A%A5%E6%96%B0%E9%97%BB%E5%B0%8F%E8%A7%86%E9%A2%91/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取新加坡联合早报新闻小视频/</id>
    <published>2018-03-14T10:58:25.000Z</published>
    <updated>2018-03-14T11:35:26.345Z</updated>
    
    <content type="html"><![CDATA[<p>位于新加坡的<a href="http://www.zaobao.com" target="_blank" rel="external">联合早报</a>是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…</p><p>今天看了这一篇文章：<a href="http://www.zaobao.com/realtime/china/story20180313-842407" target="_blank" rel="external">女记者提问冗长 人民大会堂部长通道出现“飙戏”一幕</a></p><blockquote><p>非常有意思，想收藏其中的视频，于是想到了用<em>python</em>爬取好了。</p></blockquote><p>这个新闻网站明显是一个动态网站啊，两种方式：</p><ul><li>第一种：通过抓包，如下可得知相关的<em>video</em>信息</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.30.png" alt=""></p><ul><li>第二种：网站自带的连接</li></ul><p>如下操作，点击视频中<em>share</em>，可发现资源地址</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.57.png" alt=""></p><blockquote><p>右上角的<em>share</em>。</p></blockquote><p>其中有地址信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.04.10.png" alt=""></p><p>在获取的地址前加上<em>http:</em>简单测试一下:</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.07.10.png" alt=""></p><blockquote><p>是正确的，网站真的很贴心呢～</p></blockquote><p>以刚刚的地址输入爬取下来的代码：</p><pre><code>from urllib.request import urlretrieve if __name__ == &apos;__main__&apos;:    print(&apos;开始下载...&apos;)    urlretrieve(url=&apos;http://players.brightcove.net/4802324430001/H1dr7zTWz_default/index.html?videoId=5750255765001.mp4&apos;, filename=&apos;两会小视频.mp4&apos;)    print(&apos;下载完成！&apos;)</code></pre><p><strong>结果：发现可在网上播放的视频下载之后却不能播放…占用的内存才几百kb…这一看就知道地址是错的…</strong></p><hr><p>经过上面网址播放的连接，再次进行抓包，打开相关的网页意外发现了<em>mp4格式</em>的连接：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.28.05.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.30.37.png" alt=""></p><hr><p><strong>将此链接替换掉上面代码中的URL地址，发现可以了（如下图），完工。</strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.31.51.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位于新加坡的&lt;a href=&quot;http://www.zaobao.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;联合早报&lt;/a&gt;是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…&lt;/p&gt;
&lt;p&gt;今
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取动态网页图片</title>
    <link href="https://liujunjie11.github.io/2018/03/12/python3%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/"/>
    <id>https://liujunjie11.github.io/2018/03/12/python3爬取动态网页图片/</id>
    <published>2018-03-12T12:16:32.000Z</published>
    <updated>2018-03-12T12:44:38.949Z</updated>
    
    <content type="html"><![CDATA[<p>爬取的<em>URL地址</em>:<a href="https://unsplash.com/" target="_blank" rel="external">https://unsplash.com/</a></p><blockquote><p>这是一个优美图片地址，往下拉就可以出来更多的图片，这显然是一个动态网页呀…</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>关于详细分析以及我的参考可见：<a href="http://blog.csdn.net/c406495762/article/details/78123502" target="_blank" rel="external">http://blog.csdn.net/c406495762/article/details/78123502</a></p><blockquote><p><strong>因为分析方向一致，我就不在此说了，我跟这位博主的工具有一些出入，实际上用<em>Chrome</em>分析已经足够了。</strong></p></blockquote><p>##代码</p><p>这是我后来自己写的代码，比上面博主的简短一些，亦可参考参考。</p><pre><code>&apos;&apos;&apos;    函数目标：    爬取动态网页的图片&apos;&apos;&apos;import requestsimport jsonfrom urllib.request import urlretrieveimport osimport timeif __name__ == &apos;__main__&apos;:    url = &apos;https://unsplash.com/napi/feeds/home&apos;    &apos;&apos;&apos;    添加需要的代理:    authorization证书配置,有时网站需要此类的代理html信息才会出来...    json格式分析js页面的利器，有时用js渲染出来的页面，要注意观察其URL及相对准确的信息    &apos;&apos;&apos;    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;authorization&apos;:&apos;Client-ID c94869b36aa272dd62dfaeefed769d4115fb3189a9xxxxxxxxxxx&apos;}    re = requests.get(url=url, headers=header)    re.encoding = &apos;utf-8&apos;    # 通过分析易知一页有包括多张图片的ID链接，可用python的json格式处理解析    json_info = json.loads(re.text)    # 建立一个空的列表用于装ID信息    list_id = []    if &apos;优美图片集&apos; not in os.listdir():        os.makedirs(&apos;优美图片集&apos;)    for each in json_info[&apos;photos&apos;]:        list_id.append(each[&apos;id&apos;])    # 利用urlretrieve函数一一下载，设置延迟    for i in range(0, len(list_id)):        print(&apos;开始下载指定页面中的第%d张&apos; % (i + 1))        urlretrieve(url=&apos;https://unsplash.com/photos/&apos; + list_id[i] + &apos;/download?force=true.jpg&apos;, filename=&apos;优美图片集/&apos; + &apos;系列%d.jpg&apos; % i)    print(&apos;下载完成！请查收...&apos;)    </code></pre><blockquote><p>虽然说我的代码简短一点，不过我还是支持面对对象模式编程的，方便以后的学习，也是对自己的一种考验。</p></blockquote><p>运行之后在本工程目录可见：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.16.51.png" alt=""></p><p>最后再补充说明一下：</p><p>每一次的拉取新的图片时，进行抓包，得知新的图片ID以及一个页面，通过分析此页面便可得到图片相关的信息，进而进行下载保存了（如下图）。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.12.58.png" alt=""></p><hr><p>简单说说当时的情况，在参考了上面博主的分析过程后，利用了<em>Chrome下载器</em>的下载发现了图片的信息，然后我用<em>urlretrieve函数</em>单张下载的测试，发现成功了…附上代码。</p><pre><code>from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    urlretrieve(&apos;https://unsplash.com/photos/NrflUuJJK0I/download?force=true.jpg&apos;, &apos;tu.jpg&apos;)         print(&apos;下载完成！&apos;)</code></pre><blockquote><p>不过从来都没有见过这种<em>src信息</em>…算是开了眼界，长了知识啦。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;爬取的&lt;em&gt;URL地址&lt;/em&gt;:&lt;a href=&quot;https://unsplash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://unsplash.com/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是一个优美图片地址
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>学习python3网络爬虫的总结</title>
    <link href="https://liujunjie11.github.io/2018/03/12/%E5%AD%A6%E4%B9%A0python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>https://liujunjie11.github.io/2018/03/12/学习python3网络爬虫的总结/</id>
    <published>2018-03-12T11:01:41.000Z</published>
    <updated>2018-03-13T13:18:32.382Z</updated>
    
    <content type="html"><![CDATA[<ul><li>有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…</li></ul><ul><li>在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对可以达到目的的每一点进行抓包分析，挖掘其中的信息。另外，在爬取网页信息中，有一些反爬虫的或者是必须加入一些参数代理才可得到需要的信息等，俊需要一个点一个步骤的去分析。</li></ul><ul><li><p>在爬取网页的过程中，编写代码时，检查代码的函数方法的准确性，少一个‘s’与多一个‘s’，都是让人头疼的问题。</p></li><li><p>在编写代码的过程中，追求最好的解决方案，习惯于用面向对象来编写代码，便于以后的学习。</p></li><li><p>编写爬虫代码，要让其像是一个浏览器一般的去爬取数据，所以代理之类的应当要严谨使用。</p></li><li><p>分析html信息，善于用<strong>正则表达式</strong>解决一些代码与文字的混合信息。</p></li><li><p>对URL的分析到位亦然很重要。</p></li><li><p>学会快速判断是否为动态网页。</p></li><li><p><em>python</em> 编程缩进很重要！！！！</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Mac触摸板小问题记录</title>
    <link href="https://liujunjie11.github.io/2018/03/12/Mac%E8%A7%A6%E6%91%B8%E6%9D%BF%E5%B0%8F%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2018/03/12/Mac触摸板小问题记录/</id>
    <published>2018-03-12T02:40:07.000Z</published>
    <updated>2018-03-12T02:42:35.506Z</updated>
    
    <content type="html"><![CDATA[<p>今早<em>触摸板</em>的<em>三指点击</em>功能不灵了，有点急…</p><p>解决：<strong>重启。</strong></p><blockquote><p>一般电脑上的小问题重启之后可能就会得到解决了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今早&lt;em&gt;触摸板&lt;/em&gt;的&lt;em&gt;三指点击&lt;/em&gt;功能不灵了，有点急…&lt;/p&gt;
&lt;p&gt;解决：&lt;strong&gt;重启。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般电脑上的小问题重启之后可能就会得到解决了。&lt;/p&gt;
&lt;/blockquote&gt;

      
    
    </summary>
    
      <category term="笔记" scheme="https://liujunjie11.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="笔记" scheme="https://liujunjie11.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>利用python3网络爬虫爬取成绩</title>
    <link href="https://liujunjie11.github.io/2018/03/10/%E5%88%A9%E7%94%A8python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E6%88%90%E7%BB%A9/"/>
    <id>https://liujunjie11.github.io/2018/03/10/利用python3网络爬虫爬取成绩/</id>
    <published>2018-03-10T14:07:18.000Z</published>
    <updated>2018-03-11T02:38:31.390Z</updated>
    
    <content type="html"><![CDATA[<p>因为最近在学习<em>python3网络爬虫</em>，想自己写一些小程序来实战一下。爬的是<em>URP教务网</em>。</p><p>一开始的思路是利用<em>beautiful</em>模块来进行爬取相关的<em>html信息</em>，直接来得到需要的信息。结果发现程序运行不通…</p><p>后来查了一下，发现用<em><a href="https://docs.python.org/3/library/re.html" target="_blank" rel="external">re模块</a></em>好啊…配合<em><a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a></em>那是相当简单…</p><p>下面开始分析，代码编写阶段。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="学习模拟登陆"><a href="#学习模拟登陆" class="headerlink" title="学习模拟登陆"></a>学习模拟登陆</h2><p>这是第一步，有两种简单的方法，可直接参考的链接：<a href="http://blog.csdn.net/JoeHF/article/details/48424335" target="_blank" rel="external">参考链接在此</a></p><p>一种是查看抓的包中的：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.02.png" alt=""></p><p>代码：</p><pre><code>import requestsf __name__ == &apos;__main__&apos;:    # 登陆页面的URL,此处的URL为登陆页面的URL以及在登陆之后的header中的request均可    url = &apos;http://60.219.165.24/loginAction.do&apos;    # 设置相关的代理以及在登陆之后的fordate信息        # &apos;Connection&apos;: &apos;keep-alive&apos; ：保证持续连接    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    login_fordate = {&apos;zjh&apos;:&apos;20160xxxxx&apos;,                    &apos;mm&apos;:&apos;x&apos;}    # 利用session方法爬取请求    s = requests.session()    response = s.post(url, data=login_fordate, headers=header)     # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>不懂可查看文档：<br><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">requests文档</a></p></blockquote><hr><p>第二种是查看包中的<em>cookie信息：</em></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.21.png" alt=""></p><p>对应的实现模拟代码：</p><pre><code>import requestsif __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;hhttp://60.219.165.24/loginAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers）    # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>相关的信息我用<em>x</em>换掉了。不懂的朋友可以看文档，查资料咯。再次说明一下，<em>URL部分**</em>可以是登陆界面的，也可以是登陆之后的URL，经过测试两者均可。**</p></blockquote><h2 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h2><p>到了分析阶段了。</p><p>打开我的<em>Chrome浏览器</em>，开始分析每个链接的<em>html信息</em>，看看有没有我想要的信息。</p><p>根据下面的操作得到了<em>本学期的成绩查询</em>的相关的超链接。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-48-17.gif" alt=""></p><p>再结合下面两张图的分析易知：<em>本学期成绩查询</em>的超链接为：<strong><a href="http://60.219.165.24//bxqcjcxAction.do" target="_blank" rel="external">http://60.219.165.24//bxqcjcxAction.do</a></strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.01.png" alt=""></p><blockquote><p>超链接部分。</p></blockquote><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.25.png" alt=""></p><blockquote><p>主URL部分。</p></blockquote><p>为了保证准确性，再向某成绩采取相应的操作（如下图所示），再往上看看，就发现它是<em>本学期成绩查询</em>的一部分。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-49-16.gif" alt=""></p><hr><blockquote><p>接下来就是编写代码了，以上若是有不懂的地方，还需要利用搜索引擎多多查询哟。</p></blockquote><h2 id="测试代码部分"><a href="#测试代码部分" class="headerlink" title="测试代码部分"></a>测试代码部分</h2><p>编写测试代码，爬取网页<em>html信息</em>：</p><pre><code>import requests    if __name__ == &apos;__main__&apos;:        # 登陆页面url        url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;        # 设置代理相关        headers = {                &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,                &apos;Connection&apos;: &apos;keep-alive&apos;                 }        # 利用session爬取请求,之后可方便的get与post        s = requests.session()        response = s.post(url=url, headers=headers）        # 设置成网页对应的编码格式        response.encoding = &apos;GB2312&apos;        # 查看相关的网页内容        print(response.text)</code></pre><p>运行查看效果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.05.02.png" alt=""></p><blockquote><p>成功得到了想要的<em>html信息</em>，接下来利用<em>正则表达式</em>选想要的部分即可。<strong>在这里要说明一下，不可用<em>ForDate</em>的那个模拟登陆，经过测试发现返回的是错误信息…所以以后老老实实用<em>cookie</em>模拟更为靠谱一点…</strong></p></blockquote><h2 id="完整代码部分"><a href="#完整代码部分" class="headerlink" title="完整代码部分"></a>完整代码部分</h2><pre><code>import requestsimport re if __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSESSIONID=xxxxxxxxxxx_gUJ4aiw&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers)    # 设置成网页对应的编码格式    response.encoding = &apos;GB2312&apos;    # 设置成为符合需要的表达式以及模式为&apos;任意匹配模式&apos;    pattern = re.compile(&apos;&lt;tr.*?class=&quot;even&quot;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td align=&quot;center&quot;&gt;(.*?)&lt;/td&gt;.*?&amp;npsb;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td&gt;align=&quot;center&quot;&gt;(.*?)&amp;npsb;&lt;/P&gt;&lt;/td&gt;.*?&amp;npsb;&lt;/P&gt;&apos;,                             re.S)    # 成绩信息采集    grades = re.findall(pattern, response.text)    # 输出对应的课程信息    for each_grades in grades:        print(&apos;课程名称：&apos; + each_grades[0] + &apos;分数：&apos; + each_grades[1])</code></pre><p>关于<em>正则表达式</em>的解说可结合<a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a>此篇文章学习。</p><p>简单解说一下，先贴出来<em>html信息</em>是怎么样的。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.12.41.png" alt=""></p><blockquote><p>只截取了一部分，可自行了解。</p></blockquote><p>解说：</p><ol><li><p>.<em>? 是一个固定的搭配，.和</em>代表可以匹配任意无限多个字符，加上？表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。</p></li><li><p>(.<em>?)代表一个分组，在这个正则表达式中我们匹配了两个分组，在后面的遍历grades中，grade[0]就代表第一个(.</em>?)所指代的内容，grade[1]就代表第二个(.*?)所指代的内容，以此类推。</p></li><li><p>re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。</p></li></ol><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>参考文章列表：</li></ul><ol><li><p><a href="http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/" target="_blank" rel="external">参考一</a></p></li><li><p><a href="http://blog.csdn.net/c406495762/article/details/69817490" target="_blank" rel="external">参考二</a></p></li><li><p><a href="http://www.cnblogs.com/greenteemo/p/6629126.html" target="_blank" rel="external">参考三</a></p></li></ol><blockquote><p>还有一些文档，均可在官网上看到，利用搜索引擎即可。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为最近在学习&lt;em&gt;python3网络爬虫&lt;/em&gt;，想自己写一些小程序来实战一下。爬的是&lt;em&gt;URP教务网&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;一开始的思路是利用&lt;em&gt;beautiful&lt;/em&gt;模块来进行爬取相关的&lt;em&gt;html信息&lt;/em&gt;，直接来得到需要的信息。结果发
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
