<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LXiHa`Notes</title>
  
  <subtitle>The House Belong to Love and Freedom.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liujunjie11.github.io/"/>
  <updated>2018-03-14T11:35:26.345Z</updated>
  <id>https://liujunjie11.github.io/</id>
  
  <author>
    <name>刘俊</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用python3爬取新加坡联合早报新闻小视频</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96%E6%96%B0%E5%8A%A0%E5%9D%A1%E8%81%94%E5%90%88%E6%97%A9%E6%8A%A5%E6%96%B0%E9%97%BB%E5%B0%8F%E8%A7%86%E9%A2%91/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取新加坡联合早报新闻小视频/</id>
    <published>2018-03-14T10:58:25.000Z</published>
    <updated>2018-03-14T11:35:26.345Z</updated>
    
    <content type="html"><![CDATA[<p>位于新加坡的<a href="http://www.zaobao.com" target="_blank" rel="external">联合早报</a>是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…</p><p>今天看了这一篇文章：<a href="http://www.zaobao.com/realtime/china/story20180313-842407" target="_blank" rel="external">女记者提问冗长 人民大会堂部长通道出现“飙戏”一幕</a></p><blockquote><p>非常有意思，想收藏其中的视频，于是想到了用<em>python</em>爬取好了。</p></blockquote><p>这个新闻网站明显是一个动态网站啊，两种方式：</p><ul><li>第一种：通过抓包，如下可得知相关的<em>video</em>信息</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.30.png" alt=""></p><ul><li>第二种：网站自带的连接</li></ul><p>如下操作，点击视频中<em>share</em>，可发现资源地址</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.57.png" alt=""></p><blockquote><p>右上角的<em>share</em>。</p></blockquote><p>其中有地址信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.04.10.png" alt=""></p><p>在获取的地址前加上<em>http:</em>简单测试一下:</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.07.10.png" alt=""></p><blockquote><p>是正确的，网站真的很贴心呢～</p></blockquote><p>以刚刚的地址输入爬取下来的代码：</p><pre><code>from urllib.request import urlretrieve if __name__ == &apos;__main__&apos;:    print(&apos;开始下载...&apos;)    urlretrieve(url=&apos;http://players.brightcove.net/4802324430001/H1dr7zTWz_default/index.html?videoId=5750255765001.mp4&apos;, filename=&apos;两会小视频.mp4&apos;)    print(&apos;下载完成！&apos;)</code></pre><p><strong>结果：发现可在网上播放的视频下载之后却不能播放…占用的内存才几百kb…这一看就知道地址是错的…</strong></p><hr><p>经过上面网址播放的连接，再次进行抓包，打开相关的网页意外发现了<em>mp4格式</em>的连接：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.28.05.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.30.37.png" alt=""></p><hr><p><strong>将此链接替换掉上面代码中的URL地址，发现可以了（如下图），完工。</strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.31.51.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位于新加坡的&lt;a href=&quot;http://www.zaobao.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;联合早报&lt;/a&gt;是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…&lt;/p&gt;
&lt;p&gt;今
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取动态网页图片</title>
    <link href="https://liujunjie11.github.io/2018/03/12/python3%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/"/>
    <id>https://liujunjie11.github.io/2018/03/12/python3爬取动态网页图片/</id>
    <published>2018-03-12T12:16:32.000Z</published>
    <updated>2018-03-12T12:44:38.949Z</updated>
    
    <content type="html"><![CDATA[<p>爬取的<em>URL地址</em>:<a href="https://unsplash.com/" target="_blank" rel="external">https://unsplash.com/</a></p><blockquote><p>这是一个优美图片地址，往下拉就可以出来更多的图片，这显然是一个动态网页呀…</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>关于详细分析以及我的参考可见：<a href="http://blog.csdn.net/c406495762/article/details/78123502" target="_blank" rel="external">http://blog.csdn.net/c406495762/article/details/78123502</a></p><blockquote><p><strong>因为分析方向一致，我就不在此说了，我跟这位博主的工具有一些出入，实际上用<em>Chrome</em>分析已经足够了。</strong></p></blockquote><p>##代码</p><p>这是我后来自己写的代码，比上面博主的简短一些，亦可参考参考。</p><pre><code>&apos;&apos;&apos;    函数目标：    爬取动态网页的图片&apos;&apos;&apos;import requestsimport jsonfrom urllib.request import urlretrieveimport osimport timeif __name__ == &apos;__main__&apos;:    url = &apos;https://unsplash.com/napi/feeds/home&apos;    &apos;&apos;&apos;    添加需要的代理:    authorization证书配置,有时网站需要此类的代理html信息才会出来...    json格式分析js页面的利器，有时用js渲染出来的页面，要注意观察其URL及相对准确的信息    &apos;&apos;&apos;    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;authorization&apos;:&apos;Client-ID c94869b36aa272dd62dfaeefed769d4115fb3189a9xxxxxxxxxxx&apos;}    re = requests.get(url=url, headers=header)    re.encoding = &apos;utf-8&apos;    # 通过分析易知一页有包括多张图片的ID链接，可用python的json格式处理解析    json_info = json.loads(re.text)    # 建立一个空的列表用于装ID信息    list_id = []    if &apos;优美图片集&apos; not in os.listdir():        os.makedirs(&apos;优美图片集&apos;)    for each in json_info[&apos;photos&apos;]:        list_id.append(each[&apos;id&apos;])    # 利用urlretrieve函数一一下载，设置延迟    for i in range(0, len(list_id)):        print(&apos;开始下载指定页面中的第%d张&apos; % (i + 1))        urlretrieve(url=&apos;https://unsplash.com/photos/&apos; + list_id[i] + &apos;/download?force=true.jpg&apos;, filename=&apos;优美图片集/&apos; + &apos;系列%d.jpg&apos; % i)    print(&apos;下载完成！请查收...&apos;)    </code></pre><blockquote><p>虽然说我的代码简短一点，不过我还是支持面对对象模式编程的，方便以后的学习，也是对自己的一种考验。</p></blockquote><p>运行之后在本工程目录可见：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.16.51.png" alt=""></p><p>最后再补充说明一下：</p><p>每一次的拉取新的图片时，进行抓包，得知新的图片ID以及一个页面，通过分析此页面便可得到图片相关的信息，进而进行下载保存了（如下图）。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.12.58.png" alt=""></p><hr><p>简单说说当时的情况，在参考了上面博主的分析过程后，利用了<em>Chrome下载器</em>的下载发现了图片的信息，然后我用<em>urlretrieve函数</em>单张下载的测试，发现成功了…附上代码。</p><pre><code>from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    urlretrieve(&apos;https://unsplash.com/photos/NrflUuJJK0I/download?force=true.jpg&apos;, &apos;tu.jpg&apos;)         print(&apos;下载完成！&apos;)</code></pre><blockquote><p>不过从来都没有见过这种<em>src信息</em>…算是开了眼界，长了知识啦。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;爬取的&lt;em&gt;URL地址&lt;/em&gt;:&lt;a href=&quot;https://unsplash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://unsplash.com/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是一个优美图片地址
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>学习python3网络爬虫的总结</title>
    <link href="https://liujunjie11.github.io/2018/03/12/%E5%AD%A6%E4%B9%A0python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>https://liujunjie11.github.io/2018/03/12/学习python3网络爬虫的总结/</id>
    <published>2018-03-12T11:01:41.000Z</published>
    <updated>2018-03-13T13:18:32.382Z</updated>
    
    <content type="html"><![CDATA[<ul><li>有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…</li></ul><ul><li>在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对可以达到目的的每一点进行抓包分析，挖掘其中的信息。另外，在爬取网页信息中，有一些反爬虫的或者是必须加入一些参数代理才可得到需要的信息等，俊需要一个点一个步骤的去分析。</li></ul><ul><li><p>在爬取网页的过程中，编写代码时，检查代码的函数方法的准确性，少一个‘s’与多一个‘s’，都是让人头疼的问题。</p></li><li><p>在编写代码的过程中，追求最好的解决方案，习惯于用面向对象来编写代码，便于以后的学习。</p></li><li><p>编写爬虫代码，要让其像是一个浏览器一般的去爬取数据，所以代理之类的应当要严谨使用。</p></li><li><p>分析html信息，善于用<strong>正则表达式</strong>解决一些代码与文字的混合信息。</p></li><li><p>对URL的分析到位亦然很重要。</p></li><li><p>学会快速判断是否为动态网页。</p></li><li><p><em>python</em> 编程缩进很重要！！！！</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Mac触摸板小问题记录</title>
    <link href="https://liujunjie11.github.io/2018/03/12/Mac%E8%A7%A6%E6%91%B8%E6%9D%BF%E5%B0%8F%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2018/03/12/Mac触摸板小问题记录/</id>
    <published>2018-03-12T02:40:07.000Z</published>
    <updated>2018-03-12T02:42:35.506Z</updated>
    
    <content type="html"><![CDATA[<p>今早<em>触摸板</em>的<em>三指点击</em>功能不灵了，有点急…</p><p>解决：<strong>重启。</strong></p><blockquote><p>一般电脑上的小问题重启之后可能就会得到解决了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今早&lt;em&gt;触摸板&lt;/em&gt;的&lt;em&gt;三指点击&lt;/em&gt;功能不灵了，有点急…&lt;/p&gt;
&lt;p&gt;解决：&lt;strong&gt;重启。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般电脑上的小问题重启之后可能就会得到解决了。&lt;/p&gt;
&lt;/blockquote&gt;

      
    
    </summary>
    
      <category term="笔记" scheme="https://liujunjie11.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="笔记" scheme="https://liujunjie11.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>利用python3网络爬虫爬取成绩</title>
    <link href="https://liujunjie11.github.io/2018/03/10/%E5%88%A9%E7%94%A8python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E6%88%90%E7%BB%A9/"/>
    <id>https://liujunjie11.github.io/2018/03/10/利用python3网络爬虫爬取成绩/</id>
    <published>2018-03-10T14:07:18.000Z</published>
    <updated>2018-03-11T02:38:31.390Z</updated>
    
    <content type="html"><![CDATA[<p>因为最近在学习<em>python3网络爬虫</em>，想自己写一些小程序来实战一下。爬的是<em>URP教务网</em>。</p><p>一开始的思路是利用<em>beautiful</em>模块来进行爬取相关的<em>html信息</em>，直接来得到需要的信息。结果发现程序运行不通…</p><p>后来查了一下，发现用<em><a href="https://docs.python.org/3/library/re.html" target="_blank" rel="external">re模块</a></em>好啊…配合<em><a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a></em>那是相当简单…</p><p>下面开始分析，代码编写阶段。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="学习模拟登陆"><a href="#学习模拟登陆" class="headerlink" title="学习模拟登陆"></a>学习模拟登陆</h2><p>这是第一步，有两种简单的方法，可直接参考的链接：<a href="http://blog.csdn.net/JoeHF/article/details/48424335" target="_blank" rel="external">参考链接在此</a></p><p>一种是查看抓的包中的：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.02.png" alt=""></p><p>代码：</p><pre><code>import requestsf __name__ == &apos;__main__&apos;:    # 登陆页面的URL,此处的URL为登陆页面的URL以及在登陆之后的header中的request均可    url = &apos;http://60.219.165.24/loginAction.do&apos;    # 设置相关的代理以及在登陆之后的fordate信息        # &apos;Connection&apos;: &apos;keep-alive&apos; ：保证持续连接    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    login_fordate = {&apos;zjh&apos;:&apos;20160xxxxx&apos;,                    &apos;mm&apos;:&apos;x&apos;}    # 利用session方法爬取请求    s = requests.session()    response = s.post(url, data=login_fordate, headers=header)     # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>不懂可查看文档：<br><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">requests文档</a></p></blockquote><hr><p>第二种是查看包中的<em>cookie信息：</em></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.21.png" alt=""></p><p>对应的实现模拟代码：</p><pre><code>import requestsif __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;hhttp://60.219.165.24/loginAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers）    # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>相关的信息我用<em>x</em>换掉了。不懂的朋友可以看文档，查资料咯。再次说明一下，<em>URL部分**</em>可以是登陆界面的，也可以是登陆之后的URL，经过测试两者均可。**</p></blockquote><h2 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h2><p>到了分析阶段了。</p><p>打开我的<em>Chrome浏览器</em>，开始分析每个链接的<em>html信息</em>，看看有没有我想要的信息。</p><p>根据下面的操作得到了<em>本学期的成绩查询</em>的相关的超链接。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-48-17.gif" alt=""></p><p>再结合下面两张图的分析易知：<em>本学期成绩查询</em>的超链接为：<strong><a href="http://60.219.165.24//bxqcjcxAction.do" target="_blank" rel="external">http://60.219.165.24//bxqcjcxAction.do</a></strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.01.png" alt=""></p><blockquote><p>超链接部分。</p></blockquote><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.25.png" alt=""></p><blockquote><p>主URL部分。</p></blockquote><p>为了保证准确性，再向某成绩采取相应的操作（如下图所示），再往上看看，就发现它是<em>本学期成绩查询</em>的一部分。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-49-16.gif" alt=""></p><hr><blockquote><p>接下来就是编写代码了，以上若是有不懂的地方，还需要利用搜索引擎多多查询哟。</p></blockquote><h2 id="测试代码部分"><a href="#测试代码部分" class="headerlink" title="测试代码部分"></a>测试代码部分</h2><p>编写测试代码，爬取网页<em>html信息</em>：</p><pre><code>import requests    if __name__ == &apos;__main__&apos;:        # 登陆页面url        url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;        # 设置代理相关        headers = {                &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,                &apos;Connection&apos;: &apos;keep-alive&apos;                 }        # 利用session爬取请求,之后可方便的get与post        s = requests.session()        response = s.post(url=url, headers=headers）        # 设置成网页对应的编码格式        response.encoding = &apos;GB2312&apos;        # 查看相关的网页内容        print(response.text)</code></pre><p>运行查看效果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.05.02.png" alt=""></p><blockquote><p>成功得到了想要的<em>html信息</em>，接下来利用<em>正则表达式</em>选想要的部分即可。<strong>在这里要说明一下，不可用<em>ForDate</em>的那个模拟登陆，经过测试发现返回的是错误信息…所以以后老老实实用<em>cookie</em>模拟更为靠谱一点…</strong></p></blockquote><h2 id="完整代码部分"><a href="#完整代码部分" class="headerlink" title="完整代码部分"></a>完整代码部分</h2><pre><code>import requestsimport re if __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSESSIONID=xxxxxxxxxxx_gUJ4aiw&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers)    # 设置成网页对应的编码格式    response.encoding = &apos;GB2312&apos;    # 设置成为符合需要的表达式以及模式为&apos;任意匹配模式&apos;    pattern = re.compile(&apos;&lt;tr.*?class=&quot;even&quot;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td align=&quot;center&quot;&gt;(.*?)&lt;/td&gt;.*?&amp;npsb;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td&gt;align=&quot;center&quot;&gt;(.*?)&amp;npsb;&lt;/P&gt;&lt;/td&gt;.*?&amp;npsb;&lt;/P&gt;&apos;,                             re.S)    # 成绩信息采集    grades = re.findall(pattern, response.text)    # 输出对应的课程信息    for each_grades in grades:        print(&apos;课程名称：&apos; + each_grades[0] + &apos;分数：&apos; + each_grades[1])</code></pre><p>关于<em>正则表达式</em>的解说可结合<a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a>此篇文章学习。</p><p>简单解说一下，先贴出来<em>html信息</em>是怎么样的。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.12.41.png" alt=""></p><blockquote><p>只截取了一部分，可自行了解。</p></blockquote><p>解说：</p><ol><li><p>.<em>? 是一个固定的搭配，.和</em>代表可以匹配任意无限多个字符，加上？表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。</p></li><li><p>(.<em>?)代表一个分组，在这个正则表达式中我们匹配了两个分组，在后面的遍历grades中，grade[0]就代表第一个(.</em>?)所指代的内容，grade[1]就代表第二个(.*?)所指代的内容，以此类推。</p></li><li><p>re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。</p></li></ol><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>参考文章列表：</li></ul><ol><li><p><a href="http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/" target="_blank" rel="external">参考一</a></p></li><li><p><a href="http://blog.csdn.net/c406495762/article/details/69817490" target="_blank" rel="external">参考二</a></p></li><li><p><a href="http://www.cnblogs.com/greenteemo/p/6629126.html" target="_blank" rel="external">参考三</a></p></li></ol><blockquote><p>还有一些文档，均可在官网上看到，利用搜索引擎即可。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为最近在学习&lt;em&gt;python3网络爬虫&lt;/em&gt;，想自己写一些小程序来实战一下。爬的是&lt;em&gt;URP教务网&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;一开始的思路是利用&lt;em&gt;beautiful&lt;/em&gt;模块来进行爬取相关的&lt;em&gt;html信息&lt;/em&gt;，直接来得到需要的信息。结果发
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于计算机基础知识（一）</title>
    <link href="https://liujunjie11.github.io/2018/03/08/%E5%85%B3%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://liujunjie11.github.io/2018/03/08/关于计算机基础知识（一）/</id>
    <published>2018-03-08T00:56:07.000Z</published>
    <updated>2018-03-08T02:09:56.897Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>平时有些兴趣想要了解一些关于<em>计算机</em>的基础知识，我就想着为何不将其作为一个笔记记录在博客上呢？以后想从事与<em>计算机</em>相关的工作，了解基本的<em>计算机知识</em>必不可少，因为时间，精力的有限，我会将一些认为重要的或者是必要的基础知识记录在此，打算作为一个系列在此展示了，希望能帮助更多的人。</p></blockquote><h2 id="编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解"><a href="#编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解" class="headerlink" title="编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解"></a>编程语言知识：关于<em>强类型语言</em>与<em>弱类型语言</em>，语言的<em>动静态</em>区分以及<em>解释性语言</em>与<em>编译性语言</em>的理解</h2><blockquote><p>可以一张图大概的简单理解一下关于<em>强类型语言</em>与<em>弱类型语言</em>，语言的<em>动静态</em>（下图来源于参考一链接）</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/1283539-31968e5f19abed4b.jpg" alt=""></p><blockquote><p>那么我们常见的语言分类：</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/1283539-1820df734cf34260.png" alt=""></p><hr><ul><li>简单总结<em>强类型语言</em>与<em>弱类型语言</em>：</li></ul><ol><li><p>编译时前者需要赋予变量对应的数据类型，后者可不必必须。（如<em>c语言</em>与<em>python</em>的编译时）</p></li><li><p>前者需要改变数据类型时需要相关的函数方法强制改变，后者可忽略。（如在<em>c语言</em>强制要求更改类型时的<strong>（double）变量</strong>）</p></li></ol><ul><li>简单总结语言的<em>动静态</em>区别：</li></ul><ol><li><p><em>动态性语言</em>在编译器上编译时不提供代码数据类型的指错，仅仅是在运行时指出，而静态则是在编译器上编译时就提供代码数据类型的指错。（如<strong><em>int a=’a’;</em></strong>时编译器马上会在一旁提示错误时的便可理解为静态语言，如<em>c语言</em>）</p></li><li><p><em>动态性</em>较于<em>静态性</em>更具有严谨性，可读性。</p></li></ol><ul><li><em>解释性语言</em>与<em>编译性语言</em>的理解（因为在参考4的说明较为详细了，我就直接复制相关的内容好了…）</li></ul><p>首先，我们编程都是用的高级语言(写汇编和机器语言的大牛们除外)，计算机不能直接理解高级语言，只能理解和运行机器语言，所以必须要把高级语言翻译成机器语言，计算机才能运行高级语言所编写的程序。</p><p>说到翻译，其实翻译的方式有两种，一个是编译，一个是解释。两种方式只是翻译的时间不同。</p><p>用编译型语言写的程序执行之前，需要一个专门的编译过程，通过编译系统（不仅仅只是通过编译器，编译器只是编译系统的一部分）把高级语言翻译成机器语言（具体翻译过程可以参看下图），把源高级程序编译成为机器语言文件，比如windows下的exe文件。以后就可以直接运行而不需要编译了，因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高，但也不能一概而论，部分解释型语言的解释器通过在运行时动态优化代码，甚至能够使解释型语言的性能超过编译型语言。<br><img src="http://owudg3xs2.bkt.clouddn.com/20131124170842718.png" alt=""></p><blockquote><p>一个完整的编译系统与 一个用C编写的程序hello.c的编译过程 </p></blockquote><p>解释则不同，解释型语言编写的程序不需要编译。解释型语言在运行的时候才翻译，比如VB语言，在执行的时候，专门有一个解释器能够将VB语言翻译成机器语言，每个语句都是执行的时候才翻译。这样解释型语言每执行一次就要翻译一次，效率比较低。</p><p>编译型与解释型，两者各有利弊。前者由于程序执行速度快，同等条件下对系统要求较低，因此像开发操作系统、大型应用程序、数据库系统等时都采用它，像C/C++、Pascal/Object Pascal（Delphi）等都是编译语言，而一些网页脚本、服务器脚本及辅助开发接口这样的对速度要求不高、对不同系统平台间的兼容性有一定要求的程序则通常使用解释性语言，如JavaScript、VBScript、Perl、Python、Ruby、MATLAB 等等。</p><p>但随着硬件的升级和设计思想的变革，编译型和解释型语言越来越笼统，主要体现在一些新兴的高级语言上，而解释型语言的自身特点也使得编译器厂商愿意花费更多成本来优化解释器，解释型语言性能超过编译型语言也是必然的。</p><ul><li><strong>以下更为详细的参考：</strong></li></ul><p><a href="https://www.jianshu.com/p/336f19772046" target="_blank" rel="external">参考1</a></p><p><a href="https://www.zhihu.com/question/19918532" target="_blank" rel="external">参考2</a></p><p><a href="https://zh.wikipedia.org/wiki/強弱型別" target="_blank" rel="external">参考3</a></p><p><a href="http://blog.csdn.net/zhu_xun/article/details/16921413" target="_blank" rel="external">参考4</a></p><h2 id="计算机基础知识推荐图书"><a href="#计算机基础知识推荐图书" class="headerlink" title="计算机基础知识推荐图书"></a>计算机基础知识推荐图书</h2><blockquote><p>一个日本相关行业人员的书籍系列，通俗易懂。在此分享。</p></blockquote><p><a href="https://pan.baidu.com/s/13OUBV9zvmBAY3eTMI6DlFg" target="_blank" rel="external">网络是怎样链接的</a></p><p><a href="https://pan.baidu.com/s/1XYR5uJasnrbBXY43hupA0w" target="_blank" rel="external">计算机是怎么跑起来的</a></p><p><a href="https://pan.baidu.com/s/1k9KAxONC5pd1qQAHRoEnAg" target="_blank" rel="external">程序是怎么跑起来的</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;平时有些兴趣想要了解一些关于&lt;em&gt;计算机&lt;/em&gt;的基础知识，我就想着为何不将其作为一个笔记记录在博客上呢？以后想从事与&lt;em&gt;计算机&lt;/em&gt;相关的工作，了解基本的&lt;em&gt;计算机知识&lt;/em&gt;必不可少，因为时间，精力的有限，我会将一些认为重要的
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>关于在学习python3网络爬虫模块时的易模糊点</title>
    <link href="https://liujunjie11.github.io/2018/03/06/%E5%85%B3%E4%BA%8E%E5%9C%A8%E5%AD%A6%E4%B9%A0python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A8%A1%E5%9D%97%E6%97%B6%E7%9A%84%E6%98%93%E6%A8%A1%E7%B3%8A%E7%82%B9/"/>
    <id>https://liujunjie11.github.io/2018/03/06/关于在学习python3网络爬虫模块时的易模糊点/</id>
    <published>2018-03-06T12:10:44.000Z</published>
    <updated>2018-03-12T11:31:34.605Z</updated>
    
    <content type="html"><![CDATA[<ul><li>两种代理格式</li></ul><p>在学习<em>urlretrieve</em>函数方法时，尝试了与<em>BeautifulSoup</em>模块的结合编码。</p><p>在<em>request.bulid_opener()</em>的一类函数方法中，如：</p><pre><code>headers = (&apos;User-Agent&apos;,&apos;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;)# 安装openeropener = request.build_opener()# 添加代理opener.addheaders = [headers]# 使用临时的&apos;urlopen&apos;request.install_opener(opener)</code></pre><hr><p>而在有<em>requests.get(url=url,headers=headers…)</em>一类函数方法中，如：</p><pre><code>headers = {&apos;User-Agent&apos;:           &apos;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;}    # 利用requests模块中的方法获取URL信息，加上我们的代理re = requests.get(url=url, headers=headers)</code></pre><blockquote><p><strong>比较两者易知，两者的代理规定与是否为字典形式有偏差。</strong></p></blockquote><ul><li><p>在利用<em>requests.get()</em>下载图片时</p><pre><code>re = requests.get(url=url)        #应当加上二进制的格式规定，避免不必要的错误麻烦   with open(&apos;美女图片.jpg&apos;,&apos;wb&apos;) as mntu:       for chunk in re.iter_content(chunk_size = 1024):            if chunk:                mntu.write(chunk)                mntu.flush()</code></pre></li></ul><blockquote><p>在查询相关的资料时，有<em>Stream=True</em>的参数说明要加入，但是加入之后却是错误显示。在<em>with</em>下面的<em>for chunk…</em>在下载图片时应当加入。<a href="https://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py" target="_blank" rel="external">为何要加入，在此可易知</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;两种代理格式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在学习&lt;em&gt;urlretrieve&lt;/em&gt;函数方法时，尝试了与&lt;em&gt;BeautifulSoup&lt;/em&gt;模块的结合编码。&lt;/p&gt;
&lt;p&gt;在&lt;em&gt;request.bulid_opener()&lt;/em&gt;的一类函数方法
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于Charle的简单抓包操作</title>
    <link href="https://liujunjie11.github.io/2018/03/06/%E5%85%B3%E4%BA%8ECharle%E7%9A%84%E7%AE%80%E5%8D%95%E6%8A%93%E5%8C%85%E6%93%8D%E4%BD%9C/"/>
    <id>https://liujunjie11.github.io/2018/03/06/关于Charle的简单抓包操作/</id>
    <published>2018-03-06T03:47:36.000Z</published>
    <updated>2018-03-06T04:23:05.352Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看他人的博客学习<em>python3网络爬虫</em>的实战，看到了“抓包”的这个词，以前不知道…</p></blockquote><p>用的是<em>Macbook</em>，在网上搜索了一下，<em>Charle</em>好评多多，急忙到网上找了破解教程…（关于如何找到，可利用百度谷歌搜索关键词）</p><p>第一次用这个，啥都不会，基本操作也不会，摸索了一下，打算记录下来帮助需要的人。</p><h2 id="抓包的操作"><a href="#抓包的操作" class="headerlink" title="抓包的操作"></a>抓包的操作</h2><p>进入主界面先看看快捷键也是个不错的选择。如图：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.02.png" alt=""></p><blockquote><p>可以知道，与<em>Mac本机浏览器</em>的快捷键相似。</p></blockquote><p><em>New Session</em>即为打开一个新的界面。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.46.png" alt=""></p><p>点击<em>钢笔</em>图标，输入<em>URL</em>，自动开始抓包了。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.09.56.png" alt=""></p><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.58.png" alt=""></p><p>查看相关的快捷键，定向查找<em>html</em>中格式，如<em>json</em>格式</p><ul><li><em>Find</em></li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.03.49.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.03.59.png" alt=""></p><h2 id="系统代理配置"><a href="#系统代理配置" class="headerlink" title="系统代理配置"></a>系统代理配置</h2><ul><li>可以自动对目前系统中的一些<em>URL</em>进行抓包。</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.17.png" alt=""></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>继续摸索中..听说功能很强大，网上的教程非常多，暂时用不到太多，简单的操作说明到处结束了。</li></ul><ul><li>一些可参考的教程：</li></ul><p><a href="https://www.jianshu.com/p/9822e3f28f0a" target="_blank" rel="external">可参考一</a></p><p><a href="http://blog.csdn.net/liulanghk/article/details/46342205" target="_blank" rel="external">可参考二</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;最近在看他人的博客学习&lt;em&gt;python3网络爬虫&lt;/em&gt;的实战，看到了“抓包”的这个词，以前不知道…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用的是&lt;em&gt;Macbook&lt;/em&gt;，在网上搜索了一下，&lt;em&gt;Charle&lt;/em&gt;好评多多，
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取网络图片的小程序项目</title>
    <link href="https://liujunjie11.github.io/2018/03/05/python3%E7%88%AC%E5%8F%96%E7%BD%91%E7%BB%9C%E5%9B%BE%E7%89%87%E7%9A%84%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/03/05/python3爬取网络图片的小程序项目/</id>
    <published>2018-03-05T13:25:16.000Z</published>
    <updated>2018-03-10T03:28:35.670Z</updated>
    
    <content type="html"><![CDATA[<p> 最近在学习<em>python3网络爬虫</em>，看的是<a href="http://blog.csdn.net/c406495762/article/details/72597755" target="_blank" rel="external">这位学长的博客</a>。</p><blockquote><p>因为刚刚开始接触，想通过实战来一步步学习，所以先记录下我的学习心得，然后再记录下自己实战的成果以及相关的代码程序。</p></blockquote><ul><li><strong>先声明，以下是学习心得，代码原搬，链接在上。</strong></li></ul><p>##开始爬取的图片的下载代码的演示</p><pre><code> &apos;&apos;&apos;        #开始爬取的图片的下载代码的演示 &apos;&apos;&apos;from bs4 import BeautifulSoupfrom urllib.request import urlretrieve import requestsimport osimport timeif __name__ == &apos;__main__&apos;:    # 指定页面图片的多少数量    for num in range(1, 5):        if num == 1:            url = &apos;http://www.shuaia.net/index.html&apos;        else:            url = &apos;http://www.shuaia.net/index_%d.html&apos; % num        # 因为是下载多个页面的图片，所以之后的图片名称与其相应的图片地址的代码抒写在for循环下编写        &apos;&apos;&apos;          #在此需要设置代理，避免爬取失败几率        &apos;&apos;&apos;        headers = {&apos;User-Agent&apos;:                &apos;* Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;}        # 利用requests模块中的方法获取URL信息，加上我们的代理        re = requests.get(url=url, headers=headers)        # 指定编码，避免中文乱码        re.encoding = &apos;utf-8&apos;        &apos;&apos;&apos;            # 利用beautifulsoup模块来进行处理            #re.text : 转换成规定的字符格式            #&apos;lxml&apos; : 利用lxml解析器进行解析        &apos;&apos;&apos;        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 解析完成之后，需要对页面的html信息进行分析了，指定相关的节点        &apos;&apos;&apos;            #class_是为了避免错误混淆，所以不可&quot;原班人马&quot;        &apos;&apos;&apos;        tasges_html = bf.find_all(class_=&apos;item-img&apos;)        # 指定空的列表，装下相关的需要的信息        list = []        # 利用循环，一一导入        for each in tasges_html:            &apos;&apos;&apos;                #加上img是因为&apos;alt&apos;在以下的子节点中，所以需要相关的指明，即一个节点（父，子）一个指明            &apos;&apos;&apos;            list.append(each.img.get(&apos;alt&apos;) + &apos;=&apos; + each.get(&apos;href&apos;))        # 至此，相关的信息就采集完成了        print(&apos;采集over，开始下载：&apos;)    &apos;&apos;&apos;        #开始将采集好的信息一一下载    &apos;&apos;&apos;    for each_img in list:        # 以=分割图片地址与图片名称        img_info = each_img.split(&apos;=&apos;)        target_url = img_info[1]        filename = img_info[0] + &apos;.jpg&apos;        print(&apos;下载：&apos; + filename)        headers = {            &quot;User-Agent&quot;:          &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;        }        img_req = requests.get(url=target_url, headers=headers)        img_req.encoding = &apos;utf-8&apos;        img_html = img_req.text        img_bf_1 = BeautifulSoup(img_html, &apos;lxml&apos;)        img_url = img_bf_1.find_all(&apos;div&apos;, class_=&apos;wr-single-content-list&apos;)        img_bf_2 = BeautifulSoup(str(img_url), &apos;lxml&apos;)        img_url = &apos;http://www.shuaia.net&apos; + img_bf_2.div.img.get(&apos;src&apos;)        #若是指定的目录不存在则建立一个        if &apos;images&apos; not in os.listdir():            os.makedirs(&apos;images&apos;)        # 即指定URL地址下载        urlretrieve(url=img_url, filename=&apos;images/&apos; + filename)        time.sleep(1)    print(&apos;下载完成！&apos;)</code></pre><blockquote><p><strong>以上是通过学习<a href="http://blog.csdn.net/c406495762/article/details/72597755" target="_blank" rel="external">此篇博客文章</a>的学习心得，相关的加上图片解说可以看这篇博客文章。</strong></p></blockquote><ul><li><strong>附上用得到的知识链接：</strong></li></ul><p><a href="http://qinxuye.me/article/details-about-time-module-in-python/" target="_blank" rel="external">Python中time模块详解</a></p><p><a href="http://blog.csdn.net/vevenlcf/article/details/46777023" target="_blank" rel="external">python3中的urlretrieve() 函数使用</a></p><p><a href="https://docs.python.org/3/library/urllib.request.html" target="_blank" rel="external">urllib.request文档</a></p><p><a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="external">Requests文档</a></p><p><a href="http://www.runoob.com/python3/python3-os-file-methods.html" target="_blank" rel="external">Python3 os模块文件/目录方法</a></p><p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="external">Beautiful Soup 4.2.0 文档</a></p><p><a href="http://www.runoob.com/python/python-files-io.html" target="_blank" rel="external">Python 文件I/O模块文档</a></p><h2 id="实战部分-批量爬取美女图片"><a href="#实战部分-批量爬取美女图片" class="headerlink" title="实战部分:批量爬取美女图片"></a>实战部分:批量爬取美女图片</h2><blockquote><p>在这里收藏上<a href="http://www.bijishequ.com/detail/424024?p=" target="_blank" rel="external">这一篇python3爬取图片的快速入门地址</a>，有相关的模块方法介绍，有助于快速入门，再从一系列模块入手，之后再实战，一步步掌握。</p></blockquote><ul><li>通过另外一个网站的分析，爬取其图片并且下载。（下载的图片均可在本工程目录可找到）</li></ul><p><strong>网址：<a href="http://www.27270.com/ent/meinvtupian/2017/223643.html" target="_blank" rel="external">http://www.27270.com/ent/meinvtupian/2017/223643.html</a></strong></p><blockquote><p>搜索的美女图片关键词，随便点开的一个…</p></blockquote><p>现在我们打开网址，利用<em>Chrome</em>浏览器的抓包，看看此网址的<em>html</em>信息（如下）</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.02.32.png" alt=""></p><hr><p>在此有一个特别好用的方法，即为点击下方指出的图标，当我们把鼠标移到网页上的异常时，对应的<em>html</em>信息就会自动对齐。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-08-2018%2020-10-20.gif" alt=""></p><blockquote><p>现在开始分析代码，这个网址是专题式的…点开一个专题就会有一个类型的图片…就网页看来一个页面仅仅一张图片。</p></blockquote><p><strong>在我们将鼠标移到图片处时，对应的代码也就自动对齐了（图1），还有它在的那个节点（图2），再点击下一张看看，查看<em>html</em>信息（图3），发现它们的节点信息一致，不同的仅仅为对应的标题以及图片的目录地址了（图4），我们可以从这里下手一步步来</strong></p><ul><li><p>图1<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.53.png" alt=""></p></li><li><p>图2<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.35.png" alt=""></p></li><li><p>图3<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.14.png" alt=""></p></li><li><p>图4<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.15.00.png" alt=""></p></li></ul><blockquote><p>关于<em>html</em>的基础知识，只需要知道它们的结构以及节点，一些变量等的基础知识就够了，不需要太深入。可以在网上搜索关键词‘html基础知识’查看。这绝对是一劳永逸的。</p></blockquote><p>在分析之后写个测试代码：</p><pre><code>&apos;&apos;&apos;    爬取美女图片实战代码&apos;&apos;&apos;        import requestsfrom bs4 import BeautifulSoupif __name__ == &apos;__main__&apos;:    url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;    re = requests.get(url=url)    # 编译解码格式，避免乱码出现    re.encoding = &apos;utf-8&apos;    # 用beautiful模块进行解析    bf = BeautifulSoup(re.text, &apos;lxml&apos;)    #在此注意find_all与find方法的使用，因为一个页面仅仅有一张图片，用前者会出错    bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)    list_html = []    list_html.append(bf_html.img.get(&apos;alt&apos;) + &apos;  :  &apos; + bf_html.img.get(&apos;src&apos;))    print(list_html)</code></pre><p>运行，发现出现了乱码的情况：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.30.29.png" alt=""></p><p>将’utf-8’格式换成了’GB2312’，发现乱码问题得到解决：<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.35.17.png" alt=""></p><hr><p>查看了一下<em>html</em>信息，发现’utf-8’格式是不可取的，对于这个网站来说。<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.34.26.png" alt=""></p><p>还可以用以下这段代码得出网址的编码方式。</p><pre><code>#关于输出网页编码方式的判断from urllib import requestimport chardetif __name__ == &quot;__main__&quot;:    response = request.urlopen(&quot;http://www.27270.com/ent/meinvtupian/2017/223643.html&quot;)    html = response.read()    charset = chardet.detect(html)    print(charset)    </code></pre><p>运行看看：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.44.21.png" alt=""></p><ul><li>可参考：<a href="http://blog.csdn.net/qq_21460525/article/details/78217697" target="_blank" rel="external">chardet模块简单认识</a></li></ul><blockquote><p>得到了图片的目录信息以及相关的信息之后，就好办了。</p></blockquote><hr><p>为什么这么说？实际上我们爬取图片时仅仅需要图片的目录地址就可以利用一两行<em>python爬虫</em>代码下载下来了，代码利用<em>urllib.requests模块中的urlretrieve() 函数</em>就能轻而易举的办到。代码在下，也可以查看相关的文档来学习。</p><pre><code>from  urllib.request import urlretrieveimport time# 简单爬取一张图片，或者是需要代理的测试if __name__ == &apos;__main__&apos;:    print(&apos;开始下载：&apos;)    urlretrieve(url=&apos;html上的图片src信息&apos;, filename=&apos;xx照片&apos;+&apos;.jpg&apos;)    time.sleep(1)    print(&apos;下载完成！&apos;)</code></pre><blockquote><p>可以自己试试。</p></blockquote><hr><p>因为我们是为了爬取多张图片…所以可以分析一下每一页<em>URL</em>的不同或者是相似点。看图1，2发现了有索引<strong>_2</strong>出现，那再点击下一页会有<strong>_3</strong>…<br>每一页有不同的照片，因为是一个专题…那么照片地址也不一样了（如图3，4）。</p><ul><li><p>图1<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.57.34.png" alt=""></p></li><li><p>图2<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.57.47.png" alt=""></p></li></ul><hr><ul><li>图3</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%889.01.44.png" alt=""></p><ul><li>图4<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%889.01.53.png" alt=""></li></ul><blockquote><p>可以轻而易举的发现问题的所在了，接下来直接贴上完整的下载代码，不懂的朋友应当找相关的文档来自己查看，多写多实战！</p></blockquote><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><ul><li><p><strong>代码编译步骤：查看信息到分析整合最后是代码实现。</strong></p><pre><code>&apos;&apos;&apos;    函数目标：    爬取美女图片完整代码    编写开始时间：    2018-3-08&apos;&apos;&apos;import requestsfrom bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport os import time if __name__ == &apos;__main__&apos;:    # 一个专题8页..    for  num in range(1, 9):        if num == 1:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;          else:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643_%d.html&apos; % num        # 在此就不添加代理部分的了，添加可起到隐蔽的效果，在需要的场合应当添加上        re = requests.get(url=url)        # 之指定编码格式        re.encoding = &apos;GB2312&apos;        # 用beautiful模块解析,具体可查看文档内容        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 一页一张，用find方法        bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)        # 在创建的目录下保存！        urlretrieve(url=bf_html.img.get(&apos;src&apos;), filename=bf_html.img.get(&apos;alt&apos;) + &apos;系列之%d.jpg&apos; % num)        # 一秒一个步骤实行代码        time.sleep(1)        print(&apos;开始下载第%d张&apos; % num)    print(&apos;下载完成，请查收...&apos;)</code></pre></li></ul><p>运行，查看效果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-09%20%E4%B8%8B%E5%8D%889.47.25.png" alt=""></p><p>在本工程目录夹可查收…</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-09%20%E4%B8%8B%E5%8D%889.50.25.png" alt=""></p><hr><h2 id="最后总结"><a href="#最后总结" class="headerlink" title="最后总结"></a>最后总结</h2><p>代码还可以完善，比如可创建一个目录专门便于我们查收…</p><p>总之，还需要多多学习。</p><h2 id="修改添加目录程序"><a href="#修改添加目录程序" class="headerlink" title="修改添加目录程序"></a>修改添加目录程序</h2><pre><code>import requestsfrom bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport os import time if __name__ == &apos;__main__&apos;:    #可先建立一个专门存放图片的目录文件夹    if &apos;images&apos; not in os.listdir():        os.makedirs(&apos;images&apos;)    print(&apos;建立目录夹完成，开始下载图片！&apos;)    # 一个专题8页..    for  num in range(1, 9):        if num == 1:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;          else:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643_%d.html&apos; % num        # 在此就不添加代理部分的了，添加可起到隐蔽的效果，在需要的场合应当添加上        re = requests.get(url=url)        # 之指定编码格式        re.encoding = &apos;GB2312&apos;        # 用beautiful模块解析,具体可查看文档内容        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 一页一张，用find方法        bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)        # 在创建的目录下保存！        urlretrieve(url=bf_html.img.get(&apos;src&apos;), filename=&apos;images/&apos;+bf_html.img.get(&apos;alt&apos;) + &apos;系列之%d.jpg&apos; % num)        # 一秒一个步骤实行代码        time.sleep(1)        print(&apos;开始下载第%d张&apos; % num)    print(&apos;下载完成，请查收...&apos;)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 最近在学习&lt;em&gt;python3网络爬虫&lt;/em&gt;，看的是&lt;a href=&quot;http://blog.csdn.net/c406495762/article/details/72597755&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这位学长的博客&lt;/
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>麻木</title>
    <link href="https://liujunjie11.github.io/2018/03/03/%E9%BA%BB%E6%9C%A8/"/>
    <id>https://liujunjie11.github.io/2018/03/03/麻木/</id>
    <published>2018-03-03T07:15:57.000Z</published>
    <updated>2018-03-03T07:50:58.179Z</updated>
    
    <content type="html"><![CDATA[<p>新的学期开始了。</p><p>当我反思了自己在之前的一些所做所言，我开始了一股深深的愧疚感，我的理想到了哪里去了？我不愿做一个“偏激”的代言人之一，但是麻木亦然不可取。但我应该谨记在心里，不能忘记，更不能因为这现实的种种而丧失了我的初心，忘了我的理想，那我曾经认为的美好。</p><p>我是慢慢的变得麻木了，开始想要得安稳与胸无理想，开始想要的安于现状，这是多么的可怕。</p><p>人在一种环境当中总是或多或少的被影响一些的。如今的我想要一种平静，但那是一种可以应对自如的一种理性，而不是慢慢的陷于麻木的颓废。可能是我太舒服了，变得安于现状了，慢慢的麻木了，开始变得曾经的那些“不喜欢”了。</p><p>写着写着，突然想起来一句话：生于忧患，死于安乐。此时此刻的我对于这句话有了自己的更为深刻的认识。一个人活着应该是为了这个人类社会的忧患做出应有的努力。至少我是认同这样的一种观点的。我有些怀疑过这些许观点的必要性，为什么这样的一种观点会得到我的或者是说为什么会得到如今大部分人类的认同从而占据人类社会的主流思想呢？我认为这与人类道德中的“善”是相挂钩的，这与大部分人的期许相对应符合，大多数的人们都渴望有“平和”（“和谐”）的生活状态与“有教养”的人文社会。而我或许也是其中的一份子。</p><p>我曾经有过的一种强烈的理想：通过发明一种技术，从而造福整个人类，让人们生活的更好。</p><p>我也应当时时刻刻的记得自己的期许，有着自己的见解，有着自己的看法，而不是跟着“世俗”走，以免再一次变得“麻木”，我所认为的麻木。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;新的学期开始了。&lt;/p&gt;
&lt;p&gt;当我反思了自己在之前的一些所做所言，我开始了一股深深的愧疚感，我的理想到了哪里去了？我不愿做一个“偏激”的代言人之一，但是麻木亦然不可取。但我应该谨记在心里，不能忘记，更不能因为这现实的种种而丧失了我的初心，忘了我的理想，那我曾经认为的美好。
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>假期小记</title>
    <link href="https://liujunjie11.github.io/2018/01/22/%E5%81%87%E6%9C%9F%E5%B0%8F%E8%AE%B0/"/>
    <id>https://liujunjie11.github.io/2018/01/22/假期小记/</id>
    <published>2018-01-22T10:36:27.000Z</published>
    <updated>2018-01-23T09:33:21.122Z</updated>
    
    <content type="html"><![CDATA[<p>最近总是想记一些东西，不过总是因为太懒就搁下了。</p><p>我谈恋爱了，是我喜欢的女孩。但是让我感觉有一些不靠谱，让我感觉到一场恋爱的开始是如此的容易…这是我没有想到的。她也喜欢我，我也喜欢她，一场恋爱就这样的开始了。我想开始一场恋爱也就如此而已，哈哈。</p><p>刚刚因为<em>iCloud</em>的一些事搁写了一下，现在继续。说到底<em>Apple</em>关于对于<em>iCloud</em>的宣布是对我有一些影响的，我对于自己的东西总是会有一些担心呢，哈哈。</p><p>我好久没怎么看三毛的书了，我以为我看了一本她的《三毛全集》就以为已将她的书都看完了，我发现还有一本她的书《送你一匹马》，依旧是从前的感觉，不过看的时候我的心情却是比从前平静的多了，但是，在其中带给我的感动总是满满的。</p><p>我如今觉得读书人读太多的书，或许是一种悲哀，一种赤裸裸的悲哀。难道不是吗，很多时候的可笑都是这一群自以为是的读书人做出来的。人都是矛盾的，这一点在读书人身上我却是更加的容易看到。读书人读书人，可笑的就是现在的我呢，哈哈。</p><p>这次放假的生活，也就像在学校一样的，吃喝拉玩睡等，不过玩和睡的比例却是大大的增加的了，哈哈。</p><p>突然的写着写着发现其实也没什么好写的了，不过是生活中的一些小小的记录。生活中的每一次清醒，我都会有一种极端的感觉，既然人生一场本来就是一无所有，我们现在做的却也不过是一场徒劳。不过有意义的总是在生活中的处处场景可以看到，我也一直相信，人生一场，意义是绝对存在的。</p><p>如今我更加的看重生活中的每一处，因为那是生活。我不过也就是一个学习者，我还要多多的学习，向生活学习。我自称学习者，刚刚写完这句话我又想笑了，哈哈。</p><p>算了算了。我发现如今的自己没有从前的那般的“热情记录”了，我变得不想说话在很多时候，其实说到底可能就是一种心态的体现而已。我不见得就是所谓的成长，有些东西随着时间的流逝有得有失，过着过着就变成今天这个样子了。</p><p>有时候总是想记一些东西，总是想写一些东西，有一种不写不舒服的感觉，有时候我觉得有一些欲望总是好的，特别是在这个无所事事的假期，我想我真的是懒得过分了过分了。</p><p>这一篇小记在此就结束了，想写的时候我会再来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近总是想记一些东西，不过总是因为太懒就搁下了。&lt;/p&gt;
&lt;p&gt;我谈恋爱了，是我喜欢的女孩。但是让我感觉有一些不靠谱，让我感觉到一场恋爱的开始是如此的容易…这是我没有想到的。她也喜欢我，我也喜欢她，一场恋爱就这样的开始了。我想开始一场恋爱也就如此而已，哈哈。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="生活 感悟" scheme="https://liujunjie11.github.io/categories/%E7%94%9F%E6%B4%BB-%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="生活 感悟" scheme="https://liujunjie11.github.io/tags/%E7%94%9F%E6%B4%BB-%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>关于在学习python爬虫时的学习记录</title>
    <link href="https://liujunjie11.github.io/2017/12/08/%E5%85%B3%E4%BA%8E%E5%9C%A8%E5%AD%A6%E4%B9%A0python%E7%88%AC%E8%99%AB%E6%97%B6%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/08/关于在学习python爬虫时的学习记录/</id>
    <published>2017-12-08T13:36:46.000Z</published>
    <updated>2018-03-13T10:14:18.751Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习<em>python3爬虫</em>，看的是<a href="http://blog.csdn.net/c406495762/article/details/78123502" target="_blank" rel="external">这位博主的博客</a>，不得不说，是真的厉害，通俗易懂^ _ ^</p><p><strong>我要学习的还有很多…从基本的<em>python</em>知识，我就被难倒了…</strong></p><p>哎，记录下我的盲点…</p><p>花了近一个钟头测试出来的结果。</p><ul><li>在爬取相关的<em>html</em>时，<strong>text ≠ text[0]</strong></li></ul><blockquote><p>后者是正确的。我一直以为不加的效果也是一样的结果，在我理解看来就是从头开始的，即<strong>从0到尾的所有相关的内容</strong>，实际上我的理解与相关的<em>python</em>基础不谋而和，可能是爬虫就需要如此的？我就默认好了…</p></blockquote><ul><li>在<em>python</em>中的方法后面的<strong><em>（）</em></strong>是不可省去的</li></ul><blockquote><p>在我学过一些其他的语言，在方法后面可不加括号，如<em>Scala</em>，曾经我在其他的<em>python</em>编程中是行得通的…但是在处理爬虫的代码时就报错了，我也默认了…</p></blockquote><ul><li>在<em>class</em>括号中的<em>object</em>是可有可无的</li></ul><blockquote><p> 这无疑是<em>python</em>的基本知识…</p></blockquote><ul><li>关于在<em>python</em>中类似<em>C</em>中的<em>printf</em>函数的使用</li></ul><blockquote><p>基本上是与<em>C</em>中的用法一致的，但是在爬虫中需要将爬取的内容输出，就需要<em>%%</em>来表示了…参考了<a href="http://www.169it.com/article/11773602545120851576.html" target="_blank" rel="external">这篇文章</a>，其中的有句这样解释道：<strong>用进行转义一样，这里用%作为格式标记的标识，也有一个%本身应该如何输出的问题。如果要在”格式标记字符串“中输出%本身，可以用%%来表示。</strong></p></blockquote><p>在用文件的<em>io</em>与<em>requests模块</em>的结合（<strong>x.content()</strong>）,可达到与<em>urlretrieve函数</em>同样的效果（只需输入URL地址即可）。</p><p>总结：还需要更多的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近学习&lt;em&gt;python3爬虫&lt;/em&gt;，看的是&lt;a href=&quot;http://blog.csdn.net/c406495762/article/details/78123502&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这位博主的博客&lt;/a&gt;，不
      
    
    </summary>
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>今后的打算</title>
    <link href="https://liujunjie11.github.io/2017/12/07/%E4%BB%8A%E5%90%8E%E7%9A%84%E6%89%93%E7%AE%97/"/>
    <id>https://liujunjie11.github.io/2017/12/07/今后的打算/</id>
    <published>2017-12-07T07:18:53.000Z</published>
    <updated>2017-12-07T07:49:44.073Z</updated>
    
    <content type="html"><![CDATA[<p>恍然大悟，等我看到还有保研一说的政策时，我如今已是一名大二的学生了，保研的机会我基本上已是错过了。在我参考了一些相关的资料时，我又一次看到了人与人之间的差距…</p><p><strong>眼界对于一个人真的是太重要了，一个人不应该总是停留在现在，应该时时思考自己未来的方向。</strong></p><p>在如今的科技浪潮上，更高的学历当然是我所向往的，而我不喜欢考试，我深深的感觉到了自己不擅长考试，不管是小的还是大的，我对于考试已是到了难以忍耐的地步了，我能从内心感觉的到。</p><p>在我的这个时代，人工智能的浪潮早已是滚滚而来了。如今的我们身处在这个看似大好的“时代”，我也想分一杯羹，我也想通过这一次浪潮发挥自己的效用，借此实现自己的理想，为世界带来更多的福利，为世界人民带来更好的生活。</p><p>而在如今的现实是，高学历的普遍存在…这无疑会成为我的短板…而我可能也会因此而做出退让，放弃自己的技术选项…这真的是我的悲剧啊,会是我人生最大的悲剧吗？</p><p>如今的我需要突出自己的优势了，一定要突出自己的优势。更高的学历意味着无疑拥有更大的机会会得到更好的资源与更加广阔的眼界，不得不说，一个人的学习应该是终生的…</p><p>目前的我太过于浮躁与无定向的迷乱了…<strong>我还需要更多的学习与实践，突出自己的优势，必将是我应当做的，就在不远的今后。</strong></p><p><strong>摆正心态，不断实践，突出优势！</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;恍然大悟，等我看到还有保研一说的政策时，我如今已是一名大二的学生了，保研的机会我基本上已是错过了。在我参考了一些相关的资料时，我又一次看到了人与人之间的差距…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;眼界对于一个人真的是太重要了，一个人不应该总是停留在现在，应该时时思考自己未来的方向
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>关于利用python爬虫爬取小说的实战例子</title>
    <link href="https://liujunjie11.github.io/2017/12/04/%E5%85%B3%E4%BA%8E%E5%88%A9%E7%94%A8python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E7%9A%84%E5%AE%9E%E6%88%98%E4%BE%8B%E5%AD%90/"/>
    <id>https://liujunjie11.github.io/2017/12/04/关于利用python爬虫爬取小说的实战例子/</id>
    <published>2017-12-04T14:33:41.000Z</published>
    <updated>2017-12-07T13:35:45.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近对<strong><em>python爬虫</em></strong>很感兴趣，就花了一些时间来学习，学习了近一周的时间，终于在看了一些其他博客的大神写的教程之后，学会了利用<strong><em>beautifulsoup</em></strong>的方法爬取了一些网站的小说。在此分享我学习的成果。</p><ul><li><strong>关于详细的<em>python3爬虫</em>的学习教程以及参考：</strong><a href="http://blog.csdn.net/column/details/15321.html" target="_blank" rel="external">学习教程</a></li></ul><ul><li>本文参考并且学习了：<a href="http://blog.csdn.net/c406495762/article/details/71158264" target="_blank" rel="external">这篇文章</a></li></ul><ul><li>在以下的文中主要介绍<strong>爬小说的技巧</strong>。</li></ul><h1 id="爬取的过程"><a href="#爬取的过程" class="headerlink" title="爬取的过程"></a>爬取的过程</h1><h2 id="素材"><a href="#素材" class="headerlink" title="素材"></a>素材</h2><ul><li><p><a href="http://www.biqukan.com/38_38278/17032848.html" target="_blank" rel="external">笔趣阁小说网</a></p></li><li><p><a href="https://www.readnovel.com/chapter/7943133504728103/21429716840455516" target="_blank" rel="external">小说阅读网</a></p></li></ul><hr><h2 id="技巧分析"><a href="#技巧分析" class="headerlink" title="技巧分析"></a>技巧分析</h2><p>在编写代码前应先在浏览器中（推荐<em>Chrome</em>，我用的<em>Safari</em>）查看在网页抓包中的相关的内容信息，一般查看其中的：</p><ul><li><p><code>charset</code>（<strong>即其中的编码类型，在后面的程序中需要</strong>）</p></li><li><p><code>在文章开始阶段的前的那一小段html编码</code></p></li></ul><p>如下图所示的（等下将要用到）：</p><ul><li><code>charset</code>所示：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.43.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.49.48.png" alt=""></p><blockquote><p>一般在最前头可以看到。</p></blockquote><hr><ul><li><code>主要的一小段html</code>(即为<code>div</code>的标签，后面为相应的属性值，即一个<code>html</code>中有多个标签，每一个标签用不同的属性值来进行标记以表示不同的标签，从而在一个页面中可以有多个不同的内容展示出来)所示：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.55.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.09.png" alt=""></p><blockquote><p>如上在文章的前边的那一小段的<em>html代码</em>。</p></blockquote><hr><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><ul><li>可先用相关的代码查看相关的网页字符属于那种编码类型</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 用beautifulsoup爬取小说的例子</div><div class="line">#     可先将网页编码的字符先行得知</div><div class="line"> from urllib import request</div><div class="line"> import chardet</div><div class="line"> </div><div class="line"> re=request.urlopen(&apos;http://www.biqukan.com/1_1094/5403177.html&apos;)</div><div class="line"> charset=chardet.detect(re.read())</div><div class="line"> print(&apos;we can see the cahrset about the html:&apos;,charset)</div></pre></td></tr></table></figure><blockquote><p>具体的学习可以参考<a href="http://blog.csdn.net/c406495762/article/details/58716886" target="_blank" rel="external">这篇文章</a>。</p></blockquote><hr><ul><li>开始编写爬取相关内容的代码。<strong>将其中的<em>URL</em>地址以及相关的<em>charset所属字符</em>在以下的代码中修改一下均可达到在下方效果的显示</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">#开始爬取相关的内容</div><div class="line">from urllib import request</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">from urllib import error</div><div class="line">try:</div><div class="line">    if __name__ == &apos;__main__&apos;:</div><div class="line">        #相关的URL的输入以及代理</div><div class="line">        re=request.Request(url=&apos;http://www.biqukan.com/1_1094/5403177.html&apos;,headers=&#123;&apos;User-Agent&apos;:&apos;* Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30&apos;&#125;)</div><div class="line">         #打开,并且以相关的解码方式，此处应当对应上方的charset</div><div class="line">             html=request.urlopen(re).read().decode(&apos;gbk&apos;,&apos;ignore&apos;)</div><div class="line">        #接下来创建beautifulsoup对象,找到相关的参数以便爬取内容</div><div class="line">        soup_text=BeautifulSoup(html,&apos;lxml&apos;)</div><div class="line">        #在html中若是有class注意将其中的class改为class_（因为python中有个class关键字的存在了),在此应当对应上方说明的html的一小段的编码</div><div class="line">        texts=soup_text.find_all(id=&quot;content&quot; ，class_=&quot;showtxt&quot;)</div><div class="line">        soup_texts=BeautifulSoup(str(texts),&apos;lxml&apos;)</div><div class="line">        #输出时将删除相关的不符合要求的字符,将其替换为空白</div><div class="line">        print(soup_texts.div.text.replace(&apos;\xa0&apos;,&apos;&apos;))</div><div class="line">except error.URLError as e:</div><div class="line">    if hasattr(e, &apos;code&apos;):</div><div class="line">        print(&apos;httperroe:&apos;)</div><div class="line">        print(e.cond)</div><div class="line">    if hasattr(e, &apos;reason&apos;):</div><div class="line">        print(&apos;urlerror&apos;)</div><div class="line">        print(e.reason)</div></pre></td></tr></table></figure><blockquote><p>详细可参考：<a href="http://blog.csdn.net/c406495762/article/details/71158264" target="_blank" rel="external">这篇文章</a></p></blockquote><hr><h2 id="结果查看"><a href="#结果查看" class="headerlink" title="结果查看"></a>结果查看</h2><ul><li>关于在第一段的实战效果：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.25.11.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.25.26.png" alt=""></p><hr><ul><li>关于在第二段的实战效果：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.27.06.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.27.17.png" alt=""></p><hr><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><blockquote><p><strong>在网页中爬取的小说内容都是可以直接下载至本地的，具体的可以参考以上的那个学习教程。</strong></p></blockquote><ul><li><p>可学习的参考网站：</p><p>  <a href="http://blog.csdn.net/column/details/15321.html" target="_blank" rel="external"><em>python3爬虫教程</em></a></p><p>  <a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">beautifulsoup官网教程</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;最近对&lt;strong&gt;&lt;em&gt;python爬虫&lt;/em&gt;&lt;/strong&gt;很感兴趣，就花了一些时间来学习，学习了近一周的时间，终于在看了一些
      
    
    </summary>
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>关于Mac关机与iCloud的打不开的问题记录</title>
    <link href="https://liujunjie11.github.io/2017/12/03/%E5%85%B3%E4%BA%8EMac%E5%85%B3%E6%9C%BA%E4%B8%8EiCloud%E7%9A%84%E6%89%93%E4%B8%8D%E5%BC%80%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/03/关于Mac关机与iCloud的打不开的问题记录/</id>
    <published>2017-12-03T07:16:59.000Z</published>
    <updated>2017-12-03T07:25:22.367Z</updated>
    
    <content type="html"><![CDATA[<p>这一段时间因为开的网页比较重要，<em>Mac</em>一直没有关机，晚上睡觉也是合上就这样了，没想到才3天这样，<em>Mac</em>好像支撑不了了…变得有些卡了，在未关机前的硬盘空间大小与关机后打开之后的硬盘空间大小相差了<strong>5个G</strong>，在重启开机时出现了一段绿屏…，由此可知，一段有规律的关机保养还是很有必要的…</p><p>关于<em>iCloud</em>的一些打开错误，如打开查看存储信息时的错误问题等等，一般的解决方法为：换一个网络连接即可解决。</p><blockquote><p>此时换<em>DNS</em>的地址是不管用的。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一段时间因为开的网页比较重要，&lt;em&gt;Mac&lt;/em&gt;一直没有关机，晚上睡觉也是合上就这样了，没想到才3天这样，&lt;em&gt;Mac&lt;/em&gt;好像支撑不了了…变得有些卡了，在未关机前的硬盘空间大小与关机后打开之后的硬盘空间大小相差了&lt;strong&gt;5个G&lt;/strong&gt;，在重
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于Mac网速慢的原因记录</title>
    <link href="https://liujunjie11.github.io/2017/12/01/%E5%85%B3%E4%BA%8EMac%E7%BD%91%E9%80%9F%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/01/关于Mac网速慢的原因记录/</id>
    <published>2017-12-01T10:56:07.000Z</published>
    <updated>2017-12-01T11:07:50.743Z</updated>
    
    <content type="html"><![CDATA[<p>最近<em>Mac</em>突然变得很慢，明明网速很快的。</p><p>在网上看了一下，发现可能是<em>DNS</em>的问题，果然换了一下（以前用的都是<em>8.8.8.8</em>估计用的人多了就卡了），把号删除了，发现网速飞的快起…</p><p>不过在平时中下载<em>app</em>失败时可以考虑换回来就好使了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近&lt;em&gt;Mac&lt;/em&gt;突然变得很慢，明明网速很快的。&lt;/p&gt;
&lt;p&gt;在网上看了一下，发现可能是&lt;em&gt;DNS&lt;/em&gt;的问题，果然换了一下（以前用的都是&lt;em&gt;8.8.8.8&lt;/em&gt;估计用的人多了就卡了），把号删除了，发现网速飞的快起…&lt;/p&gt;
&lt;p&gt;不过在平时中下
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于python下载scrapy失败的问题记录</title>
    <link href="https://liujunjie11.github.io/2017/12/01/%E5%85%B3%E4%BA%8Epython%E4%B8%8B%E8%BD%BDscrapy%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/01/关于python下载scrapy失败的问题记录/</id>
    <published>2017-12-01T05:10:33.000Z</published>
    <updated>2017-12-01T05:16:20.172Z</updated>
    
    <content type="html"><![CDATA[<p>用的<em>anaconda</em>的<em>python3</em>，下载<em>scrapy</em>出现以下情况：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-01%20%E4%B8%8B%E5%8D%881.09.17.png" alt=""></p><blockquote><p>即表示下载失败。</p></blockquote><hr><p><strong>看到网上大部分说的太复杂的解决方案，其实太过于小题大做了。</strong></p><p><strong>解决方法：重新下载一次。</strong></p><ul><li>图1</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-01%20%E4%B8%8B%E5%8D%881.09.23.png" alt=""></p><hr><ul><li>用命令行<em>conda list</em>查看</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-01%20%E4%B8%8B%E5%8D%881.09.36.png" alt=""></p><blockquote><p>下载成功了。</p></blockquote><hr><ul><li><strong>若是失败了，多下载几次大多即可解决。</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用的&lt;em&gt;anaconda&lt;/em&gt;的&lt;em&gt;python3&lt;/em&gt;，下载&lt;em&gt;scrapy&lt;/em&gt;出现以下情况：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>关于用java编译的mapreduce与用Scala编译的spark的对比（单词统计的例子说明）</title>
    <link href="https://liujunjie11.github.io/2017/11/29/%E5%85%B3%E4%BA%8E%E7%94%A8java%E7%BC%96%E8%AF%91%E7%9A%84mapreduce%E4%B8%8E%E7%94%A8Scala%E7%BC%96%E8%AF%91%E7%9A%84spark%E7%9A%84%E5%AF%B9%E6%AF%94%EF%BC%88%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1%E7%9A%84%E4%BE%8B%E5%AD%90%E8%AF%B4%E6%98%8E%EF%BC%89/"/>
    <id>https://liujunjie11.github.io/2017/11/29/关于用java编译的mapreduce与用Scala编译的spark的对比（单词统计的例子说明）/</id>
    <published>2017-11-29T13:56:35.000Z</published>
    <updated>2017-11-29T14:08:30.911Z</updated>
    
    <content type="html"><![CDATA[<p>最近在学习<em>spark</em>，顺便也学习了<em>Scala</em>的在其间的使用。我发现与用<em>java</em>编译的<em>mapreduce</em>相比，用<em>Scala</em>编译的<em>spark</em>更加的容易让人被其折服…</p><p>直接看图说话吧，感受<em>spark</em>的强大！以统计单词为例。</p><ul><li>以下为<em>java</em>编译的<em>mapreduce</em></li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%20%E4%B8%8B%E5%8D%8810.00.19.png" alt=""></p><blockquote><p>70+行的代码。</p></blockquote><hr><ul><li><em>Scala</em>编译的<em>spark</em></li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%20%E4%B8%8B%E5%8D%8810.00.32.png" alt=""></p><blockquote><p>仅仅为6行代码….</p></blockquote><hr><ul><li>同样的结果如下：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-11-29%20%E4%B8%8B%E5%8D%8810.01.11.png" alt=""></p><hr><ul><li>最后，据说官方会考虑将<em>mapreduce</em>优化,拭目以待吧…相比如今的<em>spark</em>的威力…</li></ul><p><strong>记录此文章仅仅是为了表达我心中的兴奋，因为遇到了<em>spark</em>…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在学习&lt;em&gt;spark&lt;/em&gt;，顺便也学习了&lt;em&gt;Scala&lt;/em&gt;的在其间的使用。我发现与用&lt;em&gt;java&lt;/em&gt;编译的&lt;em&gt;mapreduce&lt;/em&gt;相比，用&lt;em&gt;Scala&lt;/em&gt;编译的&lt;em&gt;spark&lt;/em&gt;更加的容易让人被其折服…&lt;/p
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于在用Scala编译spark中的reduce方法计算的问题疑惑记录</title>
    <link href="https://liujunjie11.github.io/2017/11/28/%E5%85%B3%E4%BA%8E%E5%9C%A8%E7%94%A8Scala%E7%BC%96%E8%AF%91spark%E4%B8%AD%E7%9A%84reduce%E6%96%B9%E6%B3%95%E8%AE%A1%E7%AE%97%E7%9A%84%E9%97%AE%E9%A2%98%E7%96%91%E6%83%91%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/11/28/关于在用Scala编译spark中的reduce方法计算的问题疑惑记录/</id>
    <published>2017-11-28T13:34:35.000Z</published>
    <updated>2017-11-28T13:46:19.143Z</updated>
    
    <content type="html"><![CDATA[<p>在学习<em>spark</em>中，<em>reduce</em>方法是绕不开的。但是我在测试时发现与从前的规则好像有一点冲突了…</p><p>如下分析：</p><ul><li>输入代码   </li></ul><blockquote><p>1, <strong>val a=sc.parallelize(1 to 10)</strong>//理论上理解为0～9的数字放在一个分区</p><p>2, <strong>a.reduce((x,y) =&gt; x+y)</strong></p></blockquote><ul><li>输出</li></ul><blockquote><p><strong>55</strong>//即意味着从0（0可省亦可）到10(包括10在内了，与上方的<em>to</em>方法冲突了)都被相加了…</p></blockquote><p>这测试我想了老半天了…没想到是这种结果…<em>bug</em>???</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习&lt;em&gt;spark&lt;/em&gt;中，&lt;em&gt;reduce&lt;/em&gt;方法是绕不开的。但是我在测试时发现与从前的规则好像有一点冲突了…&lt;/p&gt;
&lt;p&gt;如下分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入代码   &lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;1, &lt;stron
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>名人效应</title>
    <link href="https://liujunjie11.github.io/2017/11/28/%E5%90%8D%E4%BA%BA%E6%95%88%E5%BA%94/"/>
    <id>https://liujunjie11.github.io/2017/11/28/名人效应/</id>
    <published>2017-11-28T10:33:07.000Z</published>
    <updated>2017-11-28T11:03:13.369Z</updated>
    
    <content type="html"><![CDATA[<p>“名人效应”在我们的生活中屡见不鲜，当我们在遇到一些挂有所谓的“名气”的人，比如“名校”出来的人，或者是一些本身就是所谓的“名人”的人时，心中大多会不由自主的产生一种敬畏与向往，有时竟然会产生自卑的感觉…我感觉这好似一种“阴差阳错”，又不得不感慨这种“名人效应”对我们的这种“潜移默化”教化的感觉。</p><p>那么这种效应究竟是从何而来？为何又产生这种貌似人与生就有的“天性”？我想可以在我们的生活中可以找到答案。</p><p>因为我们从小到大的生活环境都是如此“教育”我们的，所以在我们身边大部分的人的这种相关的“感觉”会伴随我们成长，久而久之我们也是这般如此了。简要分析可知这一类的大部分人在我们成长的过程中好似又是不可或缺的，又好似是不可避免的，哈哈，难道不是吗，我们身边的大多数父母即是如此，最简单的理解，“考上一个好的大学”。在此我们就知道了，原来世界上有一些“有名”的大学，从身边的一些“名人”的崇拜者，我们与之交流亦或者是道听途说，听多了看多了自然也就易被“名化”了…</p><p>那么“自卑感”又从何而来？…我想这是一种错误的，被“名化”扭曲了的心理活动吧，至少我是这么认为的。这里的“名化”可以理解为被“名”影响化了的意思，这样的人往往不由自主的随波逐流，被“名”波及到。当然，可能也与所谓的“好强心”，所谓的“羡慕嫉妒”也会有一定的挂钩吧。</p><p>不管怎么说，我感觉这就是一种“不应该”，带“名”的东西亦然有其的优势，但是如果一味“随波逐流”的因此把自己搞得扭曲，那真的是得不偿失，对不起自己了…与我而言，真的是一种非常蠢的心理活动呢…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;“名人效应”在我们的生活中屡见不鲜，当我们在遇到一些挂有所谓的“名气”的人，比如“名校”出来的人，或者是一些本身就是所谓的“名人”的人时，心中大多会不由自主的产生一种敬畏与向往，有时竟然会产生自卑的感觉…我感觉这好似一种“阴差阳错”，又不得不感慨这种“名人效应”对我们的这种
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
</feed>
