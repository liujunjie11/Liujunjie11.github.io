<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LXiHa`Notes</title>
  
  <subtitle>The House Belong to Love and Freedom.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liujunjie11.github.io/"/>
  <updated>2018-03-20T01:41:04.007Z</updated>
  <id>https://liujunjie11.github.io/</id>
  
  <author>
    <name>刘俊</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>用python/R可视化GitHub上的java热门开源项目</title>
    <link href="https://liujunjie11.github.io/2018/03/19/%E7%94%A8python%E5%8F%AF%E8%A7%86%E5%8C%96GitHub%E4%B8%8A%E7%9A%84java%E7%83%AD%E9%97%A8%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/03/19/用python可视化GitHub上的java热门开源项目/</id>
    <published>2018-03-19T08:36:14.000Z</published>
    <updated>2018-03-20T01:41:04.007Z</updated>
    
    <content type="html"><![CDATA[<p>直接开始这个小项目吧。</p><ul><li>网页地址：<a href="https://api.github.com/search/repositories?q=language:java&amp;sort=stars" target="_blank" rel="external">https://api.github.com/search/repositories?q=language:java&amp;sort=stars</a></li></ul><blockquote><p>在这个网页中有相关的目前比较热门的开源项目（以<em>star</em>的数目来衡量），打开发现这是典型的<em>json</em>格式啊。</p></blockquote><h2 id="简单分析"><a href="#简单分析" class="headerlink" title="简单分析"></a>简单分析</h2><p>经过抓包可发现：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.31.32.png" alt=""></p><p>即：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.29.37.png" alt=""></p><blockquote><p>换一换就可知道所有编程语言目前比较热门的开源项目了。</p></blockquote><h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><pre><code>&apos;&apos;&apos;    函数目标：    将GitHub上的java热门的开源项目可视化    编写时间：    2018-3-19&apos;&apos;&apos;import requestsfrom matplotlib import pyplot as pltimport pygalfrom pygal.style import LightColorizedStyle as lcs, LightenStyle as lsif __name__ == &apos;__main__&apos;:    #添加代理配置    url = &apos;https://api.github.com/search/repositories&apos;    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    paramter = {&apos;q&apos;: &apos;language:java&apos;,              &apos;sort&apos;: &apos;stars&apos;}    re = requests.get(url=url, params=paramter, headers=header)    # 将网页转化为python字典即用json()函数方法才可显示与网页内容一致！    re.encoding = &apos;utf-8&apos;    js_cont = re.json()    item = js_cont[&apos;items&apos;]    star_count = []    names = []    full_names = []    for each in item:            star_count.append(each[&apos;stargazers_count&apos;])            names.append(each[&apos;name&apos;])            full_names.append(each[&apos;full_name&apos;])    # 添加高亮颜色    my_style = ls(&apos;#333366&apos;, base_style=lcs)    # 添加相关的设置    my_config = pygal.Config()    my_config.label_font_size = 28    bar_chart = pygal.Bar(config=my_config, style=my_style, x_label_rotation=60, show_legend=False)    bar_chart.add(&apos;&apos;, star_count)    bar_chart.title = &apos;Java  projects stars on Github&apos;    bar_chart.x_labels = names    # 保存至目录下的文件中    bar_chart.render_to_file(&apos;Java stars in Github.svg&apos;)</code></pre><blockquote><p>之中不懂的可利用好搜索引擎。有一些爬虫的知识。</p></blockquote><p>运行得到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.24.44.png" alt=""></p><blockquote><p>简单说说用<em>python</em>可视化的感受，比较喜欢用<em>python</em>，用的比较多，意味发现<em>pygal</em>这个库做的图很漂亮。</p></blockquote><h2 id="R代码实现"><a href="#R代码实现" class="headerlink" title="R代码实现"></a>R代码实现</h2><ul><li>说明：因为<em>R</em>的爬虫没怎么看，先用可视化…日后有时间爬虫写上…</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;直接开始这个小项目吧。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网页地址：&lt;a href=&quot;https://api.github.com/search/repositories?q=language:java&amp;amp;sort=stars&quot; target=&quot;_blank&quot; rel=&quot;ex
      
    
    </summary>
    
      <category term="可视化" scheme="https://liujunjie11.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="python/R可视化" scheme="https://liujunjie11.github.io/tags/python-R%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（三）：带宽单位换算与存储单位换算</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%B8%A6%E5%AE%BD%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97%E4%B8%8E%E5%AD%98%E5%82%A8%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（三）：带宽单位换算与存储单位换算/</id>
    <published>2018-03-15T14:06:15.000Z</published>
    <updated>2018-03-15T14:25:50.120Z</updated>
    
    <content type="html"><![CDATA[<p>位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.</p><blockquote><p>1比特 = 一个二进制位，只有0和1两种状态<br>  1字节 = 8 比特</p></blockquote><p>1 Byte(B)＝8bit（位）<br>1KB＝1024Byte（字节）</p><ul><li>再来看看平时常见的下载参数：</li></ul><p><strong>Mbps：</strong>带宽单位，在 Mbps 单位中的“b”是指“Bit（位）</p><p><strong>MB/s：</strong>速度单位，其中的 B 是指“Byte（字节）</p><blockquote><p>其中1MB/s=8Mbps，下载工具一般以Bps计算，所以它们之间有8bit=1Byte的换算关系，一个字节，是由八位二进制位组成的，所以可解释一个200M的网，换算为字节，实际上仅仅极限速度能达到200/8=25M的速度。</p></blockquote><h2 id="存储单位的换算"><a href="#存储单位的换算" class="headerlink" title="存储单位的换算"></a>存储单位的换算</h2><p>计算机存储单位一般用bit、B、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB……来表示，它们之间的关系是：<br>位 bit (比特)(Binary Digits)：存放一位二进制数，即 0 或 1，最小的存储单位。</p><p><strong>换算：</strong></p><p>字节byte：8个二进制位为一个字节(B)，最常用的单位。</p><p>1 Byte（B） = 8 bit</p><p>1 Kilo Byte（KB） = 1024B</p><p>1 Mega Byte（MB） = 1024 KB</p><p>1 Giga Byte （GB）= 1024 MB</p><p>1 Tera Byte（TB）= 1024 GB</p><p>1 Peta Byte（PB） = 1024 TB</p><p>1 Exa Byte（EB） = 1024 PB</p><p>1 Zetta Byte（ZB） = 1024 EB</p><p>1Yotta Byte（YB）= 1024 ZB</p><p>1 Bronto Byte（BB） = 1024 YB</p><p>1Nona Byte（NB）=1024 BB</p><p>1 Dogga Byte（DB）=1024 NB</p><p>1 Corydon Byte（CB）=1024DB</p><p>1 Xero Byte （XB）=1024CB</p><blockquote><p>进制单位全称及译音：</p><p>yotta，[尧]它， Y. 10^24，</p><p>zetta，[泽]它， Z. 10^21，</p><p>exa，[艾]可萨， E. 10^18，</p><p>peta，[拍]它， P. 10^15，</p><p>tera，[太]拉， T. 10^12，</p><p>giga，[吉]咖， G. 10^9，</p><p>mega，[兆]，M. 10^6</p></blockquote><ul><li><strong>b(bit)与B的认识</strong></li></ul><p>字节(B)是电脑中表示信息含义的最小单位，通常情况下一个ACSII码就是一个字节的空间来存放。而事实上电脑中还有比字节更小的单位，因为一个字节是由八个二进制位组成的，换一句话说，每个二进制位所占的空间才是电脑中最小的单位，我们把它称为位，也称比特（bit）。一个字节等于八位。人们之所以把字节称为电脑中表示信息含义的最小单位，是因为一位并不能表示我们现实生活中的一个相对完整的信息。</p><ul><li><strong>计算机储存单位的进率是1024而不是1000？</strong></li></ul><p>目前计算机都是二进制的，让它们计算单位，只有2的整数幂时才能非常方便计算机计算，因为电脑内部的电路工作有高电平和低电平两种状态.所以就用二进制来表示信号，(控制信号和数据)，以便计算机识别。而人习惯于使用10进制，所以存储器厂商们才用1000作进率。这样导致的后果就是实际容量要比标称容量少，不过这是合法的。1024是2的10次方，因为如果取大了，不接近10的整数次方，不方便人们计算；取小了，进率太低，单位要更多才能满足需求，所以取2的10次方正好。<br>计算实例：标称100GB的硬盘，其实际容量为100×1000×1000×1000字节/1024×1024×1024≈93.1GB<br>可见产品容量缩水只要满足计算的实际容量结果（上下误差应该在10%内）。</p><ul><li>参考：</li></ul><p><a href="https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305" target="_blank" rel="external">https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305</a></p><p><a href="https://www.jianshu.com/p/2b57116c27de" target="_blank" rel="external">https://www.jianshu.com/p/2b57116c27de</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1比特 = 一个二进制位，只有0和1两种状态&lt;br&gt;  1字节 = 8 比特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1 Byte(B)＝8bi
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%95%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程/</id>
    <published>2018-03-15T13:58:48.000Z</published>
    <updated>2018-03-15T14:03:52.921Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>转载自：<a href="http://blog.csdn.net/zolalad/article/details/28393209" target="_blank" rel="external">http://blog.csdn.net/zolalad/article/details/28393209</a></p></blockquote><p><strong>一．进程、线程、单核处理器</strong></p><p>进程和线程都是操作系统的概念。进程是应用程序的执行实例，每个进程是由私有的虚拟地址空间、代码、数据和其它各种系统资源组成，即进程是操作系统进行资源分配的最小单元。进程在运行过程中创建的资源随着进程的终止而被销毁，所使用的系统资源在进程终止时被释放或关闭。</p><p>线程是进程内部的一个执行单元。系统创建好进程后，实际上就启动执行了该进程的主执行线程，主执行线程以函数地址形式，比如说main或WinMain函数，将程序的启动点提供给Windows系统。主执行线程终止了，进程也就随之终止。</p><p>每一个进程至少有一个主执行线程，它无需由用户去主动创建，是由系统自动创建的。用户根据需要在应用程序中创建其它线程，多个线程并发地运行于同一个进程中。一个进程中的所有线程都在该进程的虚拟地址空间中，共同使用这些虚拟地址空间、全局变量和系统资源，所以线程间的通讯非常方便，多线程技术的应用也较为广泛。</p><p>多线程可以实现并行处理，避免了某项任务长时间占用CPU时间。要说明的一点是，目前大多数的操作系统教材中的单处理器都是指的单核处理器。对于单核单处理器（CPU）的，为了运行所有这些线程，操作系统为每个独立线程安排一些CPU时间，操作系统以轮换方式向线程提供时间片，这就给人一种假象，好象这些线程都在同时运行。由此可见，如果两个非常活跃的线程为了抢夺对CPU的控制权，在线程切换时会消耗很多的CPU资源，反而会降低系统的性能。</p><p>最开始，线程只是用于分配单个处理器的处理时间的一种工具。但假如操作系统本身支持多个处理器，那么每个线程都可分配给一个不同的处理器，真正进入“并行运算”状态。从程序设计语言的角度看，多线程操作最有价值的特性之一就是程序员不必关心到底使用了多少个处理器，程序员只需将程序编写成多线程模式即可。程序在逻辑意义上被分割为数个线程；假如机器本身安装了多个处理器，那么程序会运行得更快，毋需作出任何特殊的调校。根据前面的论述，大家可能感觉线程处理非常简单。但必须注意一个问题：共享资源！如果有多个线程同时运行，而且它们试图访问相同的资源，就会遇到一个问题。举个例子来说，两个线程不能将信息同时发送给一台打印机。为解决这个问题，对那些可共享的资源来说（比如打印机），它们在使用期间必须进入锁定状态。所以一个线程可将资源锁定，在完成了它的任务后，再解开（释放）这个锁，使其他线程可以接着使用同样的资源。</p><p><strong>多线程是为了同步完成多项任务，不是为了提高运行效率，而是为了提高资源使用效率来提高系统的效率。线程是在同一时间需要完成多项任务的时候实现的。</strong></p><p>最简单的比喻多线程就像火车的每一节车厢，而进程则是火车。车厢离开火车是无法跑动的，同理火车也不可能只有一节车厢。多线程的出现就是为了提高效率。同时它的出现也带来了一些问题。</p><p><strong>注</strong>：单核处理器并不是一个长久以来存在的概念，在近年来多核心处理器逐步普及之后，单核心的处理器为了与双核和四核对应而提出。顾名思义处理器只有一个逻辑核心。</p><p><strong>二、多核处理器和多处理器的区别</strong></p><p>多核是指一个CPU有多个核心处理器，处理器之间通过CPU内部总线进行通讯。而多CPU是指简单的多个CPU工作在同一个系统上，多个CPU之间的通讯是通过主板上的总线进行的。从以上原理可知，N个核的CPU，要比N个CPU在一起的工作效率要高（单核性能一致的情况下）。</p><p><strong>三、 处理器结构对并发程序的影响</strong></p><p>对称多处理器是最主要的多核处理器架构。在这种架构中所有的CPU共享一条系统总线（BUS）来连接主存。而每一个核又有自己的一级缓存，相对于BUS对称分布[2]，如下图：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/20140509162415500.jpeg" alt=""></p><p>这种架构在并发程序设计中，大致会引来两个问题，一个是内存可见性，一个是Cache一致性流量。内存可见性属于并发安全的问题，Cache一致性流量引起的是性能上的问题。</p><p><strong>内存可见性</strong>：内存可见性在单处理器或单线程情况下是不会发生的。在一个单线程环境中，一个变量选写入值，然后在没有干涉的情况下读取这个变量，得到的值应该是修改过的值。但是在读和写不在同一个线程中的时候，情况却是不可以预料的。Core1和Core2可能会同时把主存中某个位置的值Load到自己的一级缓存中，而Core1修改了自己一级缓存中的值后，却不更新主存中的值，这样对于Core2来讲，永远看不到Core1对值的修改。在Java程序设计中，用锁，关键字volatile，CAS原子操作可以保证内存可见。</p><p><strong>Cache一致性问题</strong>：指的是在SMP结构中，Core1和Core2同时下载了主存中的值到自己的一级缓存中，Core1修改了值后，会通过总线让Core2中的值失效，Core2发现自己存的值失效后，会再通过总线从主存中得到新值。总线的通信能力是固定的，通过总线使各CPU的一级缓存值数据同步的流量过大，那么总线就会成瓶颈。这种影响属于性能上的影响，减小同步竞争就能减少一致性流量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;转载自：&lt;a href=&quot;http://blog.csdn.net/zolalad/article/details/28393209&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/zola
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取QQ音乐列表音乐</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96QQ%E9%9F%B3%E4%B9%90%E5%88%97%E8%A1%A8%E9%9F%B3%E4%B9%90/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取QQ音乐列表音乐/</id>
    <published>2018-03-14T12:57:31.000Z</published>
    <updated>2018-03-15T08:48:37.038Z</updated>
    
    <content type="html"><![CDATA[<p>最近想爬取一些音乐来实战一下，选择了<em>qq音乐</em>。</p><p><em>qq音乐</em>明显的就是一个动态网页，所以需要抓包了。</p><blockquote><p>不懂的关键词可利用好搜索引擎。</p></blockquote><h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>在此就说说分析的大致过程吧。</p><p>先看看主页：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.08.29.png" alt=""></p><p>我们随便点开一个主题列表：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.09.44.png" alt=""></p><p>因为是动态网页，所以就在这里抓包吧，因为<em>qq音乐</em>是动态网页，需要相关的参数信息才能得到想要的音乐地址，随便以播放一首歌曲为例，如下图1中的歌曲<em>ID</em>，点进去这个看看，即点击播放按钮，发现来到了播放页面，打开我用的<em>Chrome</em>中的开发者工具，里面有我们想要的音乐地址（如下图2所示），图3展示得到列表歌曲的所有信息，需要编程清洗之后才能得到我们想要，在此会在下面的代码中标明。</p><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.37.52.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.41.18.png" alt=""></p><ul><li>图3:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.51.03.png" alt=""></p><p>其中的<em>URL</em>地址，代码中会用到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.48.57.png" alt=""></p><blockquote><p>现在我们可能就会有思路出现了：抓包爬取列表的所有歌曲的<em>ID号</em>以及歌曲信息 <strong>–&gt;</strong> 整合到以<em>ID</em>为基的<em>html</em>地址 <strong>–&gt;</strong> 到播放页面利用<em>beautiful模块</em>爬取相关的音乐地址即可！so easy~</p></blockquote><p><strong>但经过我的测试说明，爬取播放页面的<em>html</em>信息是没有相关的音乐地址的，所以在得到歌曲<em>ID</em>信息，整合到以<em>ID</em>为基的<em>html</em>地址之后，我们还需要对播放页面进行抓包。下面说说如何在播放页面抓包。</strong></p><p>先播放一首歌曲，再进行抓包，发现了这些信息（如下图1），再结合上面的音乐地址分析一下，发现了<em>vkey</em>信息的存在，即每一首歌的<em>vkey</em>信息是不同的，并且经过测试即便是同一个<em>ID</em>，<em>vkey</em>也是一直不断自动变换着的，不过在测试之后可得出结论：<strong>只要得到<em>vkey</em>信息，再整合上面的音乐地址就能抓到音乐信息，并且经过代码编译之后下载下来。</strong></p><blockquote><p>特别说明一下，<strong>下载歌曲的地址以及我们抓包时的<em>URL</em>之中，仅仅有如<em>vkey</em>的不同，或者是一些<em>ID</em>的不同，其他的参数是相同的！</strong>所以我们才能仅仅抓到<em>vkey</em>信息就能方便的下载歌曲，就是这么个意思。不理解的朋友可细心观察一下。</p></blockquote><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.55.11.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.58.13.png" alt=""></p><p>其中的<em>URL</em>地址：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.49.29.png" alt=""></p><hr><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><pre><code>&apos;&apos;&apos;函数目标：爬取QQ音乐列表音乐编写时间：2018-3-15&apos;&apos;&apos;import requestsimport os import timeimport json from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    # 建立目录用于装爬取的音乐    if &apos;QQ音乐列表音乐&apos; not in os.listdir():        os.makedirs(&apos;QQ音乐列表音乐&apos;)    &apos;&apos;&apos;        从URL中添加代理记忆必要的相关的参数以获取歌曲的ID以及歌曲名                                                            &apos;&apos;&apos;    playlist_url = &apos;https://c.y.qq.com/qzone/fcg-bin/fcg_ucc_getcdinfo_byids_cp.fcg&apos;    # 添加页面中的代理信息    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yplayer_open=1; yq_index=0; yq_playschange=0; player_exist=1&apos;,            &apos;referer&apos;:&apos;https://y.qq.com/n/yqq/playlist/3766176211.html&apos;}    # 添加参数信息,有些是非必须的，待研究，有兴趣的可以自己测试    paramter = {                &apos;type&apos;:&apos;1&apos;,                &apos;json&apos;:&apos;1&apos;,                &apos;utf8&apos;:&apos;1&apos;,                &apos;onlysong&apos;:&apos;0&apos;,                &apos;disstid&apos;:&apos;3766176211&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                &apos;jsonpCallback&apos;:&apos;playlistinfoCallback&apos;,  # 值可更改                &apos;loginUin&apos;:&apos;0&apos;,                &apos;hostUin&apos;:&apos;0&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &quot;inCharset&quot;:&apos;utf8&apos;,                &apos;outCharset&apos;:&apos;utf-8&apos;,                &apos;notice&apos;:&apos;0&apos;,                &apos;platform&apos;:&apos;yqq&apos;,                &apos;needNewCode&apos;:&apos;0&apos;,                }    playlist_re = requests.get(url=playlist_url, params=paramter, headers=header)    # 指定编码格式    playlist_re.encoding = &apos;utf-8&apos;    # 改变为python可识别的json格式,进行必要的数据清洗,去掉前面的&apos;jsonpCallback&apos;部分    playlist_info = json.loads(playlist_re.text.lstrip(&apos;playlistinfoCallback(&apos;).rstrip(&apos;)&apos;))    # 指定整体索引    playlist_info1 = playlist_info[&apos;cdlist&apos;][0]    # 先存储歌手的姓名,观察可知，一共有19个索引,因为歌曲本身仅仅有20首，取前20个歌手名    singer_name = []    for num in range(0, 17):        singer_eainfo = playlist_info1[&apos;songlist&apos;][num]        for each_info in singer_eainfo[&apos;singer&apos;]:            singer_name.append(each_info[&apos;name&apos;])    num = 0    # 在循环体系中进行下一步的编写    for each in playlist_info1[&apos;songlist&apos;]:        &apos;&apos;&apos;            在获取歌曲vkey的主URL传入相关的参数得到相关的数据之后进行挖掘，得到vkey信息            其中的参数有些是不必要的，可自由修改，有些是必要的                                                               &apos;&apos;&apos;        key_url = &apos;https://c.y.qq.com/base/fcgi-bin/fcg_music_express_mobile3.fcg&apos;        # 传入相关的代理以及参数        header_1 = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                  &apos;referer&apos;:&apos;https://y.qq.com/portal/player.html&apos;,                  &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yq_index=0; yq_playschange=0; player_exist=1; ts_last=y.qq.com/n/yqq/playlist/3766176211.html; yplayer_open=1&apos;}            paramter_1 = {                    &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                    &apos;jsonpCallback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &quot;loginUin&quot;:&apos;0&apos;,                    &apos;hostUin&apos;:&apos;0&apos;,                    &apos;format&apos;:&apos;json&apos;,                    &apos;inCharset&apos;:&apos;utf8&apos;,                    &apos;outCharset&apos;:&apos;utf-8&apos;,                    &apos;notice&apos;:&apos;0&apos;,                    &apos;platform&apos;:&quot;yqq&quot;,                    &apos;needNewCode&apos;:&apos;0&apos;,                    &apos;cid&apos;:&apos;205361747&apos;,  # 一致必须                    &apos;callback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &apos;uin&apos;:&apos;0&apos;,                    # 传入获取的信息                    &apos;songmid&apos;:each[&apos;songmid&apos;],                    &apos;filename&apos;:&apos;C400&apos; + each[&apos;songmid&apos;] + &apos;.m4a&apos;,                    &apos;guid&apos;:&apos;7344469728&apos;                       }        # 解析得到含有vkey的数据信息，然后进行清洗得到想要的信息        key_re = requests.get(url=key_url, params=paramter_1, headers=header_1)        # 指定编码格式        key_re.encoding = &apos;utf-8&apos;        # 转换为python的json格式，进行简单的清洗        key_info = json.loads(key_re.text.lstrip(&apos;MusicJsonCallback(&apos;).rstrip(&apos;)&apos;))        # 进一步的清洗        data_info = key_info[&apos;data&apos;]        items_info = data_info[&apos;items&apos;][0]        print(&apos;数据采集完成，开始下载任务...&apos;)        # 接下来就是可以下载了        urlretrieve(url=&apos;http://dl.stream.qqmusic.qq.com/C4000041FwTv0Ai3Ku.m4a?vkey=&apos; + items_info[&apos;vkey&apos;] + &apos;&amp;guid=7344469728&amp;uin=0&amp;fromtag=66.mp3&apos;, filename=&apos;QQ音乐列表音乐/&apos; + singer_name[num] + &apos;-&apos; + each[&apos;songname&apos;] + &apos;.mp3&apos;)        print(&apos;正在下载:&apos; + singer_name[num] + &apos;的&apos; + each[&apos;songname&apos;] + &apos;!&apos;)        print(&apos;下载中....&apos;)        print(&apos;下载此歌曲完成！&apos;)        # 跳传到下一个歌手名        num = num + 1        time.sleep(1)    print(&apos;全部下载完成，请在本过程目录下查收！&apos;)</code></pre><blockquote><p><strong>在使用urlretrieve函数时，其中的url参数输入时应当加上格式，如下载视频时加上.mp4,下载音乐时加上.mp3,否则会易出现HTTP 403 错误 – 禁止访问 (Forbidden)</strong></p></blockquote><ul><li>链接：<a href="http://www.checkupdown.com/status/E403_zh.html" target="_blank" rel="external">认识HTTP 403 错误 – 禁止访问 (Forbidden)</a></li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>运行：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.08.01.png" alt=""></p><p>到目录查看：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.09.17.png" alt=""></p><blockquote><p>一切还算是顺利。前段时间想爬取腾讯视频，研究了挺久，没有成功，还需要学习，腾讯的资源都在腾讯云上，我想方式都差不多。</p></blockquote><ul><li>参考：<a href="http://blog.csdn.net/lht_okk/article/details/77206510" target="_blank" rel="external">http://blog.csdn.net/lht_okk/article/details/77206510</a></li></ul><h2 id="后续说明"><a href="#后续说明" class="headerlink" title="后续说明"></a>后续说明</h2><p><strong>经过后来的测试，本代码爬取的思路还是正确的，但是爬取的信息流只能是同一个了…即便是不同的ID…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想爬取一些音乐来实战一下，选择了&lt;em&gt;qq音乐&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;qq音乐&lt;/em&gt;明显的就是一个动态网页，所以需要抓包了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不懂的关键词可利用好搜索引擎。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;分
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取新加坡联合早报新闻小视频</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96%E6%96%B0%E5%8A%A0%E5%9D%A1%E8%81%94%E5%90%88%E6%97%A9%E6%8A%A5%E6%96%B0%E9%97%BB%E5%B0%8F%E8%A7%86%E9%A2%91/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取新加坡联合早报新闻小视频/</id>
    <published>2018-03-14T10:58:25.000Z</published>
    <updated>2018-03-14T11:35:26.345Z</updated>
    
    <content type="html"><![CDATA[<p>位于新加坡的<a href="http://www.zaobao.com" target="_blank" rel="external">联合早报</a>是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…</p><p>今天看了这一篇文章：<a href="http://www.zaobao.com/realtime/china/story20180313-842407" target="_blank" rel="external">女记者提问冗长 人民大会堂部长通道出现“飙戏”一幕</a></p><blockquote><p>非常有意思，想收藏其中的视频，于是想到了用<em>python</em>爬取好了。</p></blockquote><p>这个新闻网站明显是一个动态网站啊，两种方式：</p><ul><li>第一种：通过抓包，如下可得知相关的<em>video</em>信息</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.30.png" alt=""></p><ul><li>第二种：网站自带的连接</li></ul><p>如下操作，点击视频中<em>share</em>，可发现资源地址</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.57.png" alt=""></p><blockquote><p>右上角的<em>share</em>。</p></blockquote><p>其中有地址信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.04.10.png" alt=""></p><p>在获取的地址前加上<em>http:</em>简单测试一下:</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.07.10.png" alt=""></p><blockquote><p>是正确的，网站真的很贴心呢～</p></blockquote><p>以刚刚的地址输入爬取下来的代码：</p><pre><code>from urllib.request import urlretrieve if __name__ == &apos;__main__&apos;:    print(&apos;开始下载...&apos;)    urlretrieve(url=&apos;http://players.brightcove.net/4802324430001/H1dr7zTWz_default/index.html?videoId=5750255765001.mp4&apos;, filename=&apos;两会小视频.mp4&apos;)    print(&apos;下载完成！&apos;)</code></pre><p><strong>结果：发现可在网上播放的视频下载之后却不能播放…占用的内存才几百kb…这一看就知道地址是错的…</strong></p><hr><p>经过上面网址播放的连接，再次进行抓包，打开相关的网页意外发现了<em>mp4格式</em>的连接：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.28.05.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.30.37.png" alt=""></p><hr><p><strong>将此链接替换掉上面代码中的URL地址，发现可以了（如下图），完工。</strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.31.51.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位于新加坡的&lt;a href=&quot;http://www.zaobao.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;联合早报&lt;/a&gt;是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…&lt;/p&gt;
&lt;p&gt;今
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取动态网页图片</title>
    <link href="https://liujunjie11.github.io/2018/03/12/python3%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87/"/>
    <id>https://liujunjie11.github.io/2018/03/12/python3爬取动态网页图片/</id>
    <published>2018-03-12T12:16:32.000Z</published>
    <updated>2018-03-12T12:44:38.949Z</updated>
    
    <content type="html"><![CDATA[<p>爬取的<em>URL地址</em>:<a href="https://unsplash.com/" target="_blank" rel="external">https://unsplash.com/</a></p><blockquote><p>这是一个优美图片地址，往下拉就可以出来更多的图片，这显然是一个动态网页呀…</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>关于详细分析以及我的参考可见：<a href="http://blog.csdn.net/c406495762/article/details/78123502" target="_blank" rel="external">http://blog.csdn.net/c406495762/article/details/78123502</a></p><blockquote><p><strong>因为分析方向一致，我就不在此说了，我跟这位博主的工具有一些出入，实际上用<em>Chrome</em>分析已经足够了。</strong></p></blockquote><p>##代码</p><p>这是我后来自己写的代码，比上面博主的简短一些，亦可参考参考。</p><pre><code>&apos;&apos;&apos;    函数目标：    爬取动态网页的图片&apos;&apos;&apos;import requestsimport jsonfrom urllib.request import urlretrieveimport osimport timeif __name__ == &apos;__main__&apos;:    url = &apos;https://unsplash.com/napi/feeds/home&apos;    &apos;&apos;&apos;    添加需要的代理:    authorization证书配置,有时网站需要此类的代理html信息才会出来...    json格式分析js页面的利器，有时用js渲染出来的页面，要注意观察其URL及相对准确的信息    &apos;&apos;&apos;    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;authorization&apos;:&apos;Client-ID c94869b36aa272dd62dfaeefed769d4115fb3189a9xxxxxxxxxxx&apos;}    re = requests.get(url=url, headers=header)    re.encoding = &apos;utf-8&apos;    # 通过分析易知一页有包括多张图片的ID链接，可用python的json格式处理解析    json_info = json.loads(re.text)    # 建立一个空的列表用于装ID信息    list_id = []    if &apos;优美图片集&apos; not in os.listdir():        os.makedirs(&apos;优美图片集&apos;)    for each in json_info[&apos;photos&apos;]:        list_id.append(each[&apos;id&apos;])    # 利用urlretrieve函数一一下载，设置延迟    for i in range(0, len(list_id)):        print(&apos;开始下载指定页面中的第%d张&apos; % (i + 1))        urlretrieve(url=&apos;https://unsplash.com/photos/&apos; + list_id[i] + &apos;/download?force=true.jpg&apos;, filename=&apos;优美图片集/&apos; + &apos;系列%d.jpg&apos; % i)    print(&apos;下载完成！请查收...&apos;)    </code></pre><blockquote><p>虽然说我的代码简短一点，不过我还是支持面对对象模式编程的，方便以后的学习，也是对自己的一种考验。</p></blockquote><p>运行之后在本工程目录可见：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.16.51.png" alt=""></p><p>最后再补充说明一下：</p><p>每一次的拉取新的图片时，进行抓包，得知新的图片ID以及一个页面，通过分析此页面便可得到图片相关的信息，进而进行下载保存了（如下图）。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-12%20%E4%B8%8B%E5%8D%888.12.58.png" alt=""></p><hr><p>简单说说当时的情况，在参考了上面博主的分析过程后，利用了<em>Chrome下载器</em>的下载发现了图片的信息，然后我用<em>urlretrieve函数</em>单张下载的测试，发现成功了…附上代码。</p><pre><code>from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    urlretrieve(&apos;https://unsplash.com/photos/NrflUuJJK0I/download?force=true.jpg&apos;, &apos;tu.jpg&apos;)         print(&apos;下载完成！&apos;)</code></pre><blockquote><p>不过从来都没有见过这种<em>src信息</em>…算是开了眼界，长了知识啦。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;爬取的&lt;em&gt;URL地址&lt;/em&gt;:&lt;a href=&quot;https://unsplash.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://unsplash.com/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这是一个优美图片地址
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>学习python3网络爬虫的总结</title>
    <link href="https://liujunjie11.github.io/2018/03/12/%E5%AD%A6%E4%B9%A0python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%9A%84%E6%80%BB%E7%BB%93/"/>
    <id>https://liujunjie11.github.io/2018/03/12/学习python3网络爬虫的总结/</id>
    <published>2018-03-12T11:01:41.000Z</published>
    <updated>2018-03-13T13:18:32.382Z</updated>
    
    <content type="html"><![CDATA[<ul><li>有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…</li></ul><ul><li>在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对可以达到目的的每一点进行抓包分析，挖掘其中的信息。另外，在爬取网页信息中，有一些反爬虫的或者是必须加入一些参数代理才可得到需要的信息等，俊需要一个点一个步骤的去分析。</li></ul><ul><li><p>在爬取网页的过程中，编写代码时，检查代码的函数方法的准确性，少一个‘s’与多一个‘s’，都是让人头疼的问题。</p></li><li><p>在编写代码的过程中，追求最好的解决方案，习惯于用面向对象来编写代码，便于以后的学习。</p></li><li><p>编写爬虫代码，要让其像是一个浏览器一般的去爬取数据，所以代理之类的应当要严谨使用。</p></li><li><p>分析html信息，善于用<strong>正则表达式</strong>解决一些代码与文字的混合信息。</p></li><li><p>对URL的分析到位亦然很重要。</p></li><li><p>学会快速判断是否为动态网页。</p></li><li><p><em>python</em> 编程缩进很重要！！！！</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;有时在爬取的过程中很慢，以至于没有什么反应，第一应当先检查网络连接的情况，网络带宽突然变得很慢亦是一个问题，遇到了好几次，以为程序出了问题，不想却是网络带宽问题…&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;在爬取动态网页中，学会利用抓包进行解决，分析每一个点以及对
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Mac触摸板小问题记录</title>
    <link href="https://liujunjie11.github.io/2018/03/12/Mac%E8%A7%A6%E6%91%B8%E6%9D%BF%E5%B0%8F%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2018/03/12/Mac触摸板小问题记录/</id>
    <published>2018-03-12T02:40:07.000Z</published>
    <updated>2018-03-12T02:42:35.506Z</updated>
    
    <content type="html"><![CDATA[<p>今早<em>触摸板</em>的<em>三指点击</em>功能不灵了，有点急…</p><p>解决：<strong>重启。</strong></p><blockquote><p>一般电脑上的小问题重启之后可能就会得到解决了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今早&lt;em&gt;触摸板&lt;/em&gt;的&lt;em&gt;三指点击&lt;/em&gt;功能不灵了，有点急…&lt;/p&gt;
&lt;p&gt;解决：&lt;strong&gt;重启。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一般电脑上的小问题重启之后可能就会得到解决了。&lt;/p&gt;
&lt;/blockquote&gt;

      
    
    </summary>
    
      <category term="笔记" scheme="https://liujunjie11.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="笔记" scheme="https://liujunjie11.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>利用python3网络爬虫爬取成绩</title>
    <link href="https://liujunjie11.github.io/2018/03/10/%E5%88%A9%E7%94%A8python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E6%88%90%E7%BB%A9/"/>
    <id>https://liujunjie11.github.io/2018/03/10/利用python3网络爬虫爬取成绩/</id>
    <published>2018-03-10T14:07:18.000Z</published>
    <updated>2018-03-11T02:38:31.390Z</updated>
    
    <content type="html"><![CDATA[<p>因为最近在学习<em>python3网络爬虫</em>，想自己写一些小程序来实战一下。爬的是<em>URP教务网</em>。</p><p>一开始的思路是利用<em>beautiful</em>模块来进行爬取相关的<em>html信息</em>，直接来得到需要的信息。结果发现程序运行不通…</p><p>后来查了一下，发现用<em><a href="https://docs.python.org/3/library/re.html" target="_blank" rel="external">re模块</a></em>好啊…配合<em><a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a></em>那是相当简单…</p><p>下面开始分析，代码编写阶段。</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="学习模拟登陆"><a href="#学习模拟登陆" class="headerlink" title="学习模拟登陆"></a>学习模拟登陆</h2><p>这是第一步，有两种简单的方法，可直接参考的链接：<a href="http://blog.csdn.net/JoeHF/article/details/48424335" target="_blank" rel="external">参考链接在此</a></p><p>一种是查看抓的包中的：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.02.png" alt=""></p><p>代码：</p><pre><code>import requestsf __name__ == &apos;__main__&apos;:    # 登陆页面的URL,此处的URL为登陆页面的URL以及在登陆之后的header中的request均可    url = &apos;http://60.219.165.24/loginAction.do&apos;    # 设置相关的代理以及在登陆之后的fordate信息        # &apos;Connection&apos;: &apos;keep-alive&apos; ：保证持续连接    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    login_fordate = {&apos;zjh&apos;:&apos;20160xxxxx&apos;,                    &apos;mm&apos;:&apos;x&apos;}    # 利用session方法爬取请求    s = requests.session()    response = s.post(url, data=login_fordate, headers=header)     # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>不懂可查看文档：<br><a href="http://docs.python-requests.org/zh_CN/latest/" target="_blank" rel="external">requests文档</a></p></blockquote><hr><p>第二种是查看包中的<em>cookie信息：</em></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.44.21.png" alt=""></p><p>对应的实现模拟代码：</p><pre><code>import requestsif __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;hhttp://60.219.165.24/loginAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers）    # 验证登陆状态     if response.status_code == 200:             print(&apos;模拟登陆成功！&apos;)</code></pre><blockquote><p>相关的信息我用<em>x</em>换掉了。不懂的朋友可以看文档，查资料咯。再次说明一下，<em>URL部分**</em>可以是登陆界面的，也可以是登陆之后的URL，经过测试两者均可。**</p></blockquote><h2 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h2><p>到了分析阶段了。</p><p>打开我的<em>Chrome浏览器</em>，开始分析每个链接的<em>html信息</em>，看看有没有我想要的信息。</p><p>根据下面的操作得到了<em>本学期的成绩查询</em>的相关的超链接。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-48-17.gif" alt=""></p><p>再结合下面两张图的分析易知：<em>本学期成绩查询</em>的超链接为：<strong><a href="http://60.219.165.24//bxqcjcxAction.do" target="_blank" rel="external">http://60.219.165.24//bxqcjcxAction.do</a></strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.01.png" alt=""></p><blockquote><p>超链接部分。</p></blockquote><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%889.52.25.png" alt=""></p><blockquote><p>主URL部分。</p></blockquote><p>为了保证准确性，再向某成绩采取相应的操作（如下图所示），再往上看看，就发现它是<em>本学期成绩查询</em>的一部分。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-11-2018%2009-49-16.gif" alt=""></p><hr><blockquote><p>接下来就是编写代码了，以上若是有不懂的地方，还需要利用搜索引擎多多查询哟。</p></blockquote><h2 id="测试代码部分"><a href="#测试代码部分" class="headerlink" title="测试代码部分"></a>测试代码部分</h2><p>编写测试代码，爬取网页<em>html信息</em>：</p><pre><code>import requests    if __name__ == &apos;__main__&apos;:        # 登陆页面url        url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;        # 设置代理相关        headers = {                &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                &apos;Cookie&apos;:&apos;JSExxxxxx=hxxxx-9Ixxxxxxx_xxxxxxxx&apos;,                &apos;Connection&apos;: &apos;keep-alive&apos;                 }        # 利用session爬取请求,之后可方便的get与post        s = requests.session()        response = s.post(url=url, headers=headers）        # 设置成网页对应的编码格式        response.encoding = &apos;GB2312&apos;        # 查看相关的网页内容        print(response.text)</code></pre><p>运行查看效果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.05.02.png" alt=""></p><blockquote><p>成功得到了想要的<em>html信息</em>，接下来利用<em>正则表达式</em>选想要的部分即可。<strong>在这里要说明一下，不可用<em>ForDate</em>的那个模拟登陆，经过测试发现返回的是错误信息…所以以后老老实实用<em>cookie</em>模拟更为靠谱一点…</strong></p></blockquote><h2 id="完整代码部分"><a href="#完整代码部分" class="headerlink" title="完整代码部分"></a>完整代码部分</h2><pre><code>import requestsimport re if __name__ == &apos;__main__&apos;:    # 登陆页面url    url = &apos;http://60.219.165.24//bxqcjcxAction.do&apos;    # 设置代理相关    headers = {            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;Cookie&apos;:&apos;JSESSIONID=xxxxxxxxxxx_gUJ4aiw&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;             }    # 利用session爬取请求,之后可方便的get与post    s = requests.session()    response = s.post(url=url, headers=headers)    # 设置成网页对应的编码格式    response.encoding = &apos;GB2312&apos;    # 设置成为符合需要的表达式以及模式为&apos;任意匹配模式&apos;    pattern = re.compile(&apos;&lt;tr.*?class=&quot;even&quot;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td align=&quot;center&quot;&gt;(.*?)&lt;/td&gt;.*?&amp;npsb;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;/td&gt;.*?&lt;td&gt;align=&quot;center&quot;&gt;(.*?)&amp;npsb;&lt;/P&gt;&lt;/td&gt;.*?&amp;npsb;&lt;/P&gt;&apos;,                             re.S)    # 成绩信息采集    grades = re.findall(pattern, response.text)    # 输出对应的课程信息    for each_grades in grades:        print(&apos;课程名称：&apos; + each_grades[0] + &apos;分数：&apos; + each_grades[1])</code></pre><p>关于<em>正则表达式</em>的解说可结合<a href="https://cuiqingcai.com/977.html" target="_blank" rel="external">python正则表达式</a>此篇文章学习。</p><p>简单解说一下，先贴出来<em>html信息</em>是怎么样的。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-11%20%E4%B8%8A%E5%8D%8810.12.41.png" alt=""></p><blockquote><p>只截取了一部分，可自行了解。</p></blockquote><p>解说：</p><ol><li><p>.<em>? 是一个固定的搭配，.和</em>代表可以匹配任意无限多个字符，加上？表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。</p></li><li><p>(.<em>?)代表一个分组，在这个正则表达式中我们匹配了两个分组，在后面的遍历grades中，grade[0]就代表第一个(.</em>?)所指代的内容，grade[1]就代表第二个(.*?)所指代的内容，以此类推。</p></li><li><p>re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。</p></li></ol><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>参考文章列表：</li></ul><ol><li><p><a href="http://zihaolucky.github.io/using-python-to-build-zhihu-cralwer/" target="_blank" rel="external">参考一</a></p></li><li><p><a href="http://blog.csdn.net/c406495762/article/details/69817490" target="_blank" rel="external">参考二</a></p></li><li><p><a href="http://www.cnblogs.com/greenteemo/p/6629126.html" target="_blank" rel="external">参考三</a></p></li></ol><blockquote><p>还有一些文档，均可在官网上看到，利用搜索引擎即可。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为最近在学习&lt;em&gt;python3网络爬虫&lt;/em&gt;，想自己写一些小程序来实战一下。爬的是&lt;em&gt;URP教务网&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;一开始的思路是利用&lt;em&gt;beautiful&lt;/em&gt;模块来进行爬取相关的&lt;em&gt;html信息&lt;/em&gt;，直接来得到需要的信息。结果发
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于计算机基础知识（一）</title>
    <link href="https://liujunjie11.github.io/2018/03/08/%E5%85%B3%E4%BA%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://liujunjie11.github.io/2018/03/08/关于计算机基础知识（一）/</id>
    <published>2018-03-08T00:56:07.000Z</published>
    <updated>2018-03-08T02:09:56.897Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>平时有些兴趣想要了解一些关于<em>计算机</em>的基础知识，我就想着为何不将其作为一个笔记记录在博客上呢？以后想从事与<em>计算机</em>相关的工作，了解基本的<em>计算机知识</em>必不可少，因为时间，精力的有限，我会将一些认为重要的或者是必要的基础知识记录在此，打算作为一个系列在此展示了，希望能帮助更多的人。</p></blockquote><h2 id="编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解"><a href="#编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解" class="headerlink" title="编程语言知识：关于强类型语言与弱类型语言，语言的动静态区分以及解释性语言与编译性语言的理解"></a>编程语言知识：关于<em>强类型语言</em>与<em>弱类型语言</em>，语言的<em>动静态</em>区分以及<em>解释性语言</em>与<em>编译性语言</em>的理解</h2><blockquote><p>可以一张图大概的简单理解一下关于<em>强类型语言</em>与<em>弱类型语言</em>，语言的<em>动静态</em>（下图来源于参考一链接）</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/1283539-31968e5f19abed4b.jpg" alt=""></p><blockquote><p>那么我们常见的语言分类：</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/1283539-1820df734cf34260.png" alt=""></p><hr><ul><li>简单总结<em>强类型语言</em>与<em>弱类型语言</em>：</li></ul><ol><li><p>编译时前者需要赋予变量对应的数据类型，后者可不必必须。（如<em>c语言</em>与<em>python</em>的编译时）</p></li><li><p>前者需要改变数据类型时需要相关的函数方法强制改变，后者可忽略。（如在<em>c语言</em>强制要求更改类型时的<strong>（double）变量</strong>）</p></li></ol><ul><li>简单总结语言的<em>动静态</em>区别：</li></ul><ol><li><p><em>动态性语言</em>在编译器上编译时不提供代码数据类型的指错，仅仅是在运行时指出，而静态则是在编译器上编译时就提供代码数据类型的指错。（如<strong><em>int a=’a’;</em></strong>时编译器马上会在一旁提示错误时的便可理解为静态语言，如<em>c语言</em>）</p></li><li><p><em>动态性</em>较于<em>静态性</em>更具有严谨性，可读性。</p></li></ol><ul><li><em>解释性语言</em>与<em>编译性语言</em>的理解（因为在参考4的说明较为详细了，我就直接复制相关的内容好了…）</li></ul><p>首先，我们编程都是用的高级语言(写汇编和机器语言的大牛们除外)，计算机不能直接理解高级语言，只能理解和运行机器语言，所以必须要把高级语言翻译成机器语言，计算机才能运行高级语言所编写的程序。</p><p>说到翻译，其实翻译的方式有两种，一个是编译，一个是解释。两种方式只是翻译的时间不同。</p><p>用编译型语言写的程序执行之前，需要一个专门的编译过程，通过编译系统（不仅仅只是通过编译器，编译器只是编译系统的一部分）把高级语言翻译成机器语言（具体翻译过程可以参看下图），把源高级程序编译成为机器语言文件，比如windows下的exe文件。以后就可以直接运行而不需要编译了，因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高，但也不能一概而论，部分解释型语言的解释器通过在运行时动态优化代码，甚至能够使解释型语言的性能超过编译型语言。<br><img src="http://owudg3xs2.bkt.clouddn.com/20131124170842718.png" alt=""></p><blockquote><p>一个完整的编译系统与 一个用C编写的程序hello.c的编译过程 </p></blockquote><p>解释则不同，解释型语言编写的程序不需要编译。解释型语言在运行的时候才翻译，比如VB语言，在执行的时候，专门有一个解释器能够将VB语言翻译成机器语言，每个语句都是执行的时候才翻译。这样解释型语言每执行一次就要翻译一次，效率比较低。</p><p>编译型与解释型，两者各有利弊。前者由于程序执行速度快，同等条件下对系统要求较低，因此像开发操作系统、大型应用程序、数据库系统等时都采用它，像C/C++、Pascal/Object Pascal（Delphi）等都是编译语言，而一些网页脚本、服务器脚本及辅助开发接口这样的对速度要求不高、对不同系统平台间的兼容性有一定要求的程序则通常使用解释性语言，如JavaScript、VBScript、Perl、Python、Ruby、MATLAB 等等。</p><p>但随着硬件的升级和设计思想的变革，编译型和解释型语言越来越笼统，主要体现在一些新兴的高级语言上，而解释型语言的自身特点也使得编译器厂商愿意花费更多成本来优化解释器，解释型语言性能超过编译型语言也是必然的。</p><ul><li><strong>以下更为详细的参考：</strong></li></ul><p><a href="https://www.jianshu.com/p/336f19772046" target="_blank" rel="external">参考1</a></p><p><a href="https://www.zhihu.com/question/19918532" target="_blank" rel="external">参考2</a></p><p><a href="https://zh.wikipedia.org/wiki/強弱型別" target="_blank" rel="external">参考3</a></p><p><a href="http://blog.csdn.net/zhu_xun/article/details/16921413" target="_blank" rel="external">参考4</a></p><h2 id="计算机基础知识推荐图书"><a href="#计算机基础知识推荐图书" class="headerlink" title="计算机基础知识推荐图书"></a>计算机基础知识推荐图书</h2><blockquote><p>一个日本相关行业人员的书籍系列，通俗易懂。在此分享。</p></blockquote><p><a href="https://pan.baidu.com/s/13OUBV9zvmBAY3eTMI6DlFg" target="_blank" rel="external">网络是怎样链接的</a></p><p><a href="https://pan.baidu.com/s/1XYR5uJasnrbBXY43hupA0w" target="_blank" rel="external">计算机是怎么跑起来的</a></p><p><a href="https://pan.baidu.com/s/1k9KAxONC5pd1qQAHRoEnAg" target="_blank" rel="external">程序是怎么跑起来的</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;平时有些兴趣想要了解一些关于&lt;em&gt;计算机&lt;/em&gt;的基础知识，我就想着为何不将其作为一个笔记记录在博客上呢？以后想从事与&lt;em&gt;计算机&lt;/em&gt;相关的工作，了解基本的&lt;em&gt;计算机知识&lt;/em&gt;必不可少，因为时间，精力的有限，我会将一些认为重要的
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>关于在学习python3网络爬虫模块时的易模糊点</title>
    <link href="https://liujunjie11.github.io/2018/03/06/%E5%85%B3%E4%BA%8E%E5%9C%A8%E5%AD%A6%E4%B9%A0python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E6%A8%A1%E5%9D%97%E6%97%B6%E7%9A%84%E6%98%93%E6%A8%A1%E7%B3%8A%E7%82%B9/"/>
    <id>https://liujunjie11.github.io/2018/03/06/关于在学习python3网络爬虫模块时的易模糊点/</id>
    <published>2018-03-06T12:10:44.000Z</published>
    <updated>2018-03-12T11:31:34.605Z</updated>
    
    <content type="html"><![CDATA[<ul><li>两种代理格式</li></ul><p>在学习<em>urlretrieve</em>函数方法时，尝试了与<em>BeautifulSoup</em>模块的结合编码。</p><p>在<em>request.bulid_opener()</em>的一类函数方法中，如：</p><pre><code>headers = (&apos;User-Agent&apos;,&apos;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;)# 安装openeropener = request.build_opener()# 添加代理opener.addheaders = [headers]# 使用临时的&apos;urlopen&apos;request.install_opener(opener)</code></pre><hr><p>而在有<em>requests.get(url=url,headers=headers…)</em>一类函数方法中，如：</p><pre><code>headers = {&apos;User-Agent&apos;:           &apos;Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;}    # 利用requests模块中的方法获取URL信息，加上我们的代理re = requests.get(url=url, headers=headers)</code></pre><blockquote><p><strong>比较两者易知，两者的代理规定与是否为字典形式有偏差。</strong></p></blockquote><ul><li><p>在利用<em>requests.get()</em>下载图片时</p><pre><code>re = requests.get(url=url)        #应当加上二进制的格式规定，避免不必要的错误麻烦   with open(&apos;美女图片.jpg&apos;,&apos;wb&apos;) as mntu:       for chunk in re.iter_content(chunk_size = 1024):            if chunk:                mntu.write(chunk)                mntu.flush()</code></pre></li></ul><blockquote><p>在查询相关的资料时，有<em>Stream=True</em>的参数说明要加入，但是加入之后却是错误显示。在<em>with</em>下面的<em>for chunk…</em>在下载图片时应当加入。<a href="https://stackoverflow.com/questions/16694907/how-to-download-large-file-in-python-with-requests-py" target="_blank" rel="external">为何要加入，在此可易知</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;两种代理格式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在学习&lt;em&gt;urlretrieve&lt;/em&gt;函数方法时，尝试了与&lt;em&gt;BeautifulSoup&lt;/em&gt;模块的结合编码。&lt;/p&gt;
&lt;p&gt;在&lt;em&gt;request.bulid_opener()&lt;/em&gt;的一类函数方法
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于Charle的简单抓包操作</title>
    <link href="https://liujunjie11.github.io/2018/03/06/%E5%85%B3%E4%BA%8ECharle%E7%9A%84%E7%AE%80%E5%8D%95%E6%8A%93%E5%8C%85%E6%93%8D%E4%BD%9C/"/>
    <id>https://liujunjie11.github.io/2018/03/06/关于Charle的简单抓包操作/</id>
    <published>2018-03-06T03:47:36.000Z</published>
    <updated>2018-03-06T04:23:05.352Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看他人的博客学习<em>python3网络爬虫</em>的实战，看到了“抓包”的这个词，以前不知道…</p></blockquote><p>用的是<em>Macbook</em>，在网上搜索了一下，<em>Charle</em>好评多多，急忙到网上找了破解教程…（关于如何找到，可利用百度谷歌搜索关键词）</p><p>第一次用这个，啥都不会，基本操作也不会，摸索了一下，打算记录下来帮助需要的人。</p><h2 id="抓包的操作"><a href="#抓包的操作" class="headerlink" title="抓包的操作"></a>抓包的操作</h2><p>进入主界面先看看快捷键也是个不错的选择。如图：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.02.png" alt=""></p><blockquote><p>可以知道，与<em>Mac本机浏览器</em>的快捷键相似。</p></blockquote><p><em>New Session</em>即为打开一个新的界面。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.46.png" alt=""></p><p>点击<em>钢笔</em>图标，输入<em>URL</em>，自动开始抓包了。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.09.56.png" alt=""></p><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.58.png" alt=""></p><p>查看相关的快捷键，定向查找<em>html</em>中格式，如<em>json</em>格式</p><ul><li><em>Find</em></li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.03.49.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.03.59.png" alt=""></p><h2 id="系统代理配置"><a href="#系统代理配置" class="headerlink" title="系统代理配置"></a>系统代理配置</h2><ul><li>可以自动对目前系统中的一些<em>URL</em>进行抓包。</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-06%20%E4%B8%8B%E5%8D%8812.02.17.png" alt=""></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>继续摸索中..听说功能很强大，网上的教程非常多，暂时用不到太多，简单的操作说明到处结束了。</li></ul><ul><li>一些可参考的教程：</li></ul><p><a href="https://www.jianshu.com/p/9822e3f28f0a" target="_blank" rel="external">可参考一</a></p><p><a href="http://blog.csdn.net/liulanghk/article/details/46342205" target="_blank" rel="external">可参考二</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;最近在看他人的博客学习&lt;em&gt;python3网络爬虫&lt;/em&gt;的实战，看到了“抓包”的这个词，以前不知道…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用的是&lt;em&gt;Macbook&lt;/em&gt;，在网上搜索了一下，&lt;em&gt;Charle&lt;/em&gt;好评多多，
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取网络图片的小程序项目</title>
    <link href="https://liujunjie11.github.io/2018/03/05/python3%E7%88%AC%E5%8F%96%E7%BD%91%E7%BB%9C%E5%9B%BE%E7%89%87%E7%9A%84%E5%B0%8F%E7%A8%8B%E5%BA%8F%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/03/05/python3爬取网络图片的小程序项目/</id>
    <published>2018-03-05T13:25:16.000Z</published>
    <updated>2018-03-10T03:28:35.670Z</updated>
    
    <content type="html"><![CDATA[<p> 最近在学习<em>python3网络爬虫</em>，看的是<a href="http://blog.csdn.net/c406495762/article/details/72597755" target="_blank" rel="external">这位学长的博客</a>。</p><blockquote><p>因为刚刚开始接触，想通过实战来一步步学习，所以先记录下我的学习心得，然后再记录下自己实战的成果以及相关的代码程序。</p></blockquote><ul><li><strong>先声明，以下是学习心得，代码原搬，链接在上。</strong></li></ul><p>##开始爬取的图片的下载代码的演示</p><pre><code> &apos;&apos;&apos;        #开始爬取的图片的下载代码的演示 &apos;&apos;&apos;from bs4 import BeautifulSoupfrom urllib.request import urlretrieve import requestsimport osimport timeif __name__ == &apos;__main__&apos;:    # 指定页面图片的多少数量    for num in range(1, 5):        if num == 1:            url = &apos;http://www.shuaia.net/index.html&apos;        else:            url = &apos;http://www.shuaia.net/index_%d.html&apos; % num        # 因为是下载多个页面的图片，所以之后的图片名称与其相应的图片地址的代码抒写在for循环下编写        &apos;&apos;&apos;          #在此需要设置代理，避免爬取失败几率        &apos;&apos;&apos;        headers = {&apos;User-Agent&apos;:                &apos;* Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36&apos;}        # 利用requests模块中的方法获取URL信息，加上我们的代理        re = requests.get(url=url, headers=headers)        # 指定编码，避免中文乱码        re.encoding = &apos;utf-8&apos;        &apos;&apos;&apos;            # 利用beautifulsoup模块来进行处理            #re.text : 转换成规定的字符格式            #&apos;lxml&apos; : 利用lxml解析器进行解析        &apos;&apos;&apos;        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 解析完成之后，需要对页面的html信息进行分析了，指定相关的节点        &apos;&apos;&apos;            #class_是为了避免错误混淆，所以不可&quot;原班人马&quot;        &apos;&apos;&apos;        tasges_html = bf.find_all(class_=&apos;item-img&apos;)        # 指定空的列表，装下相关的需要的信息        list = []        # 利用循环，一一导入        for each in tasges_html:            &apos;&apos;&apos;                #加上img是因为&apos;alt&apos;在以下的子节点中，所以需要相关的指明，即一个节点（父，子）一个指明            &apos;&apos;&apos;            list.append(each.img.get(&apos;alt&apos;) + &apos;=&apos; + each.get(&apos;href&apos;))        # 至此，相关的信息就采集完成了        print(&apos;采集over，开始下载：&apos;)    &apos;&apos;&apos;        #开始将采集好的信息一一下载    &apos;&apos;&apos;    for each_img in list:        # 以=分割图片地址与图片名称        img_info = each_img.split(&apos;=&apos;)        target_url = img_info[1]        filename = img_info[0] + &apos;.jpg&apos;        print(&apos;下载：&apos; + filename)        headers = {            &quot;User-Agent&quot;:          &quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&quot;        }        img_req = requests.get(url=target_url, headers=headers)        img_req.encoding = &apos;utf-8&apos;        img_html = img_req.text        img_bf_1 = BeautifulSoup(img_html, &apos;lxml&apos;)        img_url = img_bf_1.find_all(&apos;div&apos;, class_=&apos;wr-single-content-list&apos;)        img_bf_2 = BeautifulSoup(str(img_url), &apos;lxml&apos;)        img_url = &apos;http://www.shuaia.net&apos; + img_bf_2.div.img.get(&apos;src&apos;)        #若是指定的目录不存在则建立一个        if &apos;images&apos; not in os.listdir():            os.makedirs(&apos;images&apos;)        # 即指定URL地址下载        urlretrieve(url=img_url, filename=&apos;images/&apos; + filename)        time.sleep(1)    print(&apos;下载完成！&apos;)</code></pre><blockquote><p><strong>以上是通过学习<a href="http://blog.csdn.net/c406495762/article/details/72597755" target="_blank" rel="external">此篇博客文章</a>的学习心得，相关的加上图片解说可以看这篇博客文章。</strong></p></blockquote><ul><li><strong>附上用得到的知识链接：</strong></li></ul><p><a href="http://qinxuye.me/article/details-about-time-module-in-python/" target="_blank" rel="external">Python中time模块详解</a></p><p><a href="http://blog.csdn.net/vevenlcf/article/details/46777023" target="_blank" rel="external">python3中的urlretrieve() 函数使用</a></p><p><a href="https://docs.python.org/3/library/urllib.request.html" target="_blank" rel="external">urllib.request文档</a></p><p><a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="external">Requests文档</a></p><p><a href="http://www.runoob.com/python3/python3-os-file-methods.html" target="_blank" rel="external">Python3 os模块文件/目录方法</a></p><p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="external">Beautiful Soup 4.2.0 文档</a></p><p><a href="http://www.runoob.com/python/python-files-io.html" target="_blank" rel="external">Python 文件I/O模块文档</a></p><h2 id="实战部分-批量爬取美女图片"><a href="#实战部分-批量爬取美女图片" class="headerlink" title="实战部分:批量爬取美女图片"></a>实战部分:批量爬取美女图片</h2><blockquote><p>在这里收藏上<a href="http://www.bijishequ.com/detail/424024?p=" target="_blank" rel="external">这一篇python3爬取图片的快速入门地址</a>，有相关的模块方法介绍，有助于快速入门，再从一系列模块入手，之后再实战，一步步掌握。</p></blockquote><ul><li>通过另外一个网站的分析，爬取其图片并且下载。（下载的图片均可在本工程目录可找到）</li></ul><p><strong>网址：<a href="http://www.27270.com/ent/meinvtupian/2017/223643.html" target="_blank" rel="external">http://www.27270.com/ent/meinvtupian/2017/223643.html</a></strong></p><blockquote><p>搜索的美女图片关键词，随便点开的一个…</p></blockquote><p>现在我们打开网址，利用<em>Chrome</em>浏览器的抓包，看看此网址的<em>html</em>信息（如下）</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.02.32.png" alt=""></p><hr><p>在此有一个特别好用的方法，即为点击下方指出的图标，当我们把鼠标移到网页上的异常时，对应的<em>html</em>信息就会自动对齐。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/3%E6%9C%88-08-2018%2020-10-20.gif" alt=""></p><blockquote><p>现在开始分析代码，这个网址是专题式的…点开一个专题就会有一个类型的图片…就网页看来一个页面仅仅一张图片。</p></blockquote><p><strong>在我们将鼠标移到图片处时，对应的代码也就自动对齐了（图1），还有它在的那个节点（图2），再点击下一张看看，查看<em>html</em>信息（图3），发现它们的节点信息一致，不同的仅仅为对应的标题以及图片的目录地址了（图4），我们可以从这里下手一步步来</strong></p><ul><li><p>图1<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.53.png" alt=""></p></li><li><p>图2<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.35.png" alt=""></p></li><li><p>图3<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.14.14.png" alt=""></p></li><li><p>图4<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.15.00.png" alt=""></p></li></ul><blockquote><p>关于<em>html</em>的基础知识，只需要知道它们的结构以及节点，一些变量等的基础知识就够了，不需要太深入。可以在网上搜索关键词‘html基础知识’查看。这绝对是一劳永逸的。</p></blockquote><p>在分析之后写个测试代码：</p><pre><code>&apos;&apos;&apos;    爬取美女图片实战代码&apos;&apos;&apos;        import requestsfrom bs4 import BeautifulSoupif __name__ == &apos;__main__&apos;:    url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;    re = requests.get(url=url)    # 编译解码格式，避免乱码出现    re.encoding = &apos;utf-8&apos;    # 用beautiful模块进行解析    bf = BeautifulSoup(re.text, &apos;lxml&apos;)    #在此注意find_all与find方法的使用，因为一个页面仅仅有一张图片，用前者会出错    bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)    list_html = []    list_html.append(bf_html.img.get(&apos;alt&apos;) + &apos;  :  &apos; + bf_html.img.get(&apos;src&apos;))    print(list_html)</code></pre><p>运行，发现出现了乱码的情况：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.30.29.png" alt=""></p><p>将’utf-8’格式换成了’GB2312’，发现乱码问题得到解决：<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.35.17.png" alt=""></p><hr><p>查看了一下<em>html</em>信息，发现’utf-8’格式是不可取的，对于这个网站来说。<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.34.26.png" alt=""></p><p>还可以用以下这段代码得出网址的编码方式。</p><pre><code>#关于输出网页编码方式的判断from urllib import requestimport chardetif __name__ == &quot;__main__&quot;:    response = request.urlopen(&quot;http://www.27270.com/ent/meinvtupian/2017/223643.html&quot;)    html = response.read()    charset = chardet.detect(html)    print(charset)    </code></pre><p>运行看看：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.44.21.png" alt=""></p><ul><li>可参考：<a href="http://blog.csdn.net/qq_21460525/article/details/78217697" target="_blank" rel="external">chardet模块简单认识</a></li></ul><blockquote><p>得到了图片的目录信息以及相关的信息之后，就好办了。</p></blockquote><hr><p>为什么这么说？实际上我们爬取图片时仅仅需要图片的目录地址就可以利用一两行<em>python爬虫</em>代码下载下来了，代码利用<em>urllib.requests模块中的urlretrieve() 函数</em>就能轻而易举的办到。代码在下，也可以查看相关的文档来学习。</p><pre><code>from  urllib.request import urlretrieveimport time# 简单爬取一张图片，或者是需要代理的测试if __name__ == &apos;__main__&apos;:    print(&apos;开始下载：&apos;)    urlretrieve(url=&apos;html上的图片src信息&apos;, filename=&apos;xx照片&apos;+&apos;.jpg&apos;)    time.sleep(1)    print(&apos;下载完成！&apos;)</code></pre><blockquote><p>可以自己试试。</p></blockquote><hr><p>因为我们是为了爬取多张图片…所以可以分析一下每一页<em>URL</em>的不同或者是相似点。看图1，2发现了有索引<strong>_2</strong>出现，那再点击下一页会有<strong>_3</strong>…<br>每一页有不同的照片，因为是一个专题…那么照片地址也不一样了（如图3，4）。</p><ul><li><p>图1<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.57.34.png" alt=""></p></li><li><p>图2<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%888.57.47.png" alt=""></p></li></ul><hr><ul><li>图3</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%889.01.44.png" alt=""></p><ul><li>图4<br><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-08%20%E4%B8%8B%E5%8D%889.01.53.png" alt=""></li></ul><blockquote><p>可以轻而易举的发现问题的所在了，接下来直接贴上完整的下载代码，不懂的朋友应当找相关的文档来自己查看，多写多实战！</p></blockquote><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><ul><li><p><strong>代码编译步骤：查看信息到分析整合最后是代码实现。</strong></p><pre><code>&apos;&apos;&apos;    函数目标：    爬取美女图片完整代码    编写开始时间：    2018-3-08&apos;&apos;&apos;import requestsfrom bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport os import time if __name__ == &apos;__main__&apos;:    # 一个专题8页..    for  num in range(1, 9):        if num == 1:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;          else:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643_%d.html&apos; % num        # 在此就不添加代理部分的了，添加可起到隐蔽的效果，在需要的场合应当添加上        re = requests.get(url=url)        # 之指定编码格式        re.encoding = &apos;GB2312&apos;        # 用beautiful模块解析,具体可查看文档内容        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 一页一张，用find方法        bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)        # 在创建的目录下保存！        urlretrieve(url=bf_html.img.get(&apos;src&apos;), filename=bf_html.img.get(&apos;alt&apos;) + &apos;系列之%d.jpg&apos; % num)        # 一秒一个步骤实行代码        time.sleep(1)        print(&apos;开始下载第%d张&apos; % num)    print(&apos;下载完成，请查收...&apos;)</code></pre></li></ul><p>运行，查看效果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-09%20%E4%B8%8B%E5%8D%889.47.25.png" alt=""></p><p>在本工程目录夹可查收…</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-09%20%E4%B8%8B%E5%8D%889.50.25.png" alt=""></p><hr><h2 id="最后总结"><a href="#最后总结" class="headerlink" title="最后总结"></a>最后总结</h2><p>代码还可以完善，比如可创建一个目录专门便于我们查收…</p><p>总之，还需要多多学习。</p><h2 id="修改添加目录程序"><a href="#修改添加目录程序" class="headerlink" title="修改添加目录程序"></a>修改添加目录程序</h2><pre><code>import requestsfrom bs4 import BeautifulSoupfrom urllib.request import urlretrieveimport os import time if __name__ == &apos;__main__&apos;:    #可先建立一个专门存放图片的目录文件夹    if &apos;images&apos; not in os.listdir():        os.makedirs(&apos;images&apos;)    print(&apos;建立目录夹完成，开始下载图片！&apos;)    # 一个专题8页..    for  num in range(1, 9):        if num == 1:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643.html&apos;          else:            url = &apos;http://www.27270.com/ent/meinvtupian/2017/223643_%d.html&apos; % num        # 在此就不添加代理部分的了，添加可起到隐蔽的效果，在需要的场合应当添加上        re = requests.get(url=url)        # 之指定编码格式        re.encoding = &apos;GB2312&apos;        # 用beautiful模块解析,具体可查看文档内容        bf = BeautifulSoup(re.text, &apos;lxml&apos;)        # 一页一张，用find方法        bf_html = bf.find(&apos;div&apos;, class_=&apos;articleV4Body&apos;)        # 在创建的目录下保存！        urlretrieve(url=bf_html.img.get(&apos;src&apos;), filename=&apos;images/&apos;+bf_html.img.get(&apos;alt&apos;) + &apos;系列之%d.jpg&apos; % num)        # 一秒一个步骤实行代码        time.sleep(1)        print(&apos;开始下载第%d张&apos; % num)    print(&apos;下载完成，请查收...&apos;)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt; 最近在学习&lt;em&gt;python3网络爬虫&lt;/em&gt;，看的是&lt;a href=&quot;http://blog.csdn.net/c406495762/article/details/72597755&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这位学长的博客&lt;/
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>麻木</title>
    <link href="https://liujunjie11.github.io/2018/03/03/%E9%BA%BB%E6%9C%A8/"/>
    <id>https://liujunjie11.github.io/2018/03/03/麻木/</id>
    <published>2018-03-03T07:15:57.000Z</published>
    <updated>2018-03-03T07:50:58.179Z</updated>
    
    <content type="html"><![CDATA[<p>新的学期开始了。</p><p>当我反思了自己在之前的一些所做所言，我开始了一股深深的愧疚感，我的理想到了哪里去了？我不愿做一个“偏激”的代言人之一，但是麻木亦然不可取。但我应该谨记在心里，不能忘记，更不能因为这现实的种种而丧失了我的初心，忘了我的理想，那我曾经认为的美好。</p><p>我是慢慢的变得麻木了，开始想要得安稳与胸无理想，开始想要的安于现状，这是多么的可怕。</p><p>人在一种环境当中总是或多或少的被影响一些的。如今的我想要一种平静，但那是一种可以应对自如的一种理性，而不是慢慢的陷于麻木的颓废。可能是我太舒服了，变得安于现状了，慢慢的麻木了，开始变得曾经的那些“不喜欢”了。</p><p>写着写着，突然想起来一句话：生于忧患，死于安乐。此时此刻的我对于这句话有了自己的更为深刻的认识。一个人活着应该是为了这个人类社会的忧患做出应有的努力。至少我是认同这样的一种观点的。我有些怀疑过这些许观点的必要性，为什么这样的一种观点会得到我的或者是说为什么会得到如今大部分人类的认同从而占据人类社会的主流思想呢？我认为这与人类道德中的“善”是相挂钩的，这与大部分人的期许相对应符合，大多数的人们都渴望有“平和”（“和谐”）的生活状态与“有教养”的人文社会。而我或许也是其中的一份子。</p><p>我曾经有过的一种强烈的理想：通过发明一种技术，从而造福整个人类，让人们生活的更好。</p><p>我也应当时时刻刻的记得自己的期许，有着自己的见解，有着自己的看法，而不是跟着“世俗”走，以免再一次变得“麻木”，我所认为的麻木。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;新的学期开始了。&lt;/p&gt;
&lt;p&gt;当我反思了自己在之前的一些所做所言，我开始了一股深深的愧疚感，我的理想到了哪里去了？我不愿做一个“偏激”的代言人之一，但是麻木亦然不可取。但我应该谨记在心里，不能忘记，更不能因为这现实的种种而丧失了我的初心，忘了我的理想，那我曾经认为的美好。
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>假期小记</title>
    <link href="https://liujunjie11.github.io/2018/01/22/%E5%81%87%E6%9C%9F%E5%B0%8F%E8%AE%B0/"/>
    <id>https://liujunjie11.github.io/2018/01/22/假期小记/</id>
    <published>2018-01-22T10:36:27.000Z</published>
    <updated>2018-01-23T09:33:21.122Z</updated>
    
    <content type="html"><![CDATA[<p>最近总是想记一些东西，不过总是因为太懒就搁下了。</p><p>我谈恋爱了，是我喜欢的女孩。但是让我感觉有一些不靠谱，让我感觉到一场恋爱的开始是如此的容易…这是我没有想到的。她也喜欢我，我也喜欢她，一场恋爱就这样的开始了。我想开始一场恋爱也就如此而已，哈哈。</p><p>刚刚因为<em>iCloud</em>的一些事搁写了一下，现在继续。说到底<em>Apple</em>关于对于<em>iCloud</em>的宣布是对我有一些影响的，我对于自己的东西总是会有一些担心呢，哈哈。</p><p>我好久没怎么看三毛的书了，我以为我看了一本她的《三毛全集》就以为已将她的书都看完了，我发现还有一本她的书《送你一匹马》，依旧是从前的感觉，不过看的时候我的心情却是比从前平静的多了，但是，在其中带给我的感动总是满满的。</p><p>我如今觉得读书人读太多的书，或许是一种悲哀，一种赤裸裸的悲哀。难道不是吗，很多时候的可笑都是这一群自以为是的读书人做出来的。人都是矛盾的，这一点在读书人身上我却是更加的容易看到。读书人读书人，可笑的就是现在的我呢，哈哈。</p><p>这次放假的生活，也就像在学校一样的，吃喝拉玩睡等，不过玩和睡的比例却是大大的增加的了，哈哈。</p><p>突然的写着写着发现其实也没什么好写的了，不过是生活中的一些小小的记录。生活中的每一次清醒，我都会有一种极端的感觉，既然人生一场本来就是一无所有，我们现在做的却也不过是一场徒劳。不过有意义的总是在生活中的处处场景可以看到，我也一直相信，人生一场，意义是绝对存在的。</p><p>如今我更加的看重生活中的每一处，因为那是生活。我不过也就是一个学习者，我还要多多的学习，向生活学习。我自称学习者，刚刚写完这句话我又想笑了，哈哈。</p><p>算了算了。我发现如今的自己没有从前的那般的“热情记录”了，我变得不想说话在很多时候，其实说到底可能就是一种心态的体现而已。我不见得就是所谓的成长，有些东西随着时间的流逝有得有失，过着过着就变成今天这个样子了。</p><p>有时候总是想记一些东西，总是想写一些东西，有一种不写不舒服的感觉，有时候我觉得有一些欲望总是好的，特别是在这个无所事事的假期，我想我真的是懒得过分了过分了。</p><p>这一篇小记在此就结束了，想写的时候我会再来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近总是想记一些东西，不过总是因为太懒就搁下了。&lt;/p&gt;
&lt;p&gt;我谈恋爱了，是我喜欢的女孩。但是让我感觉有一些不靠谱，让我感觉到一场恋爱的开始是如此的容易…这是我没有想到的。她也喜欢我，我也喜欢她，一场恋爱就这样的开始了。我想开始一场恋爱也就如此而已，哈哈。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="生活 感悟" scheme="https://liujunjie11.github.io/categories/%E7%94%9F%E6%B4%BB-%E6%84%9F%E6%82%9F/"/>
    
    
      <category term="生活 感悟" scheme="https://liujunjie11.github.io/tags/%E7%94%9F%E6%B4%BB-%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>关于在学习python爬虫时的学习记录</title>
    <link href="https://liujunjie11.github.io/2017/12/08/%E5%85%B3%E4%BA%8E%E5%9C%A8%E5%AD%A6%E4%B9%A0python%E7%88%AC%E8%99%AB%E6%97%B6%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/08/关于在学习python爬虫时的学习记录/</id>
    <published>2017-12-08T13:36:46.000Z</published>
    <updated>2018-03-13T10:14:18.751Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习<em>python3爬虫</em>，看的是<a href="http://blog.csdn.net/c406495762/article/details/78123502" target="_blank" rel="external">这位博主的博客</a>，不得不说，是真的厉害，通俗易懂^ _ ^</p><p><strong>我要学习的还有很多…从基本的<em>python</em>知识，我就被难倒了…</strong></p><p>哎，记录下我的盲点…</p><p>花了近一个钟头测试出来的结果。</p><ul><li>在爬取相关的<em>html</em>时，<strong>text ≠ text[0]</strong></li></ul><blockquote><p>后者是正确的。我一直以为不加的效果也是一样的结果，在我理解看来就是从头开始的，即<strong>从0到尾的所有相关的内容</strong>，实际上我的理解与相关的<em>python</em>基础不谋而和，可能是爬虫就需要如此的？我就默认好了…</p></blockquote><ul><li>在<em>python</em>中的方法后面的<strong><em>（）</em></strong>是不可省去的</li></ul><blockquote><p>在我学过一些其他的语言，在方法后面可不加括号，如<em>Scala</em>，曾经我在其他的<em>python</em>编程中是行得通的…但是在处理爬虫的代码时就报错了，我也默认了…</p></blockquote><ul><li>在<em>class</em>括号中的<em>object</em>是可有可无的</li></ul><blockquote><p> 这无疑是<em>python</em>的基本知识…</p></blockquote><ul><li>关于在<em>python</em>中类似<em>C</em>中的<em>printf</em>函数的使用</li></ul><blockquote><p>基本上是与<em>C</em>中的用法一致的，但是在爬虫中需要将爬取的内容输出，就需要<em>%%</em>来表示了…参考了<a href="http://www.169it.com/article/11773602545120851576.html" target="_blank" rel="external">这篇文章</a>，其中的有句这样解释道：<strong>用进行转义一样，这里用%作为格式标记的标识，也有一个%本身应该如何输出的问题。如果要在”格式标记字符串“中输出%本身，可以用%%来表示。</strong></p></blockquote><p>在用文件的<em>io</em>与<em>requests模块</em>的结合（<strong>x.content()</strong>）,可达到与<em>urlretrieve函数</em>同样的效果（只需输入URL地址即可）。</p><p>总结：还需要更多的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近学习&lt;em&gt;python3爬虫&lt;/em&gt;，看的是&lt;a href=&quot;http://blog.csdn.net/c406495762/article/details/78123502&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这位博主的博客&lt;/a&gt;，不
      
    
    </summary>
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>今后的打算</title>
    <link href="https://liujunjie11.github.io/2017/12/07/%E4%BB%8A%E5%90%8E%E7%9A%84%E6%89%93%E7%AE%97/"/>
    <id>https://liujunjie11.github.io/2017/12/07/今后的打算/</id>
    <published>2017-12-07T07:18:53.000Z</published>
    <updated>2017-12-07T07:49:44.073Z</updated>
    
    <content type="html"><![CDATA[<p>恍然大悟，等我看到还有保研一说的政策时，我如今已是一名大二的学生了，保研的机会我基本上已是错过了。在我参考了一些相关的资料时，我又一次看到了人与人之间的差距…</p><p><strong>眼界对于一个人真的是太重要了，一个人不应该总是停留在现在，应该时时思考自己未来的方向。</strong></p><p>在如今的科技浪潮上，更高的学历当然是我所向往的，而我不喜欢考试，我深深的感觉到了自己不擅长考试，不管是小的还是大的，我对于考试已是到了难以忍耐的地步了，我能从内心感觉的到。</p><p>在我的这个时代，人工智能的浪潮早已是滚滚而来了。如今的我们身处在这个看似大好的“时代”，我也想分一杯羹，我也想通过这一次浪潮发挥自己的效用，借此实现自己的理想，为世界带来更多的福利，为世界人民带来更好的生活。</p><p>而在如今的现实是，高学历的普遍存在…这无疑会成为我的短板…而我可能也会因此而做出退让，放弃自己的技术选项…这真的是我的悲剧啊,会是我人生最大的悲剧吗？</p><p>如今的我需要突出自己的优势了，一定要突出自己的优势。更高的学历意味着无疑拥有更大的机会会得到更好的资源与更加广阔的眼界，不得不说，一个人的学习应该是终生的…</p><p>目前的我太过于浮躁与无定向的迷乱了…<strong>我还需要更多的学习与实践，突出自己的优势，必将是我应当做的，就在不远的今后。</strong></p><p><strong>摆正心态，不断实践，突出优势！</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;恍然大悟，等我看到还有保研一说的政策时，我如今已是一名大二的学生了，保研的机会我基本上已是错过了。在我参考了一些相关的资料时，我又一次看到了人与人之间的差距…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;眼界对于一个人真的是太重要了，一个人不应该总是停留在现在，应该时时思考自己未来的方向
      
    
    </summary>
    
      <category term="成长" scheme="https://liujunjie11.github.io/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="成长" scheme="https://liujunjie11.github.io/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>关于利用python爬虫爬取小说的实战例子</title>
    <link href="https://liujunjie11.github.io/2017/12/04/%E5%85%B3%E4%BA%8E%E5%88%A9%E7%94%A8python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4%E7%9A%84%E5%AE%9E%E6%88%98%E4%BE%8B%E5%AD%90/"/>
    <id>https://liujunjie11.github.io/2017/12/04/关于利用python爬虫爬取小说的实战例子/</id>
    <published>2017-12-04T14:33:41.000Z</published>
    <updated>2017-12-07T13:35:45.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>最近对<strong><em>python爬虫</em></strong>很感兴趣，就花了一些时间来学习，学习了近一周的时间，终于在看了一些其他博客的大神写的教程之后，学会了利用<strong><em>beautifulsoup</em></strong>的方法爬取了一些网站的小说。在此分享我学习的成果。</p><ul><li><strong>关于详细的<em>python3爬虫</em>的学习教程以及参考：</strong><a href="http://blog.csdn.net/column/details/15321.html" target="_blank" rel="external">学习教程</a></li></ul><ul><li>本文参考并且学习了：<a href="http://blog.csdn.net/c406495762/article/details/71158264" target="_blank" rel="external">这篇文章</a></li></ul><ul><li>在以下的文中主要介绍<strong>爬小说的技巧</strong>。</li></ul><h1 id="爬取的过程"><a href="#爬取的过程" class="headerlink" title="爬取的过程"></a>爬取的过程</h1><h2 id="素材"><a href="#素材" class="headerlink" title="素材"></a>素材</h2><ul><li><p><a href="http://www.biqukan.com/38_38278/17032848.html" target="_blank" rel="external">笔趣阁小说网</a></p></li><li><p><a href="https://www.readnovel.com/chapter/7943133504728103/21429716840455516" target="_blank" rel="external">小说阅读网</a></p></li></ul><hr><h2 id="技巧分析"><a href="#技巧分析" class="headerlink" title="技巧分析"></a>技巧分析</h2><p>在编写代码前应先在浏览器中（推荐<em>Chrome</em>，我用的<em>Safari</em>）查看在网页抓包中的相关的内容信息，一般查看其中的：</p><ul><li><p><code>charset</code>（<strong>即其中的编码类型，在后面的程序中需要</strong>）</p></li><li><p><code>在文章开始阶段的前的那一小段html编码</code></p></li></ul><p>如下图所示的（等下将要用到）：</p><ul><li><code>charset</code>所示：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.43.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.49.48.png" alt=""></p><blockquote><p>一般在最前头可以看到。</p></blockquote><hr><ul><li><code>主要的一小段html</code>(即为<code>div</code>的标签，后面为相应的属性值，即一个<code>html</code>中有多个标签，每一个标签用不同的属性值来进行标记以表示不同的标签，从而在一个页面中可以有多个不同的内容展示出来)所示：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.55.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%8812.50.09.png" alt=""></p><blockquote><p>如上在文章的前边的那一小段的<em>html代码</em>。</p></blockquote><hr><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><ul><li>可先用相关的代码查看相关的网页字符属于那种编码类型</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 用beautifulsoup爬取小说的例子</div><div class="line">#     可先将网页编码的字符先行得知</div><div class="line"> from urllib import request</div><div class="line"> import chardet</div><div class="line"> </div><div class="line"> re=request.urlopen(&apos;http://www.biqukan.com/1_1094/5403177.html&apos;)</div><div class="line"> charset=chardet.detect(re.read())</div><div class="line"> print(&apos;we can see the cahrset about the html:&apos;,charset)</div></pre></td></tr></table></figure><blockquote><p>具体的学习可以参考<a href="http://blog.csdn.net/c406495762/article/details/58716886" target="_blank" rel="external">这篇文章</a>。</p></blockquote><hr><ul><li>开始编写爬取相关内容的代码。<strong>将其中的<em>URL</em>地址以及相关的<em>charset所属字符</em>在以下的代码中修改一下均可达到在下方效果的显示</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">#开始爬取相关的内容</div><div class="line">from urllib import request</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">from urllib import error</div><div class="line">try:</div><div class="line">    if __name__ == &apos;__main__&apos;:</div><div class="line">        #相关的URL的输入以及代理</div><div class="line">        re=request.Request(url=&apos;http://www.biqukan.com/1_1094/5403177.html&apos;,headers=&#123;&apos;User-Agent&apos;:&apos;* Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30&apos;&#125;)</div><div class="line">         #打开,并且以相关的解码方式，此处应当对应上方的charset</div><div class="line">             html=request.urlopen(re).read().decode(&apos;gbk&apos;,&apos;ignore&apos;)</div><div class="line">        #接下来创建beautifulsoup对象,找到相关的参数以便爬取内容</div><div class="line">        soup_text=BeautifulSoup(html,&apos;lxml&apos;)</div><div class="line">        #在html中若是有class注意将其中的class改为class_（因为python中有个class关键字的存在了),在此应当对应上方说明的html的一小段的编码</div><div class="line">        texts=soup_text.find_all(id=&quot;content&quot; ，class_=&quot;showtxt&quot;)</div><div class="line">        soup_texts=BeautifulSoup(str(texts),&apos;lxml&apos;)</div><div class="line">        #输出时将删除相关的不符合要求的字符,将其替换为空白</div><div class="line">        print(soup_texts.div.text.replace(&apos;\xa0&apos;,&apos;&apos;))</div><div class="line">except error.URLError as e:</div><div class="line">    if hasattr(e, &apos;code&apos;):</div><div class="line">        print(&apos;httperroe:&apos;)</div><div class="line">        print(e.cond)</div><div class="line">    if hasattr(e, &apos;reason&apos;):</div><div class="line">        print(&apos;urlerror&apos;)</div><div class="line">        print(e.reason)</div></pre></td></tr></table></figure><blockquote><p>详细可参考：<a href="http://blog.csdn.net/c406495762/article/details/71158264" target="_blank" rel="external">这篇文章</a></p></blockquote><hr><h2 id="结果查看"><a href="#结果查看" class="headerlink" title="结果查看"></a>结果查看</h2><ul><li>关于在第一段的实战效果：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.25.11.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.25.26.png" alt=""></p><hr><ul><li>关于在第二段的实战效果：</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.27.06.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-12-05%20%E4%B8%8B%E5%8D%883.27.17.png" alt=""></p><hr><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><blockquote><p><strong>在网页中爬取的小说内容都是可以直接下载至本地的，具体的可以参考以上的那个学习教程。</strong></p></blockquote><ul><li><p>可学习的参考网站：</p><p>  <a href="http://blog.csdn.net/column/details/15321.html" target="_blank" rel="external"><em>python3爬虫教程</em></a></p><p>  <a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="external">beautifulsoup官网教程</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;最近对&lt;strong&gt;&lt;em&gt;python爬虫&lt;/em&gt;&lt;/strong&gt;很感兴趣，就花了一些时间来学习，学习了近一周的时间，终于在看了一些
      
    
    </summary>
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
    
      <category term="学习 成长" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0-%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>关于Mac关机与iCloud的打不开的问题记录</title>
    <link href="https://liujunjie11.github.io/2017/12/03/%E5%85%B3%E4%BA%8EMac%E5%85%B3%E6%9C%BA%E4%B8%8EiCloud%E7%9A%84%E6%89%93%E4%B8%8D%E5%BC%80%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/03/关于Mac关机与iCloud的打不开的问题记录/</id>
    <published>2017-12-03T07:16:59.000Z</published>
    <updated>2017-12-03T07:25:22.367Z</updated>
    
    <content type="html"><![CDATA[<p>这一段时间因为开的网页比较重要，<em>Mac</em>一直没有关机，晚上睡觉也是合上就这样了，没想到才3天这样，<em>Mac</em>好像支撑不了了…变得有些卡了，在未关机前的硬盘空间大小与关机后打开之后的硬盘空间大小相差了<strong>5个G</strong>，在重启开机时出现了一段绿屏…，由此可知，一段有规律的关机保养还是很有必要的…</p><p>关于<em>iCloud</em>的一些打开错误，如打开查看存储信息时的错误问题等等，一般的解决方法为：换一个网络连接即可解决。</p><blockquote><p>此时换<em>DNS</em>的地址是不管用的。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这一段时间因为开的网页比较重要，&lt;em&gt;Mac&lt;/em&gt;一直没有关机，晚上睡觉也是合上就这样了，没想到才3天这样，&lt;em&gt;Mac&lt;/em&gt;好像支撑不了了…变得有些卡了，在未关机前的硬盘空间大小与关机后打开之后的硬盘空间大小相差了&lt;strong&gt;5个G&lt;/strong&gt;，在重
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>关于Mac网速慢的原因记录</title>
    <link href="https://liujunjie11.github.io/2017/12/01/%E5%85%B3%E4%BA%8EMac%E7%BD%91%E9%80%9F%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://liujunjie11.github.io/2017/12/01/关于Mac网速慢的原因记录/</id>
    <published>2017-12-01T10:56:07.000Z</published>
    <updated>2017-12-01T11:07:50.743Z</updated>
    
    <content type="html"><![CDATA[<p>最近<em>Mac</em>突然变得很慢，明明网速很快的。</p><p>在网上看了一下，发现可能是<em>DNS</em>的问题，果然换了一下（以前用的都是<em>8.8.8.8</em>估计用的人多了就卡了），把号删除了，发现网速飞的快起…</p><p>不过在平时中下载<em>app</em>失败时可以考虑换回来就好使了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近&lt;em&gt;Mac&lt;/em&gt;突然变得很慢，明明网速很快的。&lt;/p&gt;
&lt;p&gt;在网上看了一下，发现可能是&lt;em&gt;DNS&lt;/em&gt;的问题，果然换了一下（以前用的都是&lt;em&gt;8.8.8.8&lt;/em&gt;估计用的人多了就卡了），把号删除了，发现网速飞的快起…&lt;/p&gt;
&lt;p&gt;不过在平时中下
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
